dataset_name='stats_full'
data=[[{"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.022, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.06073615327477455, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.057127680629491806, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.09943999350070953, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.09604845196008682, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05391022562980652, "mask": 0.0}, "website": {"value": 1, "prob": 0.4573890268802643, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0906280130147934, "mask": 0.0}, "badges": {"value": null, "prob": 0.13501784205436707, "mask": 0.0}, "terminate": {"prob": 0.010438697412610054, "mask": 0, "value": null}}, "cls_probs": [0.4406076669692993, 0.4286101758480072, 0.13078215718269348], "s_value": 0.5149454474449158, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.0820084735751152, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.15697334706783295, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.16689066588878632, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08115646243095398, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18966524302959442, "mask": 0.0}, "badges": {"value": null, "prob": 0.3171515166759491, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.006154336966574192, "mask": 0, "value": null}}, "cls_probs": [0.40411800146102905, 0.44065263867378235, 0.15522940456867218], "s_value": 0.5047549605369568, "orig_id": 2494, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.07581686228513718, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.14841555058956146, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.15394768118858337, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0770745649933815, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16790814697742462, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.37037208676338196, "mask": 0.0}, "terminate": {"prob": 0.006465097889304161, "mask": 0, "value": null}}, "cls_probs": [0.40730464458465576, 0.4395516812801361, 0.1531437784433365], "s_value": 0.5066061019897461, "orig_id": 2494, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.07609055191278458, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.14942167699337006, "mask": 0.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.14830608665943146, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08046688139438629, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03045189566910267, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.030463794246315956, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03227272629737854, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.029998211190104485, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033046919852495193, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03100791946053505, "mask": 0.0}, "tags": {"value": null, "prob": 0.03198343142867088, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077511675655842, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03045189566910267, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.030463794246315956, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03227272629737854, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.029998211190104485, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033046919852495193, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03100791946053505, "mask": 0.0}, "tags": {"value": null, "prob": 0.03198343142867088, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077511675655842, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.03045189566910267, "mask": 0.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.030463794246315956, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03227272629737854, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.029998211190104485, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033046919852495193, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03100791946053505, "mask": 0.0}, "tags": {"value": null, "prob": 0.03198343142867088, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077511675655842, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.03045189566910267, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.030463794246315956, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03227272629737854, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.029998211190104485, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033046919852495193, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03100791946053505, "mask": 0.0}, "tags": {"value": null, "prob": 0.03198343142867088, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077511675655842, "mask": 0.0}}], "prob": 0.15438507497310638, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.3831283748149872, "mask": 0.0}, "terminate": {"prob": 0.00820138119161129, "mask": 0, "value": null}}, "cls_probs": [0.41057100892066956, 0.44324222207069397, 0.14618678390979767], "s_value": 0.5098711848258972, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.0797898918390274, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.1593712866306305, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.1573576182126999, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.030347809195518494, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.030384479090571404, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03229455649852753, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.029954276978969574, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033164799213409424, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031019892543554306, "mask": 0.0}, "tags": {"value": null, "prob": 0.03222748637199402, "mask": 0.0}, "comments": {"value": null, "prob": 0.03060670755803585, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.030347809195518494, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.030384479090571404, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03229455649852753, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.029954276978969574, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033164799213409424, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031019892543554306, "mask": 0.0}, "tags": {"value": null, "prob": 0.03222748637199402, "mask": 0.0}, "comments": {"value": null, "prob": 0.03060670755803585, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.030347809195518494, "mask": 0.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.030384479090571404, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03229455649852753, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.029954276978969574, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033164799213409424, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031019892543554306, "mask": 0.0}, "tags": {"value": null, "prob": 0.03222748637199402, "mask": 0.0}, "comments": {"value": null, "prob": 0.03060670755803585, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.030347809195518494, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.030384479090571404, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03229455649852753, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.029954276978969574, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033164799213409424, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031019892543554306, "mask": 0.0}, "tags": {"value": null, "prob": 0.03222748637199402, "mask": 0.0}, "comments": {"value": null, "prob": 0.03060670755803585, "mask": 0.0}}], "prob": 0.16440629959106445, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.4306970536708832, "mask": 0.0}, "terminate": {"prob": 0.008377846330404282, "mask": 0, "value": null}}, "cls_probs": [0.4083015024662018, 0.4444138705730438, 0.147284597158432], "s_value": 0.5086225271224976, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.09533695131540298, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.17969366908073425, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.030322737991809845, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.03039637580513954, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03230416029691696, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.02994992956519127, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033227112144231796, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03106768988072872, "mask": 0.0}, "tags": {"value": null, "prob": 0.0322495698928833, "mask": 0.0}, "comments": {"value": null, "prob": 0.03048241324722767, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.030322737991809845, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.03039637580513954, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03230416029691696, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.02994992956519127, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.033227112144231796, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03106768988072872, "mask": 0.0}, "tags": {"value": null, "prob": 0.0322495698928833, "mask": 0.0}, "comments": {"value": null, "prob": 0.03048241324722767, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.030322737991809845, "mask": 0.0, "selected": true}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.03039637580513954, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03230416029691696, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.02994992956519127, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033227112144231796, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03106768988072872, "mask": 0.0}, "tags": {"value": null, "prob": 0.0322495698928833, "mask": 0.0}, "comments": {"value": null, "prob": 0.03048241324722767, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.030322737991809845, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03039637580513954, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03230416029691696, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.02994992956519127, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033227112144231796, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03106768988072872, "mask": 0.0}, "tags": {"value": null, "prob": 0.0322495698928833, "mask": 0.0}, "comments": {"value": null, "prob": 0.03048241324722767, "mask": 0.0}}], "prob": 0.18270371854305267, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.529816746711731, "mask": 0.0}, "terminate": {"prob": 0.01244884729385376, "mask": 0, "value": null}}, "cls_probs": [0.41946274042129517, 0.438147634267807, 0.1423896998167038], "s_value": 0.5097766518592834, "orig_id": 2494, "true_y": 1, "last_cost": 0.20000000298023224, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.09713533520698547, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.17957009375095367, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03127843514084816, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.0313597097992897, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03330758959054947, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.030887659639120102, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.034254126250743866, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.032050032168626785, "mask": 0.0}, "tags": {"value": null, "prob": 0.033260710537433624, "mask": 0.0}, "comments": {"value": null, "prob": 0.031419336795806885, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03127843514084816, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.0313597097992897, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03330758959054947, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.030887659639120102, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.034254126250743866, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032050032168626785, "mask": 0.0}, "tags": {"value": null, "prob": 0.033260710537433624, "mask": 0.0}, "comments": {"value": null, "prob": 0.031419336795806885, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.031360141932964325, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03331257030367851, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.030886413529515266, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034267157316207886, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03206097334623337, "mask": 0.0}, "tags": {"value": null, "prob": 0.033267438411712646, "mask": 0.0}, "comments": {"value": null, "prob": 0.031392499804496765, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.03127843514084816, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.0313597097992897, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03330758959054947, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.030887659639120102, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034254126250743866, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032050032168626785, "mask": 0.0}, "tags": {"value": null, "prob": 0.033260710537433624, "mask": 0.0}, "comments": {"value": null, "prob": 0.031419336795806885, "mask": 0.0}}], "prob": 0.18045344948768616, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.5290499925613403, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013791114091873169, "mask": 0, "value": null}}, "cls_probs": [0.42283129692077637, 0.4344145357608795, 0.1427541822195053], "s_value": 0.5081359148025513, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.10123518109321594, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.18972563743591309, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.031228702515363693, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.031319666653871536, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03328702971339226, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.03084169514477253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03430493548512459, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.032087601721286774, "mask": 0.0}, "tags": {"value": null, "prob": 0.033393844962120056, "mask": 0.0}, "comments": {"value": null, "prob": 0.03134165704250336, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.031228702515363693, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.031319666653871536, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03328702971339226, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.03084169514477253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03430493548512459, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032087601721286774, "mask": 0.0}, "tags": {"value": null, "prob": 0.033393844962120056, "mask": 0.0}, "comments": {"value": null, "prob": 0.03134165704250336, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.03132009878754616, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.033292006701231, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.030840452760457993, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0343179851770401, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03209855780005455, "mask": 0.0}, "tags": {"value": null, "prob": 0.03340059891343117, "mask": 0.0}, "comments": {"value": null, "prob": 0.03131488338112831, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.031228702515363693, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.031319666653871536, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03328702971339226, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03084169514477253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03430493548512459, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032087601721286774, "mask": 0.0}, "tags": {"value": null, "prob": 0.033393844962120056, "mask": 0.0}, "comments": {"value": null, "prob": 0.03134165704250336, "mask": 0.0}}], "prob": 0.19499976933002472, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.5000268220901489, "mask": 0.1666666716337204, "selected": true}, "terminate": {"prob": 0.014012631960213184, "mask": 0, "value": null}}, "cls_probs": [0.4160637855529785, 0.44803088903427124, 0.13590525090694427], "s_value": 0.5086936950683594, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.800000004470348}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.10468874871730804, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.19526787102222443, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.031229810789227486, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.03131330758333206, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.033259619027376175, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.030848952010273933, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03426545485854149, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03208703175187111, "mask": 0.0}, "tags": {"value": null, "prob": 0.033447518944740295, "mask": 0.0}, "comments": {"value": null, "prob": 0.031353726983070374, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.031229810789227486, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.03131330758333206, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.033259619027376175, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.030848952010273933, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03426545485854149, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03208703175187111, "mask": 0.0}, "tags": {"value": null, "prob": 0.033447518944740295, "mask": 0.0}, "comments": {"value": null, "prob": 0.031353726983070374, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.03131373971700668, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03326458856463432, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.030847709625959396, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03427848964929581, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032097987830638885, "mask": 0.0}, "tags": {"value": null, "prob": 0.0334542840719223, "mask": 0.0}, "comments": {"value": null, "prob": 0.03132694587111473, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.031229810789227486, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03131330758333206, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.033259619027376175, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.030848952010273933, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03426545485854149, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03208703175187111, "mask": 0.0}, "tags": {"value": null, "prob": 0.033447518944740295, "mask": 0.0}, "comments": {"value": null, "prob": 0.031353726983070374, "mask": 0.0}}], "prob": 0.20546220242977142, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.25, "mask": 0.0}}], "prob": 0.4796883761882782, "mask": 0.3333333432674408, "selected": true}, "terminate": {"prob": 0.014892778359353542, "mask": 0, "value": null}}, "cls_probs": [0.4135490655899048, 0.4483809173107147, 0.1380700170993805], "s_value": 0.5039331912994385, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.9000000059604645}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.10629790276288986, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.1987590491771698, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.031236935406923294, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.03132626786828041, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03324620798230171, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.030843595042824745, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.034256644546985626, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03209555521607399, "mask": 0.0}, "tags": {"value": null, "prob": 0.03343130275607109, "mask": 0.0}, "comments": {"value": null, "prob": 0.031370680779218674, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.031236935406923294, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.03132626786828041, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03324620798230171, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.030843595042824745, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.034256644546985626, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03209555521607399, "mask": 0.0}, "tags": {"value": null, "prob": 0.03343130275607109, "mask": 0.0}, "comments": {"value": null, "prob": 0.031370680779218674, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.03132670000195503, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03325117751955986, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.030842352658510208, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034269675612449646, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032106511294841766, "mask": 0.0}, "tags": {"value": null, "prob": 0.0334380678832531, "mask": 0.0}, "comments": {"value": null, "prob": 0.031343888491392136, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.031236935406923294, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03132626786828041, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03324620798230171, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.030843595042824745, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034256644546985626, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03209555521607399, "mask": 0.0}, "tags": {"value": null, "prob": 0.03343130275607109, "mask": 0.0}, "comments": {"value": null, "prob": 0.031370680779218674, "mask": 0.0}}], "prob": 0.21012075245380402, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}], "prob": 0.46935808658599854, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.015464235097169876, "mask": 0, "value": null}}, "cls_probs": [0.4165547788143158, 0.44756457209587097, 0.13588058948516846], "s_value": 0.5025654435157776, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.000000007450581}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.11186949163675308, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.2034674435853958, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.031150801107287407, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.031230714172124863, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.033167000859975815, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.030886216089129448, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03422064706683159, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03210830315947533, "mask": 0.0}, "tags": {"value": null, "prob": 0.033856455236673355, "mask": 0.0}, "comments": {"value": null, "prob": 0.031165478751063347, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.031150801107287407, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.031230714172124863, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.033167000859975815, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.030886216089129448, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03422064706683159, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03210830315947533, "mask": 0.0}, "tags": {"value": null, "prob": 0.033856455236673355, "mask": 0.0}, "comments": {"value": null, "prob": 0.031165478751063347, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.031231146305799484, "mask": 0.0, "selected": true}, "score": {"value": 0.04, "prob": 0.03317195549607277, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.030884968116879463, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03423366695642471, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0321192666888237, "mask": 0.0}, "tags": {"value": null, "prob": 0.03386330232024193, "mask": 0.0}, "comments": {"value": null, "prob": 0.031138857826590538, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.031150801107287407, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.031230714172124863, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.033167000859975815, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.030886216089129448, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03422064706683159, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03210830315947533, "mask": 0.0}, "tags": {"value": null, "prob": 0.033856455236673355, "mask": 0.0}, "comments": {"value": null, "prob": 0.031165478751063347, "mask": 0.0}}], "prob": 0.2211487591266632, "mask": 0.03125, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.4473000168800354, "mask": 0.6666666865348816}, "terminate": {"prob": 0.016214266419410706, "mask": 0, "value": null}}, "cls_probs": [0.3989986777305603, 0.45169171690940857, 0.14930954575538635], "s_value": 0.4941813349723816, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 5.100000008940697}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.11388827860355377, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.20266440510749817, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03212621062994003, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.032206304371356964, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03422423452138901, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.03188474103808403, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03534962236881256, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03317360207438469, "mask": 0.0}, "tags": {"value": null, "prob": 0.035083841532468796, "mask": 0.0}, "comments": {"value": null, "prob": 0.03202645853161812, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03212621062994003, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.032206304371356964, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03422423452138901, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.03188474103808403, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03534962236881256, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03317360207438469, "mask": 0.0}, "tags": {"value": null, "prob": 0.035083841532468796, "mask": 0.0}, "comments": {"value": null, "prob": 0.03202645853161812, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03423698991537094, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.03188074752688408, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03538290038704872, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.033198416233062744, "mask": 0.0}, "tags": {"value": null, "prob": 0.03510817512869835, "mask": 0.0}, "comments": {"value": null, "prob": 0.03196771815419197, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.03212621062994003, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.032206304371356964, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03422423452138901, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03188474103808403, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03534962236881256, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03317360207438469, "mask": 0.0}, "tags": {"value": null, "prob": 0.035083841532468796, "mask": 0.0, "selected": true}, "comments": {"value": null, "prob": 0.03202645853161812, "mask": 0.0}}], "prob": 0.2209588885307312, "mask": 0.0625, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.4450957775115967, "mask": 0.6666666865348816}, "terminate": {"prob": 0.017392722889780998, "mask": 0, "value": null}}, "cls_probs": [0.3996608853340149, 0.448752760887146, 0.1515863537788391], "s_value": 0.4944489896297455, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000008940697}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.11652472615242004, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.20336896181106567, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03208025544881821, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.032183729112148285, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.034198611974716187, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.031861331313848495, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0353904627263546, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0332166962325573, "mask": 0.0}, "tags": {"value": null, "prob": 0.03523813933134079, "mask": 0.0}, "comments": {"value": null, "prob": 0.03189627826213837, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03208025544881821, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.032183729112148285, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.034198611974716187, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.031861331313848495, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0353904627263546, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0332166962325573, "mask": 0.0}, "tags": {"value": null, "prob": 0.03523813933134079, "mask": 0.0}, "comments": {"value": null, "prob": 0.03189627826213837, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.034211646765470505, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.031857606023550034, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03542337194085121, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03324087709188461, "mask": 0.0}, "tags": {"value": null, "prob": 0.03525879606604576, "mask": 0.0}, "comments": {"value": null, "prob": 0.03183848783373833, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.032078564167022705, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03218105435371399, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03420338034629822, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03185414522886276, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03539643436670303, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03321775794029236, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.03521362692117691, "mask": 0.0}, "comments": {"value": null, "prob": 0.031893160194158554, "mask": 0.0}}], "prob": 0.21956667304039001, "mask": 0.0625}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.44138896465301514, "mask": 0.6666666865348816}, "terminate": {"prob": 0.01915064826607704, "mask": 0, "value": null}}, "cls_probs": [0.3993389904499054, 0.45085781812667847, 0.1498032957315445], "s_value": 0.49350130558013916, "orig_id": 2494, "true_y": 1, "last_cost": 0.5, "total_cost": 6.100000008940697}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.1493578553199768, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03198402374982834, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.032154664397239685, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.034184638410806656, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.031809572130441666, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0354158915579319, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03322690725326538, "mask": 0.0}, "tags": {"value": null, "prob": 0.03543248400092125, "mask": 0.0}, "comments": {"value": null, "prob": 0.03182601183652878, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03198402374982834, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.032154664397239685, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.034184638410806656, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.031809572130441666, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0354158915579319, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03322690725326538, "mask": 0.0}, "tags": {"value": null, "prob": 0.03543248400092125, "mask": 0.0}, "comments": {"value": null, "prob": 0.03182601183652878, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03419766575098038, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.0318058505654335, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0354488268494606, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.033251095563173294, "mask": 0.0}, "tags": {"value": null, "prob": 0.035453252494335175, "mask": 0.0}, "comments": {"value": null, "prob": 0.031768351793289185, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.031982336193323135, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03215198963880539, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03418940305709839, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03180239722132683, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03542187064886093, "mask": 0.0, "selected": true}, "favorites": {"value": 0.0, "prob": 0.03322796896100044, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.035407837480306625, "mask": 0.0}, "comments": {"value": null, "prob": 0.03182290121912956, "mask": 0.0}}], "prob": 0.2629110515117645, "mask": 0.0625, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.5561051964759827, "mask": 0.6666666865348816}, "terminate": {"prob": 0.03162592276930809, "mask": 0, "value": null}}, "cls_probs": [0.4022824764251709, 0.4502570629119873, 0.1474604457616806], "s_value": 0.4924736022949219, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.600000008940697}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.14954274892807007, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.033136036247015, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.03331376984715462, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.035446323454380035, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.03296469897031784, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03674804046750069, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03446047380566597, "mask": 0.0}, "tags": {"value": null, "prob": 0.036807477474212646, "mask": 0.0}, "comments": {"value": null, "prob": 0.032925404608249664, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.033136036247015, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.03331376984715462, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.035446323454380035, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.03296469897031784, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03674804046750069, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03446047380566597, "mask": 0.0}, "tags": {"value": null, "prob": 0.036807477474212646, "mask": 0.0}, "comments": {"value": null, "prob": 0.032925404608249664, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03545983135700226, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.03296084329485893, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03678221255540848, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03448555991053581, "mask": 0.0}, "tags": {"value": null, "prob": 0.03682905063033104, "mask": 0.0}, "comments": {"value": null, "prob": 0.032865747809410095, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.03313194587826729, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03330918401479721, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.035459455102682114, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03295285254716873, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.034468572586774826, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.03678610920906067, "mask": 0.0}, "comments": {"value": null, "prob": 0.032904136925935745, "mask": 0.0}}], "prob": 0.26132944226264954, "mask": 0.09375}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.5565655827522278, "mask": 0.6666666865348816, "selected": true}, "terminate": {"prob": 0.03256216272711754, "mask": 0, "value": null}}, "cls_probs": [0.4027971923351288, 0.44890478253364563, 0.14829808473587036], "s_value": 0.4917682409286499, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.700000010430813}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.1571529358625412, "mask": 0.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03309871628880501, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.033276744186878204, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03539032116532326, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.032979559153318405, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.036689139902591705, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03445640951395035, "mask": 0.0}, "tags": {"value": null, "prob": 0.03700298070907593, "mask": 0.0}, "comments": {"value": null, "prob": 0.032875027507543564, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03309871628880501, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.033276744186878204, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03539032116532326, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.032979559153318405, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.036689139902591705, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03445640951395035, "mask": 0.0}, "tags": {"value": null, "prob": 0.03700298070907593, "mask": 0.0}, "comments": {"value": null, "prob": 0.032875027507543564, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03540381044149399, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.0329756997525692, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03672325611114502, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03448149189352989, "mask": 0.0}, "tags": {"value": null, "prob": 0.03702466934919357, "mask": 0.0}, "comments": {"value": null, "prob": 0.032815463840961456, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.0330946259200573, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.033272165805101395, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03540343418717384, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.032967709004879, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.034464508295059204, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.036981500685214996, "mask": 0.0}, "comments": {"value": null, "prob": 0.03285379335284233, "mask": 0.0}}], "prob": 0.27413409948349, "mask": 0.09375}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 1.0, "mask": 0.0, "selected": true}}, {"badge": {"value": "Promoter", "prob": 0.0, "mask": 1.0}}], "prob": 0.5325469970703125, "mask": 0.8333333134651184, "selected": true}, "terminate": {"prob": 0.03616603836417198, "mask": 0, "value": null}}, "cls_probs": [0.3950403034687042, 0.44969442486763, 0.1552652269601822], "s_value": 0.4846478998661041, "orig_id": 2494, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.800000011920929}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.24695250391960144, "mask": 0.0, "selected": true}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.033610083162784576, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.0340188704431057, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.035478994250297546, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.03296668082475662, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03645162284374237, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03468214347958565, "mask": 0.0}, "tags": {"value": null, "prob": 0.035169072449207306, "mask": 0.0}, "comments": {"value": null, "prob": 0.03364593908190727, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.033610083162784576, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.0340188704431057, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.035478994250297546, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.03296668082475662, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03645162284374237, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03468214347958565, "mask": 0.0}, "tags": {"value": null, "prob": 0.035169072449207306, "mask": 0.0}, "comments": {"value": null, "prob": 0.03364593908190727, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03549251705408096, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.032962825149297714, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.036485519260168076, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.034707389771938324, "mask": 0.0}, "tags": {"value": null, "prob": 0.035189688205718994, "mask": 0.0}, "comments": {"value": null, "prob": 0.03358497843146324, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.03360592946410179, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.03401418775320053, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03549214079976082, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.03295483440160751, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.03469029441475868, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.03514866158366203, "mask": 0.0}, "comments": {"value": null, "prob": 0.033624209463596344, "mask": 0.0}}], "prob": 0.21529743075370789, "mask": 0.09375}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Promoter", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.5377500057220459, "mask": 0, "value": null}}, "cls_probs": [0.4909142255783081, 0.40866750478744507, 0.10041824728250504], "s_value": 0.5058144330978394, "orig_id": 2494, "true_y": 1, "last_cost": 1.0, "total_cost": 6.900000013411045}, {"sample": {"about_me": {"value": "<p>Computational Science PhD student<br>\nUniversity of Texas El Paso<br>\nClimate &amp; Energy Science Student Organization President<br>\nNational Energy Technology Laboratory Intern<br>\nNSF LSAMP Fellow</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.17, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0054, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Generating a log-normally distributed pseudorandomly generated data set", "prob": 0.03351211920380592, "mask": 0.0}, "body": {"value": "<p>The feedback I received from my initial post seemed to indicate that my question was ill-posed.  Hence, I would like to clarify what I am doing and how I hope to achieve it.</p>\n\n<p>I'm running some simulations on models with a material parameter $g(x)$ whose values, according to the literature I've read, are log-normally distributed in the spatial.  The material parameter $g(x)$ corresponds to a physical property that is strictly positive and experimentally determined to be never larger or smaller than some threshold values $g_{\\\\max},g_{\\\\min}&gt;0$.  When I run my simulations, I would like to generate a distribution of values $g$ over the spatial domain such that</p>\n\n<ol>\n<li>All values of $g$ are strictly in the interval $[g_{\\\\min},g_{\\\\max}]$</li>\n<li>The distribution of values of $g$ is as close to a log-normal distribution as possible for a given mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n</ol>\n\n<p>I have at my disposal a pseudorandom number generator that creates a vector of normally distributed values with a given mean $\\\\mu$ and standard deviation $\\\\sigma$.  I know that if the data set $X$ is normally distributed, then an exponential transformation of the data (in other words, $\\\\exp(X)$) should produce a log normal distribution.  </p>\n\n<p>I hypothesize that I can create this log normal distribution by the following process:</p>\n\n<ol>\n<li>Make a normal distribution of values X with mean $\\\\mu$ and standard deviation $\\\\sigma$.</li>\n<li>Exponentiate the values to generate a new set $Y=\\\\exp(X)$.</li>\n<li>Rescale and shift the values of $Y$ so that the minimum value of the new set is exactly $g_{\\\\min}$ and the maximum is $g_{\\\\max}$.  I would do so by creating a new data set $Z=a + (b-a)Y$.</li>\n</ol>\n\n<p>Would this give me the results I desire?  That is, in the end, would the set $Z$ be log-normally distributed with all values lying in the interval $[g_{\\\\min},g_{\\\\max}]$?</p>\n", "prob": 0.033978771418333054, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03542482480406761, "mask": 0.0}, "views": {"value": 0.025, "prob": 0.03293052688241005, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.036496005952358246, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03476313501596451, "mask": 0.0}, "tags": {"value": null, "prob": 0.035466600209474564, "mask": 0.0}, "comments": {"value": null, "prob": 0.03342787176370621, "mask": 0.0}}, {"title": {"value": "On the distribution of exam scores and the use of t-tests", "prob": 0.03351211920380592, "mask": 0.0}, "body": {"value": "<p>A while back, I did some educational research where I compared exam scores between two groups.  I remember reading somewhere that educational exams such as these tend to follow a normal distribution.  Of course, normal distributions must necessarily be infinite and educational exam scores are not.  Is it mathematically correct to say that these scores follow a \"truncated normal distribution\" rather than a traditional normal distribution?  Does this truncation preclude one from being able to use a t-test to compare two groups?</p>\n", "prob": 0.033978771418333054, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03542482480406761, "mask": 0.0}, "views": {"value": 0.0076, "prob": 0.03293052688241005, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.036496005952358246, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03476313501596451, "mask": 0.0}, "tags": {"value": null, "prob": 0.035466600209474564, "mask": 0.0}, "comments": {"value": null, "prob": 0.03342787176370621, "mask": 0.0}}, {"title": {"value": "Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>\n\n<p>I'm trying to solve a stochastic partial differential equation of the form $$\\\\alpha(\\\\omega)\\\\nabla^2u=f$$\nwhere $\\\\alpha(\\\\omega)$ represents a random field that is log-normally distributed, i.e. it has a probability density function $$pdf(x)=\\\\frac{1}{x\\\\sqrt{2\\\\pi\\\\sigma}}e^{-\\\\frac{(ln(x)-\\\\mu)^2}{2\\\\sigma^2}}.$$</p>\n\n<p>I want to represent the solution of this problem as a polynomial chaos expansion $$u=\\\\sum_{i=0}^p u_i(x)\\\\Psi_i(\\\\xi)$$ where $u_i(x)$ is a deterministic coefficient and $\\\\Psi_i(\\\\xi)$ are orthogonal polynomials in terms of a random variable $\\\\xi$ with the same log-normal probability density function.</p>\n\n<p>According to <a href=\"http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf\" rel=\"nofollow\">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution $u$.  For instance, hermite polynomials are optimal for gaussian distributions, legendre polynomials for uniform distributions, laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>\n\n<p>What is the corresponding optimal polynomial basis for log-normal distributions? </p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.03543832525610924, "mask": 0.0}, "views": {"value": 0.0187, "prob": 0.03292667493224144, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.036529943346977234, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03478844463825226, "mask": 0.0}, "tags": {"value": null, "prob": 0.03548739105463028, "mask": 0.0}, "comments": {"value": null, "prob": 0.033367305994033813, "mask": 0.0}}, {"title": {"value": "Determining the Bounds on a Random Field, given Mean and Covariance function", "prob": 0.033507976680994034, "mask": 0.0}, "body": {"value": "<p>I'm reading <a href=\"http://inside.mines.edu/~pconstan/docs/constantine-primer.pdf\" rel=\"nofollow\"><em>this tutorial</em></a> on Galerkin methods for stochastic partial differential equations, Example 2.  In this example, a random field $\\\\alpha(x,\\\\omega)$ has a mean $\\\\bar{\\\\alpha}=10$ and covariance function $C_\\\\alpha(x_1,x_2)=e^{-|x_1-x_2|}$.   </p>\n\n<p>According to the author, the mean and covariance functions were chosen so that the random field $\\\\alpha(x,\\\\omega)&gt;0$.  I understand the need to have $\\\\alpha&gt;0$, but I'm not sure how to prove that this is true.  How could I derive the bounds on $\\\\alpha(x,\\\\omega)$ given $\\\\bar{\\\\alpha}$ and $C_{\\\\alpha}(x_1,x_2)$? </p>\n", "prob": 0.033974096179008484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03543794900178909, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.032918695360422134, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.034771304577589035, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "covariance", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "stochastic-processes", "prob": 0.5, "mask": 0.0}}], "prob": 0.03544601425528526, "mask": 0.0}, "comments": {"value": null, "prob": 0.03340627998113632, "mask": 0.0}}], "prob": 0.2137397974729538, "mask": 0.09375}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Promoter", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.7862601280212402, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4817797839641571, 0.41862696409225464, 0.09959322959184647], "s_value": 0.5019775629043579, "orig_id": 2494, "true_y": 1, "last_cost": 0.0, "total_cost": 7.900000013411045}], [{"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 2535, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.06145130470395088, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12017360329627991, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.08465293794870377, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.06875791400671005, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.09526240080595016, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1043059378862381, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.1379939168691635, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.06993784755468369, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15130428969860077, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.3652646839618683, "mask": 0.0}, "terminate": {"prob": 0.007173129823058844, "mask": 0, "value": null}}, "cls_probs": [0.4319908320903778, 0.4118521213531494, 0.15615707635879517], "s_value": 0.5141831040382385, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.07997249811887741, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.11081865429878235, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.11725669354200363, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08332139998674393, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17172659933567047, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.42575690150260925, "mask": 0.0}, "terminate": {"prob": 0.011147220619022846, "mask": 0, "value": null}}, "cls_probs": [0.4420357644557953, 0.39594653248786926, 0.16201770305633545], "s_value": 0.5120828151702881, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.09088201075792313, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.12930943071842194, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09508557617664337, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18046920001506805, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.48940178751945496, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.014852000400424004, "mask": 0, "value": null}}, "cls_probs": [0.44194647669792175, 0.40196144580841064, 0.1560920923948288], "s_value": 0.5063738822937012, "orig_id": 2535, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.10021644830703735, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.14266838133335114, "mask": 0.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10446645319461823, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20810566842556, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.4246866703033447, "mask": 0.5}, "terminate": {"prob": 0.019856365397572517, "mask": 0, "value": null}}, "cls_probs": [0.44713181257247925, 0.3966023921966553, 0.15626581013202667], "s_value": 0.5014856457710266, "orig_id": 2535, "true_y": 1, "last_cost": 1.0, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.12395180761814117, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.18329352140426636, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.12612196803092957, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.5474172830581665, "mask": 0.5}, "terminate": {"prob": 0.01921536587178707, "mask": 0, "value": null}}, "cls_probs": [0.4544278383255005, 0.38915732502937317, 0.15641486644744873], "s_value": 0.49795812368392944, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.15516509115695953, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1558397263288498, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.6700732111930847, "mask": 0.5}, "terminate": {"prob": 0.01892194151878357, "mask": 0, "value": null}}, "cls_probs": [0.4322761297225952, 0.4145422577857971, 0.15318164229393005], "s_value": 0.48821166157722473, "orig_id": 2535, "true_y": 1, "last_cost": 1.0, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.21309460699558258, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.7500547170639038, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.036850690841674805, "mask": 0, "value": null}}, "cls_probs": [0.43440520763397217, 0.42093831300735474, 0.14465652406215668], "s_value": 0.48001188039779663, "orig_id": 2535, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.8356971740722656, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.16430285573005676, "mask": 0, "value": null}}, "cls_probs": [0.40691810846328735, 0.4616256654262543, 0.13145627081394196], "s_value": 0.4747008681297302, "orig_id": 2535, "true_y": 1, "last_cost": 0.5, "total_cost": 5.700000002980232}, {"sample": {"about_me": {"value": "CS PhD student in Philadelphia, PA.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.004, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3985908627510071, 0.46724337339401245, 0.13416579365730286], "s_value": 0.47266483306884766, "orig_id": 2535, "true_y": 1, "last_cost": 0.0, "total_cost": 6.200000002980232}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7842, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 7842, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.07510669529438019, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.10096142441034317, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1179569810628891, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1502375602722168, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0744071826338768, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18036718666553497, "mask": 0.0}, "badges": {"value": null, "prob": 0.2942897379398346, "mask": 0.0}, "terminate": {"prob": 0.0066732075065374374, "mask": 0, "value": null}}, "cls_probs": [0.42407476902008057, 0.41744476556777954, 0.1584804654121399], "s_value": 0.5154073238372803, "orig_id": 7842, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.11419669538736343, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.13067832589149475, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.14324931800365448, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1069258525967598, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.14082835614681244, "mask": 0.0}, "badges": {"value": null, "prob": 0.19093672931194305, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.17318478226661682, "mask": 0, "value": null}}, "cls_probs": [0.47575515508651733, 0.36542823910713196, 0.15881669521331787], "s_value": 0.5133767127990723, "orig_id": 7842, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.09727422893047333, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.10002683848142624, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.10611097514629364, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0873277485370636, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0958680734038353, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.5133921504020691, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4879782795906067, 0.34954479336738586, 0.16247694194316864], "s_value": 0.5116313099861145, "orig_id": 7842, "true_y": 1, "last_cost": 0.0, "total_cost": 2.5}], [{"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 2.7, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.05, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0654018297791481, "mask": 0.0, "selected": true}, "views": {"value": 2.7, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.05, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 2360, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.13764622807502747, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.09847594797611237, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.11362171918153763, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.1411479413509369, "mask": 0.0}, "down_votes": {"value": 0.05, "prob": 0.07544844597578049, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17735876142978668, "mask": 0.0}, "badges": {"value": null, "prob": 0.2480417937040329, "mask": 0.0}, "terminate": {"prob": 0.008259248919785023, "mask": 0, "value": null}}, "cls_probs": [0.41948139667510986, 0.4198268949985504, 0.16069172322750092], "s_value": 0.5136396288871765, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.1550167053937912, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.12554877996444702, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.15552279353141785, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.05, "prob": 0.08637535572052002, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18679282069206238, "mask": 0.0}, "badges": {"value": null, "prob": 0.27954918146133423, "mask": 0.0}, "terminate": {"prob": 0.011194380931556225, "mask": 0, "value": null}}, "cls_probs": [0.41982704401016235, 0.42548996210098267, 0.15468299388885498], "s_value": 0.5061590671539307, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.1858132928609848, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.15117193758487701, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.10847155004739761, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2286834418773651, "mask": 0.0}, "badges": {"value": null, "prob": 0.30592289566993713, "mask": 0.0}, "terminate": {"prob": 0.019936928525567055, "mask": 0, "value": null}}, "cls_probs": [0.42732346057891846, 0.41467052698135376, 0.1580059975385666], "s_value": 0.5060876607894897, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.2062782347202301, "mask": 0.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.16484501957893372, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2525559961795807, "mask": 0.0}, "badges": {"value": null, "prob": 0.35790517926216125, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.018415596336126328, "mask": 0, "value": null}}, "cls_probs": [0.4129784107208252, 0.42655810713768005, 0.16046349704265594], "s_value": 0.5022938251495361, "orig_id": 2360, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.1890352964401245, "mask": 0.0, "selected": true}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.13917535543441772, "mask": 0.0}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19771449267864227, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Citizen Patrol", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}], "prob": 0.4535614848136902, "mask": 0.0}, "terminate": {"prob": 0.020513396710157394, "mask": 0, "value": null}}, "cls_probs": [0.4178744852542877, 0.4232829213142395, 0.15884257853031158], "s_value": 0.5045655965805054, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.18548701703548431, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20594480633735657, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Citizen Patrol", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}], "prob": 0.4940141439437866, "mask": 0.0}, "terminate": {"prob": 0.1145540103316307, "mask": 0, "value": null}}, "cls_probs": [0.33393236994743347, 0.4870733916759491, 0.17899423837661743], "s_value": 0.5150967836380005, "orig_id": 2360, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15218332409858704, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.04545454680919647, "mask": 0.0, "selected": true}}, {"badge": {"value": "Citizen Patrol", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.04545454680919647, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.04545454680919647, "mask": 0.0}}], "prob": 0.44309893250465393, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.4047177731990814, "mask": 0, "value": null}}, "cls_probs": [0.38993528485298157, 0.44701579213142395, 0.16304893791675568], "s_value": 0.5049599409103394, "orig_id": 2360, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>Data Scientist at Elder Research.</p>\n\n<p>Data Mining, Machine Learning, Probability, Bayesian Statistics, Regression Analysis, Simulation</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 2.7, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.3111, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.121, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15858426690101624, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Citizen Patrol", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Nice Answer", "prob": 0.0476190485060215, "mask": 0.0}}, {"badge": {"value": "Enlightened", "prob": 0.0476190485060215, "mask": 0.0}}], "prob": 0.44954290986061096, "mask": 0.04545454680919647}, "terminate": {"prob": 0.3918728530406952, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3581186532974243, 0.454689085483551, 0.18719227612018585], "s_value": 0.4952351748943329, "orig_id": 2360, "true_y": 1, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.07328308373689651, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.1409580111503601, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.09756907820701599, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.15240715444087982, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07209322601556778, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17674970626831055, "mask": 0.0}, "badges": {"value": null, "prob": 0.28212639689445496, "mask": 0.0}, "terminate": {"prob": 0.004813333507627249, "mask": 0, "value": null}}, "cls_probs": [0.3990987539291382, 0.44010719656944275, 0.16079400479793549], "s_value": 0.5127114057540894, "orig_id": 5066, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.15751922130584717, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.11045534908771515, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1634463518857956, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08495456725358963, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19188760221004486, "mask": 0.0}, "badges": {"value": null, "prob": 0.2839216887950897, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007815234363079071, "mask": 0, "value": null}}, "cls_probs": [0.39764922857284546, 0.44380563497543335, 0.1585451066493988], "s_value": 0.5039498805999756, "orig_id": 5066, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.15029466152191162, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.10517062246799469, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1537342071533203, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08075176179409027, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17052268981933594, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.33147117495536804, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008054916746914387, "mask": 0, "value": null}}, "cls_probs": [0.3997785747051239, 0.44646763801574707, 0.15375381708145142], "s_value": 0.504787802696228, "orig_id": 5066, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.21561451256275177, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.15609435737133026, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.2216549515724182, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.12683385610580444, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2601131200790405, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.019689233973622322, "mask": 0, "value": null}}, "cls_probs": [0.4057800769805908, 0.42838746309280396, 0.16583240032196045], "s_value": 0.4881938099861145, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.2717474400997162, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.19658109545707703, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16226716339588165, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.3421363830566406, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.027267880737781525, "mask": 0, "value": null}}, "cls_probs": [0.40920940041542053, 0.4252853989601135, 0.16550524532794952], "s_value": 0.4892650246620178, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.3276977837085724, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.23480260372161865, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.40796515345573425, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.029534468427300453, "mask": 0, "value": null}}, "cls_probs": [0.3949851989746094, 0.4360778331756592, 0.16893690824508667], "s_value": 0.4868864417076111, "orig_id": 5066, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.34079763293266296, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.24294105172157288, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.1208345964550972, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.12118595093488693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.12799592316150665, "mask": 0.0}, "views": {"value": 0.0033, "prob": 0.1201753243803978, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13174816966056824, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12458226829767227, "mask": 0.0}, "tags": {"value": null, "prob": 0.1323547214269638, "mask": 0.0}, "comments": {"value": null, "prob": 0.12112311273813248, "mask": 0.0}}], "prob": 0.385315477848053, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.030945824459195137, "mask": 0, "value": null}}, "cls_probs": [0.3856026828289032, 0.4525715410709381, 0.16182579100131989], "s_value": 0.4871959984302521, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.3726004362106323, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.1209690272808075, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.12135666608810425, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.12825675308704376, "mask": 0.0}, "views": {"value": 0.0033, "prob": 0.1200074777007103, "mask": 0.0, "selected": true}, "answers": {"value": 0.1, "prob": 0.13191772997379303, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12454579025506973, "mask": 0.0}, "tags": {"value": null, "prob": 0.1314331293106079, "mask": 0.0}, "comments": {"value": null, "prob": 0.12151340395212173, "mask": 0.0}}], "prob": 0.5712317824363708, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.05616789683699608, "mask": 0, "value": null}}, "cls_probs": [0.39675816893577576, 0.44714316725730896, 0.15609866380691528], "s_value": 0.481259822845459, "orig_id": 5066, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.34617167711257935, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.13752694427967072, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.13776572048664093, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.1446732133626938, "mask": 0.0}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.14987604320049286, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14289143681526184, "mask": 0.0}, "tags": {"value": null, "prob": 0.15282365679740906, "mask": 0.0}, "comments": {"value": null, "prob": 0.13444295525550842, "mask": 0.0}}], "prob": 0.6036385297775269, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.05018980801105499, "mask": 0, "value": null}}, "cls_probs": [0.40818291902542114, 0.4239450693130493, 0.16787202656269073], "s_value": 0.4789102375507355, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 5.700000002980232}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.1373918056488037, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.13776500523090363, "mask": 0.0, "selected": true}, "score": {"value": 0.0, "prob": 0.14495623111724854, "mask": 0.0}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.15005917847156525, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14266164600849152, "mask": 0.0}, "tags": {"value": null, "prob": 0.15226396918296814, "mask": 0.0}, "comments": {"value": null, "prob": 0.1349022388458252, "mask": 0.0}}], "prob": 0.9068119525909424, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.09318806231021881, "mask": 0, "value": null}}, "cls_probs": [0.41175228357315063, 0.42801788449287415, 0.16022980213165283], "s_value": 0.47282934188842773, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 6.200000002980232}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.15942734479904175, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.16792051494121552, "mask": 0.0}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.17415520548820496, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.16618895530700684, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.17792874574661255, "mask": 0.0}, "comments": {"value": null, "prob": 0.15437930822372437, "mask": 0.0}}], "prob": 0.8766734004020691, "mask": 0.25, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.12332658469676971, "mask": 0, "value": null}}, "cls_probs": [0.4184020459651947, 0.41029348969459534, 0.17130446434020996], "s_value": 0.47236084938049316, "orig_id": 5066, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 6.700000002980232}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.19125807285308838, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.20092876255512238, "mask": 0.0, "selected": true}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.20916204154491425, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.21626128256320953, "mask": 0.0}, "comments": {"value": null, "prob": 0.18238990008831024, "mask": 0.0}}], "prob": 0.8805814981460571, "mask": 0.375, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.11941851675510406, "mask": 0, "value": null}}, "cls_probs": [0.4165761172771454, 0.3950170874595642, 0.188406839966774], "s_value": 0.46777141094207764, "orig_id": 5066, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 6.800000004470348}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.23746474087238312, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.2632323205471039, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.27880433201789856, "mask": 0.0}, "comments": {"value": null, "prob": 0.22049857676029205, "mask": 0.0, "selected": true}}], "prob": 0.862654447555542, "mask": 0.5, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.1373455822467804, "mask": 0, "value": null}}, "cls_probs": [0.40909257531166077, 0.3872746229171753, 0.20363280177116394], "s_value": 0.4727994203567505, "orig_id": 5066, "true_y": 0, "last_cost": 0.5, "total_cost": 6.9000000059604645}, {"sample": {"about_me": {"value": "<p>Currently a Ph.D. student in Software Engineering, among the other things.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Correlated variables in kmeans clustering", "prob": 0.24199965596199036, "mask": 0.0}, "body": {"value": "<p>I have a dataset with 3 variables: A, B and C. Now, A and B are ordinal variables (i.e.; the result of two questions measured using a 5-point Likert), whereas B is continuous.</p>\n\n<p>A and B are also correlated, Spearman rho = .50, p-value = 0.0046</p>\n\n<p>I want to partition my dataset in 3 cluster using kmeans (the default R implementation). Does the fact that some of the variables in my dataset are correlated violates any assumptions for running the algorithm?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0033, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.2655031085014343, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.2817539870738983, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.0, "prob": 0.4668671190738678, "mask": 0.0}, "text": {"value": "No, k-means does not require uncorrelated variables. It, however, assumes that _clusters_ in the data are convex and more or less spherical.", "prob": 0.5331329703330994, "mask": 0.0}}], "prob": 0.21074320375919342, "mask": 0.0}}], "prob": 0.7948464751243591, "mask": 0.5}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.20515352487564087, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.48316553235054016, 0.35422953963279724, 0.1626049429178238], "s_value": 0.47122904658317566, "orig_id": 5066, "true_y": 0, "last_cost": 0.0, "total_cost": 7.4000000059604645}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7992, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 7992, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.07284489274024963, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13852131366729736, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.11343514919281006, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14526531100273132, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07250678539276123, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1709306240081787, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.2798885107040405, "mask": 0.0}, "terminate": {"prob": 0.006607507821172476, "mask": 0, "value": null}}, "cls_probs": [0.42068615555763245, 0.4233361780643463, 0.15597765147686005], "s_value": 0.5146921277046204, "orig_id": 7992, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.08647037297487259, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16510194540023804, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.13912053406238556, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.17724761366844177, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.08390016108751297, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3416052460670471, "mask": 0.0}, "terminate": {"prob": 0.006554099265486002, "mask": 0, "value": null}}, "cls_probs": [0.42701786756515503, 0.4156690239906311, 0.15731310844421387], "s_value": 0.5127223134040833, "orig_id": 7992, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.10870414972305298, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.2039126604795456, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.17229101061820984, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10732360184192657, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.39749643206596375, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010272142477333546, "mask": 0, "value": null}}, "cls_probs": [0.43310898542404175, 0.40743228793144226, 0.15945877134799957], "s_value": 0.5114789605140686, "orig_id": 7992, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.18061202764511108, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.33749473094940186, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.28346002101898193, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.17871800065040588, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.019715197384357452, "mask": 0, "value": null}}, "cls_probs": [0.4415226876735687, 0.40204790234565735, 0.15642942488193512], "s_value": 0.505895733833313, "orig_id": 7992, "true_y": 0, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.4014473259449005, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.33220019936561584, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.22364544868469238, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.042707037180662155, "mask": 0, "value": null}}, "cls_probs": [0.4498778283596039, 0.3991222083568573, 0.1509999930858612], "s_value": 0.49974626302719116, "orig_id": 7992, "true_y": 0, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.09921696782112122, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0802745521068573, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8205084204673767, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5092129707336426, 0.3401052951812744, 0.15068168938159943], "s_value": 0.504798173904419, "orig_id": 7992, "true_y": 0, "last_cost": 0.0, "total_cost": 5.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 938, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 938, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.03980446234345436, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1401, "true_y": 2, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.07100486755371094, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.053532619029283524, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05739108845591545, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06583385914564133, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04188549518585205, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.5516119003295898, "mask": 0.0}, "posts": {"value": null, "prob": 0.06114558130502701, "mask": 0.0}, "badges": {"value": null, "prob": 0.07485679537057877, "mask": 0.0}, "terminate": {"prob": 0.022737743332982063, "mask": 0, "value": null}}, "cls_probs": [0.46140530705451965, 0.4083004891872406, 0.13029420375823975], "s_value": 0.5297609567642212, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.07568015903234482, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.056791335344314575, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06062546372413635, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07077920436859131, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.564860999584198, "mask": 0.0}, "posts": {"value": null, "prob": 0.06615178287029266, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.08183421194553375, "mask": 0.0}, "terminate": {"prob": 0.02327682450413704, "mask": 0, "value": null}}, "cls_probs": [0.4619877338409424, 0.4105303883552551, 0.1274818629026413], "s_value": 0.5302165150642395, "orig_id": 1401, "true_y": 2, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.06821702420711517, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05309559777379036, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.058772433549165726, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06633713096380234, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.6623178720474243, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.07105211168527603, "mask": 0.0}, "terminate": {"prob": 0.02020786702632904, "mask": 0, "value": null}}, "cls_probs": [0.45854097604751587, 0.39995405077934265, 0.14150504767894745], "s_value": 0.5193256139755249, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.1805223822593689, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1277732253074646, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.15067382156848907, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1895637810230255, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.34271442890167236, "mask": 0.0}, "terminate": {"prob": 0.008752311579883099, "mask": 0, "value": null}}, "cls_probs": [0.420009583234787, 0.41890159249305725, 0.16108885407447815], "s_value": 0.5066861510276794, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.20613162219524384, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.16844454407691956, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.212815061211586, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.40125223994255066, "mask": 0.0}, "terminate": {"prob": 0.011356553994119167, "mask": 0, "value": null}}, "cls_probs": [0.4231093227863312, 0.4224350154399872, 0.15445563197135925], "s_value": 0.500569224357605, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.2686755061149597, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2202245146036148, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.49220705032348633, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01889294572174549, "mask": 0, "value": null}}, "cls_probs": [0.42837709188461304, 0.4141528904438019, 0.1574699431657791], "s_value": 0.4996423125267029, "orig_id": 1401, "true_y": 2, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.22888384759426117, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1739538460969925, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.5779834389686584, "mask": 0.0}, "terminate": {"prob": 0.019178852438926697, "mask": 0, "value": null}}, "cls_probs": [0.4363346993923187, 0.4065181016921997, 0.15714716911315918], "s_value": 0.49974051117897034, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.2855711877346039, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.6951950192451477, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01923385076224804, "mask": 0, "value": null}}, "cls_probs": [0.4124007225036621, 0.43315237760543823, 0.15444692969322205], "s_value": 0.49191415309906006, "orig_id": 1401, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.9128401875495911, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08715979754924774, "mask": 0, "value": null}}, "cls_probs": [0.4193277955055237, 0.4268110692501068, 0.1538611799478531], "s_value": 0.4755568504333496, "orig_id": 1401, "true_y": 2, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>This area is intentionally left blank.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.42575740814208984, 0.42520618438720703, 0.14903642237186432], "s_value": 0.46964389085769653, "orig_id": 1401, "true_y": 2, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.07328308373689651, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1409580111503601, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09756907820701599, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.15240715444087982, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07209322601556778, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17674970626831055, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.28212639689445496, "mask": 0.0}, "terminate": {"prob": 0.004813333507627249, "mask": 0, "value": null}}, "cls_probs": [0.3990987539291382, 0.44010719656944275, 0.16079400479793549], "s_value": 0.5127114057540894, "orig_id": 754, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.08963814377784729, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16942733526229858, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.11810021847486496, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18814072012901306, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08511225879192352, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.34444013237953186, "mask": 0.0}, "terminate": {"prob": 0.005141121800988913, "mask": 0, "value": null}}, "cls_probs": [0.4052918553352356, 0.4295039772987366, 0.16520413756370544], "s_value": 0.5113533735275269, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.10551399737596512, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.14004555344581604, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.2187279462814331, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.10057689994573593, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.4284772574901581, "mask": 0.0}, "terminate": {"prob": 0.00665827514603734, "mask": 0, "value": null}}, "cls_probs": [0.4108120799064636, 0.429080605506897, 0.16010737419128418], "s_value": 0.5036683082580566, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.1399216651916504, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.18482919037342072, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13597767055034637, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5284108519554138, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010860699228942394, "mask": 0, "value": null}}, "cls_probs": [0.416553795337677, 0.42142313718795776, 0.16202299296855927], "s_value": 0.5023509860038757, "orig_id": 754, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.1142410933971405, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.15623551607131958, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11395687609910965, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.6050607562065125, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010505808517336845, "mask": 0, "value": null}}, "cls_probs": [0.42847907543182373, 0.41174042224884033, 0.15978051722049713], "s_value": 0.5003752708435059, "orig_id": 754, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.2940502464771271, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.3765091001987457, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.2898035943508148, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03963704779744148, "mask": 0, "value": null}}, "cls_probs": [0.4311144948005676, 0.40361979603767395, 0.16526572406291962], "s_value": 0.4870521128177643, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.46507593989372253, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.47048842906951904, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06443566083908081, "mask": 0, "value": null}}, "cls_probs": [0.43397000432014465, 0.41488853096961975, 0.15114149451255798], "s_value": 0.48058247566223145, "orig_id": 754, "true_y": 1, "last_cost": 1.0, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.844252347946167, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.1557476818561554, "mask": 0, "value": null}}, "cls_probs": [0.431033730506897, 0.4182594418525696, 0.15070682764053345], "s_value": 0.47243231534957886, "orig_id": 754, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>Hi, I'm Khanh, a seasoned software engineer based in VietNam</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4166322350502014, 0.43008294701576233, 0.15328477323055267], "s_value": 0.47027841210365295, "orig_id": 754, "true_y": 1, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1765, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 1765, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.07287237793207169, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.13854293525218964, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.11345067620277405, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.1452634632587433, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0725393146276474, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17091858386993408, "mask": 0.0}, "badges": {"value": null, "prob": 0.2797949016094208, "mask": 0.0}, "terminate": {"prob": 0.00661776028573513, "mask": 0, "value": null}}, "cls_probs": [0.4206060767173767, 0.4233916997909546, 0.15600226819515228], "s_value": 0.5146822929382324, "orig_id": 1765, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.08731821924448013, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.16289718449115753, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.13378529250621796, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08848588913679123, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20792540907859802, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.3096529543399811, "mask": 0.0}, "terminate": {"prob": 0.00993507169187069, "mask": 0, "value": null}}, "cls_probs": [0.42589399218559265, 0.4150506556034088, 0.1590554118156433], "s_value": 0.5138214230537415, "orig_id": 1765, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.10875964909791946, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.20395269989967346, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.17231976985931396, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10738920420408249, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.39727726578712463, "mask": 0.0}, "terminate": {"prob": 0.01030137948691845, "mask": 0, "value": null}}, "cls_probs": [0.4330603778362274, 0.4074535071849823, 0.1594860553741455], "s_value": 0.5114660263061523, "orig_id": 1765, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.13501061499118805, "mask": 0.0, "selected": true}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.20969508588314056, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13293558359146118, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5076953768730164, "mask": 0.0}, "terminate": {"prob": 0.014663301408290863, "mask": 0, "value": null}}, "cls_probs": [0.4389321208000183, 0.4049758315086365, 0.1560920923948288], "s_value": 0.5035340785980225, "orig_id": 1765, "true_y": 0, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2525632977485657, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.169957235455513, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5529604554176331, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.024519048631191254, "mask": 0, "value": null}}, "cls_probs": [0.43631792068481445, 0.4077111780643463, 0.15597090125083923], "s_value": 0.4956391751766205, "orig_id": 1765, "true_y": 0, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.19819827377796173, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.14345034956932068, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}], "prob": 0.6340773701667786, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.024274064227938652, "mask": 0, "value": null}}, "cls_probs": [0.44399672746658325, 0.40111881494522095, 0.154884472489357], "s_value": 0.49568894505500793, "orig_id": 1765, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackexchange.com. </p>\n\n<p>(I'm the artist formerly known around these parts as Abby T. Miller.)</p>\n\n<p><a href=\"http://stackexchange.com/users/dcdecb98f43a4398abc35095d7882464\">\n<img src=\"http://stackexchange.com/users/flair/dcdecb98f43a4398abc35095d7882464.png\" width=\"208\" height=\"58\" alt=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Abby T. Miller on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\n</a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.21253326535224915, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.15374240279197693, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Informed", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.6049823760986328, "mask": 0.25}, "terminate": {"prob": 0.028741929680109024, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4462490677833557, 0.39886224269866943, 0.15488874912261963], "s_value": 0.4935356676578522, "orig_id": 1765, "true_y": 0, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.03980446234345436, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4576, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.07574669271707535, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05651743710041046, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0612308531999588, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07065080851316452, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.044148724526166916, "mask": 0.0}, "website": {"value": 1, "prob": 0.5234477519989014, "mask": 0.0}, "posts": {"value": null, "prob": 0.0667012631893158, "mask": 0.0}, "badges": {"value": null, "prob": 0.08333966135978699, "mask": 0.0}, "terminate": {"prob": 0.018216757103800774, "mask": 0, "value": null}}, "cls_probs": [0.451022744178772, 0.4148745536804199, 0.1341027319431305], "s_value": 0.5267294645309448, "orig_id": 4576, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.09023115783929825, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.07093885540962219, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08174274116754532, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05201017111539841, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.5010828971862793, "mask": 0.0}, "posts": {"value": null, "prob": 0.07675417512655258, "mask": 0.0}, "badges": {"value": null, "prob": 0.1074584349989891, "mask": 0.0}, "terminate": {"prob": 0.019781503826379776, "mask": 0, "value": null}}, "cls_probs": [0.4548174738883972, 0.41997411847114563, 0.12520846724510193], "s_value": 0.5251372456550598, "orig_id": 4576, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.09961603581905365, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.07746941596269608, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0904252901673317, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.5035924911499023, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08654897660017014, "mask": 0.0}, "badges": {"value": null, "prob": 0.12422338128089905, "mask": 0.0}, "terminate": {"prob": 0.01812443695962429, "mask": 0, "value": null}}, "cls_probs": [0.45508623123168945, 0.42298653721809387, 0.12192728370428085], "s_value": 0.5284262299537659, "orig_id": 4576, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.16729828715324402, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1340627521276474, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.17003293335437775, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20011700689792633, "mask": 0.0}, "badges": {"value": null, "prob": 0.3184642791748047, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01002473197877407, "mask": 0, "value": null}}, "cls_probs": [0.40537843108177185, 0.43628597259521484, 0.15833556652069092], "s_value": 0.5035361051559448, "orig_id": 4576, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1582857072353363, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.11995602399110794, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1562979519367218, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1730305701494217, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.38158276677131653, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010846985504031181, "mask": 0, "value": null}}, "cls_probs": [0.4066622853279114, 0.43648773431777954, 0.1568499207496643], "s_value": 0.5057915449142456, "orig_id": 4576, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.24619558453559875, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.19543114304542542, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.24417495727539062, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2940582036972046, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02014009654521942, "mask": 0, "value": null}}, "cls_probs": [0.41479071974754333, 0.4286158084869385, 0.15659348666667938], "s_value": 0.492141991853714, "orig_id": 4576, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2602189779281616, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.3227269649505615, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.38348400592803955, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03357015177607536, "mask": 0, "value": null}}, "cls_probs": [0.4224625527858734, 0.4244548976421356, 0.15308253467082977], "s_value": 0.4853598177433014, "orig_id": 4576, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Lead Programmer at <a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">Tragnarion Studios</a> using the Unreal Engine 3 for more than 6 years.</p>\n\n<p>Published stuff:</p>\n\n<ul>\n<li><a href=\"http://www.scourgeoutbreak.com\" rel=\"nofollow\">Scourge: Outbreak</a></li>\n<li><a href=\"http://www.mapmymindthegame.com\" rel=\"nofollow\">Map My Mind</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/meneameandroid\" rel=\"nofollow\">Meneame (Digg like spanish site) for Android</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/android-page-curl\" rel=\"nofollow\">Android 2D Page Curl</a></li>\n<li><a href=\"https://github.com/MysticTreeGames/Boost-for-Android\" rel=\"nofollow\">Boost For Android</a></li>\n<li>ALive Art Wallpaper (Parallax 3D Classical Art Live Wallpaper) <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Pro</a> - <a href=\"https://play.google.com/store/apps/details?id=com.mystictreegames.lw.aliveartwallpaperpro\" rel=\"nofollow\">Lite</a></li>\n</ul>\n\n<p>Discontinued || Suspended</p>\n\n<ul>\n<li><a href=\"http://www.Tragnarion.com\" rel=\"nofollow\">The Scourge Project: Episode 1 and 2</a> (Discontinued)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">FC Barcelona Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">UEFA Champions League Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n<li><a href=\"http://www.mystictreegames.com/\" rel=\"nofollow\">Ultimate Soccer Live Wallpaper</a> (Suspended, thx to copyright holders, even for a free app)</li>\n</ul>\n\n<p>Talks and Conferences</p>\n\n<ul>\n<li><a href=\"http://tragnarion.github.com/kinectchallenge2013/#/\" rel=\"nofollow\">Kinect Challenge 2013 Talk about Tools</a> / <a href=\"http://www.youtube.com/watch?v=wWUW21nctUQ&amp;feature=plcp\" rel=\"nofollow\">Video</a></li>\n<li><a href=\"http://tragnarion.github.io/Gamepolis2013/#/\" rel=\"nofollow\">Gamepolis 2013 Game Analitics, Telemetry in videogames</a></li>\n</ul>\n\n<p>More info: <a href=\"http://www.mobygames.com/developer/sheet/view/developerId,420435/\" rel=\"nofollow\">http://www.mobygames.com/developer/sheet/view/developerId,420435/</a></p>\n\n<p>GitHub:</p>\n\n<ul>\n<li>Personal: <a href=\"https://github.com/moritz-wundke\" rel=\"nofollow\">https://github.com/moritz-wundke</a></li>\n<li>Company: <a href=\"https://github.com/MossTrag\" rel=\"nofollow\">https://github.com/MossTrag</a></li>\n</ul>\n\n<p>BitBucket:</p>\n\n<ul>\n<li>Personal: <a href=\"https://bitbucket.org/MoritzWundke\" rel=\"nofollow\">https://bitbucket.org/MoritzWundke</a></li>\n<li>Company: <a href=\"https://bitbucket.org/MossTrag\" rel=\"nofollow\">https://bitbucket.org/MossTrag</a></li>\n</ul>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.4260471761226654, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.524939775466919, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04901302605867386, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4305243194103241, 0.41409024596214294, 0.15538541972637177], "s_value": 0.4812757670879364, "orig_id": 4576, "true_y": 0, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": "<p>\u00abEsprit de G\u00e9om\u00e9trie\u00bb</p>\n\n<p>Feel free to contact me directly, at geodude dot math at gmail.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0136, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8202, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>\u00abEsprit de G\u00e9om\u00e9trie\u00bb</p>\n\n<p>Feel free to contact me directly, at geodude dot math at gmail.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0136, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 8202, "true_y": 0, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>I would like to have <code>git</code> for my life.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.34, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0226, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.028, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2993, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I would like to have <code>git</code> for my life.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.34, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0226, "prob": 0.06484165042638779, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.028, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 2993, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I would like to have <code>git</code> for my life.</p>\n", "prob": 0.07052389532327652, "mask": 0.0}, "views": {"value": 0.34, "prob": 0.10204358398914337, "mask": 0.0}, "reputation": {"value": 0.0226, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.07690940052270889, "mask": 0.0}, "up_votes": {"value": 0.028, "prob": 0.08385247737169266, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06739911437034607, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06818404048681259, "mask": 0.0}, "badges": {"value": null, "prob": 0.08470027148723602, "mask": 0.0}, "terminate": {"prob": 0.44638708233833313, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5429906845092773, 0.3728998601436615, 0.08410939574241638], "s_value": 0.5501275062561035, "orig_id": 2993, "true_y": 0, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0451396144926548, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0725313052535057, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05382183939218521, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05918215960264206, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07011939585208893, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.531177282333374, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06768590956926346, "mask": 0.0}, "badges": {"value": null, "prob": 0.08940514922142029, "mask": 0.0}, "terminate": {"prob": 0.010937291197478771, "mask": 0, "value": null}}, "cls_probs": [0.45435237884521484, 0.41012728214263916, 0.1355203092098236], "s_value": 0.5306205749511719, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09319149702787399, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.11058338731527328, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14358055591583252, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.076701320707798, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1482120007276535, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.12007257342338562, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.15600496530532837, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18295153975486755, "mask": 0.0}, "badges": {"value": null, "prob": 0.3096380829811096, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.0064195189625024796, "mask": 0, "value": null}}, "cls_probs": [0.4110143780708313, 0.4298921823501587, 0.15909342467784882], "s_value": 0.511255145072937, "orig_id": 3399, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.11078997701406479, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.21179400384426117, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1726912260055542, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.22282187640666962, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2712239921092987, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.010678905062377453, "mask": 0, "value": null}}, "cls_probs": [0.42161825299263, 0.42145419120788574, 0.15692752599716187], "s_value": 0.5044485926628113, "orig_id": 3399, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.15302440524101257, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.2848377525806427, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.24050608277320862, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.3083721101284027, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.013259646482765675, "mask": 0, "value": null}}, "cls_probs": [0.4254319667816162, 0.4141094386577606, 0.16045860946178436], "s_value": 0.5014233589172363, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.2181393951177597, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.4153880774974823, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.3447738587856293, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02169857546687126, "mask": 0, "value": null}}, "cls_probs": [0.43098804354667664, 0.4109264016151428, 0.1580856293439865], "s_value": 0.5011744499206543, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.3381078839302063, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.6326749324798584, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.029217161238193512, "mask": 0, "value": null}}, "cls_probs": [0.4110872149467468, 0.4295775294303894, 0.15933524072170258], "s_value": 0.49125897884368896, "orig_id": 3399, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.9131743907928467, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08682558685541153, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4173296093940735, 0.427418977022171, 0.1552513837814331], "s_value": 0.48481789231300354, "orig_id": 3399, "true_y": 1, "last_cost": 0.0, "total_cost": 5.0}], [{"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.07510669529438019, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.10096142441034317, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.1179569810628891, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1502375602722168, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0744071826338768, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18036718666553497, "mask": 0.0}, "badges": {"value": null, "prob": 0.2942897379398346, "mask": 0.0}, "terminate": {"prob": 0.0066732075065374374, "mask": 0, "value": null}}, "cls_probs": [0.42407476902008057, 0.41744476556777954, 0.1584804654121399], "s_value": 0.5154073238372803, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0849391296505928, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.13022947311401367, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.16600608825683594, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08431260287761688, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19095264375209808, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.33489155769348145, "mask": 0.0}, "terminate": {"prob": 0.008668467402458191, "mask": 0, "value": null}}, "cls_probs": [0.42646971344947815, 0.4210495948791504, 0.15248064696788788], "s_value": 0.5075578689575195, "orig_id": 3414, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.103250652551651, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.16309000551700592, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.20653600990772247, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09997090697288513, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.41824981570243835, "mask": 0.0}, "terminate": {"prob": 0.008902632631361485, "mask": 0, "value": null}}, "cls_probs": [0.431520938873291, 0.413403183221817, 0.15507590770721436], "s_value": 0.5053294897079468, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.122451052069664, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.24884653091430664, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.11750981211662292, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5024318099021912, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008760761469602585, "mask": 0, "value": null}}, "cls_probs": [0.41471433639526367, 0.4309520721435547, 0.15433362126350403], "s_value": 0.4965561628341675, "orig_id": 3414, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.10637835413217545, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.218489408493042, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10467418283224106, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.5620311498641968, "mask": 0.0}, "terminate": {"prob": 0.008426974527537823, "mask": 0, "value": null}}, "cls_probs": [0.4255453646183014, 0.4237743020057678, 0.1506803333759308], "s_value": 0.49629300832748413, "orig_id": 3414, "true_y": 1, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.25432518124580383, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.13272970914840698, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.598910391330719, "mask": 0.0}, "terminate": {"prob": 0.014034651219844818, "mask": 0, "value": null}}, "cls_probs": [0.41427403688430786, 0.43653443455696106, 0.14919152855873108], "s_value": 0.4893156588077545, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.28608211874961853, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.6995651721954346, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01435267273336649, "mask": 0, "value": null}}, "cls_probs": [0.40523210167884827, 0.44077789783477783, 0.1539900302886963], "s_value": 0.4867452383041382, "orig_id": 3414, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.3145994544029236, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.6663292646408081, "mask": 0.5}, "terminate": {"prob": 0.01907130889594555, "mask": 0, "value": null}}, "cls_probs": [0.40790849924087524, 0.43894022703170776, 0.153151273727417], "s_value": 0.4798721671104431, "orig_id": 3414, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>Programmer by desire, leader by nature. More information and links about me are at <a href=\"http://scott.willeke.com\" rel=\"nofollow\">scott.willeke.com</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.9630511403083801, "mask": 0.5}, "terminate": {"prob": 0.036948833614587784, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.41128596663475037, 0.43297508358955383, 0.15573890507221222], "s_value": 0.4784592390060425, "orig_id": 3414, "true_y": 1, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.055667996406555176, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 1, "prob": 0.44816169142723083, "mask": 0.0}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 7698, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.09770273417234421, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.07288249582052231, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0785747691988945, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05732293054461479, "mask": 0.0}, "website": {"value": 1, "prob": 0.4709967076778412, "mask": 0.0}, "posts": {"value": null, "prob": 0.08966957777738571, "mask": 0.0}, "badges": {"value": null, "prob": 0.107587531208992, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02526319958269596, "mask": 0, "value": null}}, "cls_probs": [0.4553866386413574, 0.4081551134586334, 0.13645824790000916], "s_value": 0.5261386036872864, "orig_id": 7698, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1230207160115242, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09201336652040482, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.08984855562448502, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07290823757648468, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.2991468906402588, "mask": 0.0}, "posts": {"value": null, "prob": 0.09450968354940414, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.18764711916446686, "mask": 0.0}, "terminate": {"prob": 0.04090547934174538, "mask": 0, "value": null}}, "cls_probs": [0.4682766795158386, 0.3950446844100952, 0.13667868077754974], "s_value": 0.5224148035049438, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.13780304789543152, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.101802296936512, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09754850715398788, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.2856714427471161, "mask": 0.0}, "posts": {"value": null, "prob": 0.10291313380002975, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.23767700791358948, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.036584604531526566, "mask": 0, "value": null}}, "cls_probs": [0.46606019139289856, 0.4032221734523773, 0.1307176798582077], "s_value": 0.5269879102706909, "orig_id": 7698, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.11440390348434448, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.08856380730867386, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09412819892168045, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.5545204281806946, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.09617722779512405, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.052206359803676605, "mask": 0, "value": null}}, "cls_probs": [0.4530579447746277, 0.39421528577804565, 0.15272672474384308], "s_value": 0.5123953819274902, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.25316232442855835, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.18412815034389496, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.20870442688465118, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.32892870903015137, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02507637068629265, "mask": 0, "value": null}}, "cls_probs": [0.41951414942741394, 0.4102000296115875, 0.17028586566448212], "s_value": 0.49956023693084717, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.3201691806316376, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.2558220624923706, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.3891963064670563, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03481249138712883, "mask": 0, "value": null}}, "cls_probs": [0.42259445786476135, 0.4169194996356964, 0.1604861468076706], "s_value": 0.493438720703125, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.21884295344352722, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17785049974918365, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.6033065915107727, "mask": 0, "value": null}}, "cls_probs": [0.47935134172439575, 0.3532627820968628, 0.16738587617874146], "s_value": 0.5016719102859497, "orig_id": 7698, "true_y": 1, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Programming since I was born.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2271924912929535, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.7728075385093689, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.49264323711395264, 0.3483865261077881, 0.15897023677825928], "s_value": 0.4977426826953888, "orig_id": 7698, "true_y": 1, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2556, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 2556, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0108, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0108, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0108, "prob": 0.09319149702787399, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.11058338731527328, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.14358055591583252, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0767030119895935, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.14821366965770721, "mask": 0.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.12007363140583038, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.15600501000881195, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18295155465602875, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.3096330463886261, "mask": 0.0}, "terminate": {"prob": 0.00642010010778904, "mask": 0, "value": null}}, "cls_probs": [0.41100943088531494, 0.42989540100097656, 0.1590951532125473], "s_value": 0.511255145072937, "orig_id": 5421, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.07869084179401398, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.15152904391288757, "mask": 0.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.11965097486972809, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1513882875442505, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.12215401977300644, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.12206949293613434, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.12859849631786346, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.12039833515882492, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13150399923324585, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12421097606420517, "mask": 0.0}, "tags": {"value": null, "prob": 0.12811139225959778, "mask": 0.0}, "comments": {"value": null, "prob": 0.12295334786176682, "mask": 0.0}}], "prob": 0.16788586974143982, "mask": 0.0}, "badges": {"value": null, "prob": 0.32206928730010986, "mask": 0.0}, "terminate": {"prob": 0.008785562589764595, "mask": 0, "value": null}}, "cls_probs": [0.4228817820549011, 0.42920881509780884, 0.14790938794612885], "s_value": 0.5166260004043579, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0939650759100914, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.18022821843624115, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.13931812345981598, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.1218888983130455, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.12200960516929626, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.1286497414112091, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.120164655148983, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13164792954921722, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12422942370176315, "mask": 0.0}, "tags": {"value": null, "prob": 0.1284865438938141, "mask": 0.0}, "comments": {"value": null, "prob": 0.12292326986789703, "mask": 0.0}}], "prob": 0.19201874732971191, "mask": 0.0}, "badges": {"value": null, "prob": 0.38160207867622375, "mask": 0.0}, "terminate": {"prob": 0.012867718935012817, "mask": 0, "value": null}}, "cls_probs": [0.42659732699394226, 0.4299706518650055, 0.14343200623989105], "s_value": 0.5166044235229492, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.11235789954662323, "mask": 0.0, "selected": true}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.1638937145471573, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.12182697653770447, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.12210555374622345, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.12876343727111816, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.1200331449508667, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13201983273029327, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12441042810678482, "mask": 0.0}, "tags": {"value": null, "prob": 0.12828640639781952, "mask": 0.0}, "comments": {"value": null, "prob": 0.12255416065454483, "mask": 0.0}}], "prob": 0.21755918860435486, "mask": 0.0}, "badges": {"value": null, "prob": 0.4880078434944153, "mask": 0.0}, "terminate": {"prob": 0.018181297928094864, "mask": 0, "value": null}}, "cls_probs": [0.4475325345993042, 0.4186548888683319, 0.13381259143352509], "s_value": 0.5159539580345154, "orig_id": 5421, "true_y": 0, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.1985355168581009, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.12150231748819351, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.1219136044383049, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.128621906042099, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.11986898630857468, "mask": 0.0, "selected": true}, "answers": {"value": 0.1, "prob": 0.13226324319839478, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12480223923921585, "mask": 0.0}, "tags": {"value": null, "prob": 0.12941741943359375, "mask": 0.0}, "comments": {"value": null, "prob": 0.12161028385162354, "mask": 0.0}}], "prob": 0.26085615158081055, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.5111899971961975, "mask": 0.0}, "terminate": {"prob": 0.029418326914310455, "mask": 0, "value": null}}, "cls_probs": [0.4363165497779846, 0.4317224621772766, 0.13196103274822235], "s_value": 0.5094303488731384, "orig_id": 5421, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.1998634785413742, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.1379706859588623, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.13825370371341705, "mask": 0.0, "selected": true}, "score": {"value": 0.01, "prob": 0.1450270712375641, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.1502263844013214, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14309704303741455, "mask": 0.0}, "tags": {"value": null, "prob": 0.15084853768348694, "mask": 0.0}, "comments": {"value": null, "prob": 0.13457666337490082, "mask": 0.0}}], "prob": 0.30254995822906494, "mask": 0.125, "selected": true}, "badges": {"value": null, "prob": 0.4678551256656647, "mask": 0.0}, "terminate": {"prob": 0.029731420800089836, "mask": 0, "value": null}}, "cls_probs": [0.44959232211112976, 0.4078601896762848, 0.1425473988056183], "s_value": 0.5021306872367859, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.21022078394889832, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.15995389223098755, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.16805249452590942, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.17460238933563232, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.16679726541042328, "mask": 0.0}, "tags": {"value": null, "prob": 0.17675508558750153, "mask": 0.0, "selected": true}, "comments": {"value": null, "prob": 0.15383897721767426, "mask": 0.0}}], "prob": 0.2982078790664673, "mask": 0.25, "selected": true}, "badges": {"value": null, "prob": 0.4506928026676178, "mask": 0.0}, "terminate": {"prob": 0.04087850823998451, "mask": 0, "value": null}}, "cls_probs": [0.45601001381874084, 0.393493115901947, 0.15049682557582855], "s_value": 0.5004993677139282, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.21597665548324585, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.16072334349155426, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.1696319282054901, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.17392899096012115, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.16511423885822296, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.17048202455043793, "mask": 0.0}, "comments": {"value": null, "prob": 0.1601194143295288, "mask": 0.0}}], "prob": 0.23940305411815643, "mask": 0.25}, "badges": {"value": null, "prob": 0.4843778610229492, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.060242414474487305, "mask": 0, "value": null}}, "cls_probs": [0.4361949563026428, 0.42510560154914856, 0.13869942724704742], "s_value": 0.49641504883766174, "orig_id": 5421, "true_y": 0, "last_cost": 1.0, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.17077268660068512, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.1593906432390213, "mask": 0.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.1701563596725464, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.1750996857881546, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.16451869904994965, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.1714397370815277, "mask": 0.0}, "comments": {"value": null, "prob": 0.15939489006996155, "mask": 0.0, "selected": true}}], "prob": 0.18089976906776428, "mask": 0.25, "selected": true}, "badges": {"value": [{"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.5874242186546326, "mask": 0.0}, "terminate": {"prob": 0.06090336665511131, "mask": 0, "value": null}}, "cls_probs": [0.45370200276374817, 0.4091397225856781, 0.13715822994709015], "s_value": 0.49900269508361816, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 6.600000001490116}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.17864175140857697, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.18765971064567566, "mask": 0.0, "selected": true}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.2011253833770752, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.20805925130844116, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.19538740813732147, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.2077682614326477, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.1982751488685608, "mask": 0.375, "selected": true}, "badges": {"value": [{"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.5674697756767273, "mask": 0.0}, "terminate": {"prob": 0.05561336874961853, "mask": 0, "value": null}}, "cls_probs": [0.4322102665901184, 0.4137352705001831, 0.1540544331073761], "s_value": 0.49357926845550537, "orig_id": 5421, "true_y": 0, "last_cost": 0.20000000298023224, "total_cost": 7.100000001490116}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.18109753727912903, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.24729923903942108, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.25630074739456177, "mask": 0.0, "selected": true}, "favorites": {"value": 0.0, "prob": 0.24099531769752502, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.2554047107696533, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.18421262502670288, "mask": 0.5, "selected": true}, "badges": {"value": [{"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.5622808337211609, "mask": 0.0}, "terminate": {"prob": 0.07240892201662064, "mask": 0, "value": null}}, "cls_probs": [0.442386269569397, 0.40743401646614075, 0.15017975866794586], "s_value": 0.4904177188873291, "orig_id": 5421, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 7.300000004470348}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.18460223078727722, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.329311341047287, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.3221342861652374, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.3485543727874756, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.1807519644498825, "mask": 0.625}, "badges": {"value": [{"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.5677686333656311, "mask": 0.0}, "terminate": {"prob": 0.06687718629837036, "mask": 0, "value": null}}, "cls_probs": [0.42523059248924255, 0.4170209765434265, 0.15774844586849213], "s_value": 0.4939028024673462, "orig_id": 5421, "true_y": 0, "last_cost": 0.5, "total_cost": 7.4000000059604645}, {"sample": {"about_me": {"value": "<p>I'm a recent physics graduate looking for a job (like most physicists I suppose).</p>\n\n<p>I have a broad interest, including: Solid state physics, quantum theories, complex systems and mathematical physics.</p>\n\n<p>Next to that I've been a canyoneer for about 7 years. I practice the sport solo as well as a guide.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0108, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "About variance mixture models and probability distributions", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I was wondering if anyone knows a good resource to learn about variance mixture models ? My interest is in particular the <a href=\"http://en.wikipedia.org/wiki/Normal_variance-mean_mixture\" rel=\"nofollow\">normal variance mean mixture</a>.</p>\n\n<p>I know what they mean with their definition of $Y=\\\\mu+\\\\beta X+\\\\sigma\\\\sqrt{X}Z$, with $Y$ the variance mixed distribution, X the mixing distribution and Z the <a href=\"http://mathworld.wolfram.com/StandardNormalDistribution.html\" rel=\"nofollow\">standard normal distribution</a>, which are independent of each other.</p>\n\n<p>In an <a href=\"http://arxiv.org/abs/1106.2333\" rel=\"nofollow\">article on ArXiV</a> the immediately suggest the generalized hyperbolic distribution as a solution. Now I was wondering how one would go from the prescription above for $Y$ to a formula for the distribution. Is it by guessing and hoping it works, or is there some kind of procedure ?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.01, "prob": 0.33064553141593933, "mask": 0.0}, "views": {"value": 0.0072, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.1, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.3247056305408478, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "probability", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "distributions", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "mixed-model", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "references", "prob": 0.20000000298023224, "mask": 0.0}}, {"tag": {"value": "gaussian-mixture", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.3446488082408905, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.12494589388370514, "mask": 0.625}, "badges": {"value": [{"badge": {"value": "Informed", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.4782349765300751, "mask": 0.0}, "terminate": {"prob": 0.39681917428970337, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5145185589790344, 0.3578941226005554, 0.12758737802505493], "s_value": 0.49461716413497925, "orig_id": 5421, "true_y": 0, "last_cost": 0.0, "total_cost": 7.9000000059604645}], [{"sample": {"about_me": {"value": "<p>Nothing Much!!!</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6795, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Nothing Much!!!</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 6795, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Nothing Much!!!</p>\n", "prob": 0.06440980732440948, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09031162410974503, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0739755854010582, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06901188939809799, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07773992419242859, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06313326209783554, "mask": 0.0}, "badges": {"value": null, "prob": 0.07046709954738617, "mask": 0.0}, "terminate": {"prob": 0.49095073342323303, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.542242169380188, 0.3691409230232239, 0.08861689269542694], "s_value": 0.5583330988883972, "orig_id": 6795, "true_y": 0, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 1, "prob": 0.44816169142723083, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.0771675556898117, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.14489252865314484, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10301674902439117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12164411693811417, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07795343548059464, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19800105690956116, "mask": 0.0}, "badges": {"value": null, "prob": 0.2697715163230896, "mask": 0.0}, "terminate": {"prob": 0.007553049363195896, "mask": 0, "value": null}}, "cls_probs": [0.4237002730369568, 0.4112336337566376, 0.16506606340408325], "s_value": 0.5202891826629639, "orig_id": 3983, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1621394008398056, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1177326887845993, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.13386256992816925, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09158706665039062, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21045713126659393, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.26984038949012756, "mask": 0.0}, "terminate": {"prob": 0.014380794949829578, "mask": 0, "value": null}}, "cls_probs": [0.43180668354034424, 0.4088531732559204, 0.15934017300605774], "s_value": 0.513636589050293, "orig_id": 3983, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.20327280461788177, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1469482183456421, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1718290150165558, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1112503781914711, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.35249799489974976, "mask": 0.0}, "terminate": {"prob": 0.014201649464666843, "mask": 0, "value": null}}, "cls_probs": [0.43817898631095886, 0.4014558494091034, 0.16036513447761536], "s_value": 0.5111439228057861, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.22680242359638214, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1621057242155075, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.18964804708957672, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.4074927270412445, "mask": 0.0}, "terminate": {"prob": 0.013951069675385952, "mask": 0, "value": null}}, "cls_probs": [0.4273635447025299, 0.41097673773765564, 0.16165973246097565], "s_value": 0.5066280961036682, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.2791458070278168, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.19679290056228638, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5096811056137085, "mask": 0.0}, "terminate": {"prob": 0.014380211941897869, "mask": 0, "value": null}}, "cls_probs": [0.40912801027297974, 0.4305022358894348, 0.16036978363990784], "s_value": 0.4980412423610687, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.26735955476760864, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.7106039524078369, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02203643135726452, "mask": 0, "value": null}}, "cls_probs": [0.41325321793556213, 0.4292427599430084, 0.15750399231910706], "s_value": 0.49043816328048706, "orig_id": 3983, "true_y": 1, "last_cost": 1.0, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.8987200260162354, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.10127997398376465, "mask": 0, "value": null}}, "cls_probs": [0.42070698738098145, 0.41583433747291565, 0.1634586751461029], "s_value": 0.4831963777542114, "orig_id": 3983, "true_y": 1, "last_cost": 0.5, "total_cost": 5.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.42412474751472473, 0.4269156754016876, 0.14895962178707123], "s_value": 0.4772082269191742, "orig_id": 3983, "true_y": 1, "last_cost": 0.0, "total_cost": 6.0}], [{"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 1111, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0769987404346466, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.148077592253685, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10395338386297226, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12679272890090942, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16148799657821655, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07459203153848648, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3030097186565399, "mask": 0.0}, "terminate": {"prob": 0.005087730009108782, "mask": 0, "value": null}}, "cls_probs": [0.4247566759586334, 0.41208311915397644, 0.16316016018390656], "s_value": 0.5213593244552612, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.08259399235248566, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.15961118042469025, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11118636280298233, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.13589635491371155, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.17521116137504578, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.33035770058631897, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005143203306943178, "mask": 0, "value": null}}, "cls_probs": [0.41251227259635925, 0.42212483286857605, 0.1653628945350647], "s_value": 0.5132908225059509, "orig_id": 1111, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.073963962495327, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.14576807618141174, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10217815637588501, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11681711673736572, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1560613214969635, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3996872901916504, "mask": 0.0}, "terminate": {"prob": 0.0055241333320736885, "mask": 0, "value": null}}, "cls_probs": [0.4218246638774872, 0.4140007197856903, 0.16417455673217773], "s_value": 0.512089192867279, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.08503995090723038, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.16798561811447144, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1159401386976242, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1834157407283783, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.44257232546806335, "mask": 0.0}, "terminate": {"prob": 0.0050462763756513596, "mask": 0, "value": null}}, "cls_probs": [0.4041425883769989, 0.43134650588035583, 0.1645108461380005], "s_value": 0.5052648782730103, "orig_id": 1111, "true_y": 1, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1942490190267563, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.13488176465034485, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.20581933856010437, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.45763149857521057, "mask": 0.0}, "terminate": {"prob": 0.0074183265678584576, "mask": 0, "value": null}}, "cls_probs": [0.3933992087841034, 0.4449894428253174, 0.1616113781929016], "s_value": 0.5002741813659668, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.24018926918506622, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.16915734112262726, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5783693194389343, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.012284054420888424, "mask": 0, "value": null}}, "cls_probs": [0.4000805616378784, 0.43397295475006104, 0.16594643890857697], "s_value": 0.4987962245941162, "orig_id": 1111, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.0}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.5512688755989075, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.40096724033355713, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04776390641927719, "mask": 0, "value": null}}, "cls_probs": [0.403181791305542, 0.42127519845962524, 0.17554301023483276], "s_value": 0.4822905361652374, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.9215796589851379, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.07842029631137848, "mask": 0, "value": null}}, "cls_probs": [0.4066634774208069, 0.43296515941619873, 0.16037139296531677], "s_value": 0.4767339825630188, "orig_id": 1111, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "C and Python programmer with a background in parallel computing and web development.", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4148690104484558, 0.43052423000335693, 0.15460678935050964], "s_value": 0.470564603805542, "orig_id": 1111, "true_y": 1, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>I am a PhD student at the University of Sussex studying Computational Linguistics. My research topic is text classification in imbalanced data streams.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.06316898763179779, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0113, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2818, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am a PhD student at the University of Sussex studying Computational Linguistics. My research topic is text classification in imbalanced data streams.</p>\n", "prob": 0.04327615723013878, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0113, "prob": 0.05277609825134277, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05615254491567612, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.06647281348705292, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.03971327096223831, "mask": 0.0}, "website": {"value": 0, "prob": 0.5779329538345337, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06199861690402031, "mask": 0.0}, "badges": {"value": null, "prob": 0.08763445168733597, "mask": 0.0}, "terminate": {"prob": 0.014043182134628296, "mask": 0, "value": null}}, "cls_probs": [0.46756479144096375, 0.4092513620853424, 0.12318389862775803], "s_value": 0.5299848318099976, "orig_id": 2818, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am a PhD student at the University of Sussex studying Computational Linguistics. My research topic is text classification in imbalanced data streams.</p>\n", "prob": 0.06194550544023514, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0113, "prob": 0.07231834530830383, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06571751087903976, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.07224355638027191, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05888696014881134, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.05824374780058861, "mask": 0.0}, "badges": {"value": null, "prob": 0.07083014398813248, "mask": 0.0}, "terminate": {"prob": 0.5398141741752625, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5498194694519043, 0.36718252301216125, 0.08299799263477325], "s_value": 0.5534292459487915, "orig_id": 2818, "true_y": 1, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.07510669529438019, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.10096142441034317, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1179569810628891, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1502375602722168, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0744071826338768, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18036718666553497, "mask": 0.0}, "badges": {"value": null, "prob": 0.2942897379398346, "mask": 0.0}, "terminate": {"prob": 0.0066732075065374374, "mask": 0, "value": null}}, "cls_probs": [0.42407476902008057, 0.41744476556777954, 0.1584804654121399], "s_value": 0.5154073238372803, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.07994939386844635, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.10770249366760254, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1261947751045227, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.16288374364376068, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19399817287921906, "mask": 0.0}, "badges": {"value": null, "prob": 0.3226999044418335, "mask": 0.0}, "terminate": {"prob": 0.006571510806679726, "mask": 0, "value": null}}, "cls_probs": [0.41387319564819336, 0.42551523447036743, 0.1606115996837616], "s_value": 0.5091726779937744, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.09046883136034012, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1212293952703476, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18799151480197906, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2229996919631958, "mask": 0.0}, "badges": {"value": null, "prob": 0.371232807636261, "mask": 0.0}, "terminate": {"prob": 0.006077748257666826, "mask": 0, "value": null}}, "cls_probs": [0.3996538519859314, 0.4378759264945984, 0.16247020661830902], "s_value": 0.5016763806343079, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.11507133394479752, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.15310317277908325, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2709878981113434, "mask": 0.0}, "badges": {"value": null, "prob": 0.45104843378067017, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.00978904776275158, "mask": 0, "value": null}}, "cls_probs": [0.4004437029361725, 0.4369761347770691, 0.16258016228675842], "s_value": 0.5001167058944702, "orig_id": 5558, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.09740274399518967, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.13438214361667633, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2077031135559082, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5502291917800903, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01028277724981308, "mask": 0, "value": null}}, "cls_probs": [0.4129171669483185, 0.4222232699394226, 0.16485954821109772], "s_value": 0.5007824301719666, "orig_id": 5558, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.2132876068353653, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.2790890336036682, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.4762982726097107, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.0313250757753849, "mask": 0, "value": null}}, "cls_probs": [0.4158197045326233, 0.41881322860717773, 0.1653670072555542], "s_value": 0.4847032427787781, "orig_id": 5558, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.3013353645801544, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.650780975818634, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04788359999656677, "mask": 0, "value": null}}, "cls_probs": [0.4171193242073059, 0.42840710282325745, 0.15447357296943665], "s_value": 0.47902706265449524, "orig_id": 5558, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.9143907427787781, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08560927212238312, "mask": 0, "value": null}}, "cls_probs": [0.4081190228462219, 0.43928346037864685, 0.15259750187397003], "s_value": 0.4733564555644989, "orig_id": 5558, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>My fields of interest are Digital Signal Processing, Artificial Intelligence, and Machine Learning.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.416808158159256, 0.43014052510261536, 0.15305137634277344], "s_value": 0.4703781008720398, "orig_id": 5558, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Computer Scientist, Free Software Advocate, Wannabe Mathematician, attracted by beautiful Typography.</p>\n\n<p>I am the current maintainer of the <code>algorithms</code> bundle for LaTeX, which includes the packages <code>algorithm</code> and <code>algorithmic</code>. It is openly developed at <a href=\"https://github.com/rbrito/algorithms\" rel=\"nofollow\">github</a>.</p>\n\n<p>By the way, my <a href=\"https://github.com/rbrito\" rel=\"nofollow\">github account</a> also features many of my projects (and I am also collaborator on other repositories).</p>\n\n<p>I am also a <a href=\"http://qa.debian.org/developer.php?login=rbrito%40ime.usp.br\" rel=\"nofollow\">proud Debian Maintainer</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0111, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7129, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Computer Scientist, Free Software Advocate, Wannabe Mathematician, attracted by beautiful Typography.</p>\n\n<p>I am the current maintainer of the <code>algorithms</code> bundle for LaTeX, which includes the packages <code>algorithm</code> and <code>algorithmic</code>. It is openly developed at <a href=\"https://github.com/rbrito/algorithms\" rel=\"nofollow\">github</a>.</p>\n\n<p>By the way, my <a href=\"https://github.com/rbrito\" rel=\"nofollow\">github account</a> also features many of my projects (and I am also collaborator on other repositories).</p>\n\n<p>I am also a <a href=\"http://qa.debian.org/developer.php?login=rbrito%40ime.usp.br\" rel=\"nofollow\">proud Debian Maintainer</a></p>\n", "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0111, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08534760773181915, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 1, "prob": 0.3274863362312317, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}], "prob": 0.15204817056655884, "mask": 0.0}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 7129, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Computer Scientist, Free Software Advocate, Wannabe Mathematician, attracted by beautiful Typography.</p>\n\n<p>I am the current maintainer of the <code>algorithms</code> bundle for LaTeX, which includes the packages <code>algorithm</code> and <code>algorithmic</code>. It is openly developed at <a href=\"https://github.com/rbrito/algorithms\" rel=\"nofollow\">github</a>.</p>\n\n<p>By the way, my <a href=\"https://github.com/rbrito\" rel=\"nofollow\">github account</a> also features many of my projects (and I am also collaborator on other repositories).</p>\n\n<p>I am also a <a href=\"http://qa.debian.org/developer.php?login=rbrito%40ime.usp.br\" rel=\"nofollow\">proud Debian Maintainer</a></p>\n", "prob": 0.06145130470395088, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.12017360329627991, "mask": 0.0}, "reputation": {"value": 0.0111, "prob": 0.08465293794870377, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 7129, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Computer Scientist, Free Software Advocate, Wannabe Mathematician, attracted by beautiful Typography.</p>\n\n<p>I am the current maintainer of the <code>algorithms</code> bundle for LaTeX, which includes the packages <code>algorithm</code> and <code>algorithmic</code>. It is openly developed at <a href=\"https://github.com/rbrito/algorithms\" rel=\"nofollow\">github</a>.</p>\n\n<p>By the way, my <a href=\"https://github.com/rbrito\" rel=\"nofollow\">github account</a> also features many of my projects (and I am also collaborator on other repositories).</p>\n\n<p>I am also a <a href=\"http://qa.debian.org/developer.php?login=rbrito%40ime.usp.br\" rel=\"nofollow\">proud Debian Maintainer</a></p>\n", "prob": 0.06457851827144623, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.12445750832557678, "mask": 0.0}, "reputation": {"value": 0.0111, "prob": 0.08794606477022171, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.10048365592956543, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.12945173680782318, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06588384509086609, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15690678358078003, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.2635612487792969, "mask": 0.5}, "terminate": {"prob": 0.006730749737471342, "mask": 0, "value": null}}, "cls_probs": [0.42721277475357056, 0.4155136048793793, 0.15727369487285614], "s_value": 0.5158800482749939, "orig_id": 7129, "true_y": 1, "last_cost": 0.5, "total_cost": 1.6000000014901161}, {"sample": {"about_me": {"value": "<p>Computer Scientist, Free Software Advocate, Wannabe Mathematician, attracted by beautiful Typography.</p>\n\n<p>I am the current maintainer of the <code>algorithms</code> bundle for LaTeX, which includes the packages <code>algorithm</code> and <code>algorithmic</code>. It is openly developed at <a href=\"https://github.com/rbrito/algorithms\" rel=\"nofollow\">github</a>.</p>\n\n<p>By the way, my <a href=\"https://github.com/rbrito\" rel=\"nofollow\">github account</a> also features many of my projects (and I am also collaborator on other repositories).</p>\n\n<p>I am also a <a href=\"http://qa.debian.org/developer.php?login=rbrito%40ime.usp.br\" rel=\"nofollow\">proud Debian Maintainer</a></p>\n", "prob": 0.09998062998056412, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.13534501194953918, "mask": 0.0}, "reputation": {"value": 0.0111, "prob": 0.11359148472547531, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1238534152507782, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0919555053114891, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1161317378282547, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.18697288632392883, "mask": 0.5}, "terminate": {"prob": 0.13216938078403473, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4755061864852905, 0.35143229365348816, 0.17306150496006012], "s_value": 0.516762375831604, "orig_id": 7129, "true_y": 1, "last_cost": 0.0, "total_cost": 2.100000001490116}], [{"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.079, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.079, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 772, "true_y": 2, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.06145130842924118, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.12017360329627991, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.08465293794870377, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.079, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Notable Question", "prob": 0.09090909361839294, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.06768263131380081, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1318981647491455, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1017780601978302, "mask": 0.0}, "up_votes": {"value": 0.079, "prob": 0.13424254953861237, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06928034871816635, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1459120213985443, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Notable Question", "prob": 0.09090909361839294, "mask": 0.0}}], "prob": 0.3419487774372101, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007257501594722271, "mask": 0, "value": null}}, "cls_probs": [0.42248886823654175, 0.42360857129096985, 0.1539025455713272], "s_value": 0.516074538230896, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.06876158714294434, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.13316772878170013, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.10280745476484299, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.079, "prob": 0.1348905861377716, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07052920013666153, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1481531411409378, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Notable Question", "prob": 0.10000000149011612, "mask": 0.0}}], "prob": 0.33386141061782837, "mask": 0.09090909361839294}, "terminate": {"prob": 0.007828840985894203, "mask": 0, "value": null}}, "cls_probs": [0.4233490526676178, 0.42267176508903503, 0.15397921204566956], "s_value": 0.5145261287689209, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 2.100000001490116}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.07691717147827148, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1501655876636505, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.1550004780292511, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07851291447877884, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16993986070156097, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Notable Question", "prob": 0.10000000149011612, "mask": 0.0}}], "prob": 0.36248770356178284, "mask": 0.09090909361839294}, "terminate": {"prob": 0.006976197008043528, "mask": 0, "value": null}}, "cls_probs": [0.4083647131919861, 0.43904682993888855, 0.15258845686912537], "s_value": 0.5043132305145264, "orig_id": 772, "true_y": 2, "last_cost": 1.0, "total_cost": 2.600000001490116}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.07745201140642166, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.15123192965984344, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.14957208931446075, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08206314593553543, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.06086065247654915, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060898225754499435, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06451281160116196, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.05997924879193306, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.0660727247595787, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.062015727162361145, "mask": 0.0}, "tags": {"value": null, "prob": 0.06413765996694565, "mask": 0.0}, "comments": {"value": null, "prob": 0.06152292340993881, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06086065247654915, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060898225754499435, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06451281160116196, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05997924879193306, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0660727247595787, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062015727162361145, "mask": 0.0}, "tags": {"value": null, "prob": 0.06413765996694565, "mask": 0.0}, "comments": {"value": null, "prob": 0.06152292340993881, "mask": 0.0}}], "prob": 0.15708297491073608, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Notable Question", "prob": 0.10000000149011612, "mask": 0.0, "selected": true}}], "prob": 0.373745858669281, "mask": 0.09090909361839294, "selected": true}, "terminate": {"prob": 0.008851880207657814, "mask": 0, "value": null}}, "cls_probs": [0.41040369868278503, 0.4425274431705475, 0.1470688134431839], "s_value": 0.5070322155952454, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.07928828150033951, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1532650738954544, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.15057793259620667, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08410678803920746, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.06082579120993614, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060885414481163025, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06447988748550415, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.05996938422322273, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06606114655733109, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06203875690698624, "mask": 0.0}, "tags": {"value": null, "prob": 0.06428346782922745, "mask": 0.0}, "comments": {"value": null, "prob": 0.06145612895488739, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06082579120993614, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060885414481163025, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06447988748550415, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05996938422322273, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06606114655733109, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06203875690698624, "mask": 0.0}, "tags": {"value": null, "prob": 0.06428346782922745, "mask": 0.0}, "comments": {"value": null, "prob": 0.06145612895488739, "mask": 0.0}}], "prob": 0.1581631600856781, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.1111111119389534, "mask": 0.0, "selected": true}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.36478105187416077, "mask": 0.1818181872367859, "selected": true}, "terminate": {"prob": 0.009817657992243767, "mask": 0, "value": null}}, "cls_probs": [0.41104140877723694, 0.4417826235294342, 0.14717602729797363], "s_value": 0.504655122756958, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 3.7000000029802322}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.08160610496997833, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.15513990819454193, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.15176339447498322, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08706294745206833, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.060695771127939224, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060771189630031586, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0644076019525528, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.05996912717819214, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06608134508132935, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.062083255499601364, "mask": 0.0}, "tags": {"value": null, "prob": 0.06481806933879852, "mask": 0.0}, "comments": {"value": null, "prob": 0.06117362156510353, "mask": 0.0}}, {"title": {"value": null, "prob": 0.060695771127939224, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060771189630031586, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0644076019525528, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05996912717819214, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06608134508132935, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062083255499601364, "mask": 0.0}, "tags": {"value": null, "prob": 0.06481806933879852, "mask": 0.0}, "comments": {"value": null, "prob": 0.06117362156510353, "mask": 0.0}}], "prob": 0.1613151580095291, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.125, "mask": 0.0, "selected": true}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.35238727927207947, "mask": 0.27272728085517883, "selected": true}, "terminate": {"prob": 0.010725236497819424, "mask": 0, "value": null}}, "cls_probs": [0.4056178033351898, 0.4429411292076111, 0.15144100785255432], "s_value": 0.5016807913780212, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 3.8000000044703484}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.08272852003574371, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1563822478055954, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.1529451608657837, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08827429264783859, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.06067377328872681, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.06075446307659149, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06436700373888016, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.059977978467941284, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06605295836925507, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06209368631243706, "mask": 0.0}, "tags": {"value": null, "prob": 0.06494995951652527, "mask": 0.0}, "comments": {"value": null, "prob": 0.061130210757255554, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06067377328872681, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.06075446307659149, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06436700373888016, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.059977978467941284, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06605295836925507, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06209368631243706, "mask": 0.0}, "tags": {"value": null, "prob": 0.06494995951652527, "mask": 0.0}, "comments": {"value": null, "prob": 0.061130210757255554, "mask": 0.0}}], "prob": 0.16394492983818054, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.1428571492433548, "mask": 0.0, "selected": true}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.3445570170879364, "mask": 0.3636363744735718, "selected": true}, "terminate": {"prob": 0.0111677972599864, "mask": 0, "value": null}}, "cls_probs": [0.4052913784980774, 0.4422453045845032, 0.15246334671974182], "s_value": 0.49968159198760986, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 3.9000000059604645}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0904933363199234, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.15237532556056976, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.15621188282966614, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.09510904550552368, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.060141053050756454, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060065604746341705, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06403443962335587, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.060456179082393646, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06608827412128448, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.062296122312545776, "mask": 0.0}, "tags": {"value": null, "prob": 0.06775827705860138, "mask": 0.0}, "comments": {"value": null, "prob": 0.059160031378269196, "mask": 0.0}}, {"title": {"value": null, "prob": 0.060141053050756454, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060065604746341705, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06403443962335587, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060456179082393646, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06608827412128448, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062296122312545776, "mask": 0.0}, "tags": {"value": null, "prob": 0.06775827705860138, "mask": 0.0}, "comments": {"value": null, "prob": 0.059160031378269196, "mask": 0.0}}], "prob": 0.16694334149360657, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.33137601613998413, "mask": 0.4545454680919647}, "terminate": {"prob": 0.0074910689145326614, "mask": 0, "value": null}}, "cls_probs": [0.3360983729362488, 0.4577893018722534, 0.20611228048801422], "s_value": 0.49316680431365967, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 4.000000007450581}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.1092495322227478, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1801796704530716, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11650726944208145, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.0599585622549057, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060017868876457214, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06395428627729416, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.06040723994374275, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06610524654388428, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06234079226851463, "mask": 0.0}, "tags": {"value": null, "prob": 0.0682697594165802, "mask": 0.0}, "comments": {"value": null, "prob": 0.058946285396814346, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0599585622549057, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060017868876457214, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06395428627729416, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06040723994374275, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06610524654388428, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06234079226851463, "mask": 0.0}, "tags": {"value": null, "prob": 0.0682697594165802, "mask": 0.0}, "comments": {"value": null, "prob": 0.058946285396814346, "mask": 0.0}}], "prob": 0.18787360191345215, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.3941054344177246, "mask": 0.4545454680919647, "selected": true}, "terminate": {"prob": 0.012084492482244968, "mask": 0, "value": null}}, "cls_probs": [0.34008297324180603, 0.45213252305984497, 0.2077845335006714], "s_value": 0.49076852202415466, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 4.500000007450581}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.1104218140244484, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.1819838285446167, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11764699965715408, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.059964619576931, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060020629316568375, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06393027305603027, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.0604141540825367, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06609439849853516, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.062367476522922516, "mask": 0.0}, "tags": {"value": null, "prob": 0.06829416006803513, "mask": 0.0}, "comments": {"value": null, "prob": 0.05891432240605354, "mask": 0.0}}, {"title": {"value": null, "prob": 0.059964619576931, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060020629316568375, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06393027305603027, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0604141540825367, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06609439849853516, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062367476522922516, "mask": 0.0}, "tags": {"value": null, "prob": 0.06829416006803513, "mask": 0.0}, "comments": {"value": null, "prob": 0.05891432240605354, "mask": 0.0}}], "prob": 0.1919415295124054, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.38576024770736694, "mask": 0.5454545617103577, "selected": true}, "terminate": {"prob": 0.012245641089975834, "mask": 0, "value": null}}, "cls_probs": [0.34062883257865906, 0.452724426984787, 0.20664672553539276], "s_value": 0.48985543847084045, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 4.600000008940697}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.11190137267112732, "mask": 0.0}, "views": {"value": 0.36, "prob": 0.18401584029197693, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11946118623018265, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.05993855372071266, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.060006480664014816, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06389342993497849, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.06040427088737488, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06609037518501282, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06239766627550125, "mask": 0.0}, "tags": {"value": null, "prob": 0.06842335313558578, "mask": 0.0}, "comments": {"value": null, "prob": 0.05884589999914169, "mask": 0.0}}, {"title": {"value": null, "prob": 0.05993855372071266, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.060006480664014816, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06389342993497849, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06040427088737488, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06609037518501282, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06239766627550125, "mask": 0.0}, "tags": {"value": null, "prob": 0.06842335313558578, "mask": 0.0}, "comments": {"value": null, "prob": 0.05884589999914169, "mask": 0.0}}], "prob": 0.1955355554819107, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.3761915862560272, "mask": 0.6363636255264282}, "terminate": {"prob": 0.01289450004696846, "mask": 0, "value": null}}, "cls_probs": [0.3417923152446747, 0.4522760808467865, 0.20593155920505524], "s_value": 0.4880600869655609, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 4.700000010430813}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.12105946987867355, "mask": 0.0, "selected": true}, "views": {"value": 0.36, "prob": 0.20315340161323547, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.059747371822595596, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.05985567346215248, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0639796033501625, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.06028858572244644, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06626763939857483, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.062301550060510635, "mask": 0.0}, "tags": {"value": null, "prob": 0.06875616312026978, "mask": 0.0}, "comments": {"value": null, "prob": 0.05880344659090042, "mask": 0.0}}, {"title": {"value": null, "prob": 0.059747371822595596, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.05985567346215248, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0639796033501625, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06028858572244644, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06626763939857483, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062301550060510635, "mask": 0.0}, "tags": {"value": null, "prob": 0.06875616312026978, "mask": 0.0}, "comments": {"value": null, "prob": 0.05880344659090042, "mask": 0.0}}], "prob": 0.2145124226808548, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4481169581413269, "mask": 0.6363636255264282}, "terminate": {"prob": 0.013157766312360764, "mask": 0, "value": null}}, "cls_probs": [0.33489614725112915, 0.4547432065010071, 0.21036064624786377], "s_value": 0.48498713970184326, "orig_id": 772, "true_y": 2, "last_cost": 1.0, "total_cost": 5.200000010430813}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.36, "prob": 0.2493138313293457, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.0595327690243721, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.05972186475992203, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06391002237796783, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.060165345668792725, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06643140316009521, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06250269711017609, "mask": 0.0}, "tags": {"value": null, "prob": 0.06945487856864929, "mask": 0.0}, "comments": {"value": null, "prob": 0.05828099697828293, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0595327690243721, "mask": 0.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.05972186475992203, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06391002237796783, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060165345668792725, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06643140316009521, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06250269711017609, "mask": 0.0}, "tags": {"value": null, "prob": 0.06945487856864929, "mask": 0.0}, "comments": {"value": null, "prob": 0.05828099697828293, "mask": 0.0}}], "prob": 0.2573992908000946, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Popular Question", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.471364289522171, "mask": 0.6363636255264282, "selected": true}, "terminate": {"prob": 0.021922508254647255, "mask": 0, "value": null}}, "cls_probs": [0.32339879870414734, 0.47059816122055054, 0.20600302517414093], "s_value": 0.47902536392211914, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 6.200000010430813}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.36, "prob": 0.25338321924209595, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.0589568130671978, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.059191830456256866, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.06357616931200027, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.060194503515958786, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.06666785478591919, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0627673789858818, "mask": 0.0}, "tags": {"value": null, "prob": 0.0719180554151535, "mask": 0.0}, "comments": {"value": null, "prob": 0.05672738328576088, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0589568130671978, "mask": 0.0, "selected": true}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.059191830456256866, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06357616931200027, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060194503515958786, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06666785478591919, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0627673789858818, "mask": 0.0}, "tags": {"value": null, "prob": 0.0719180554151535, "mask": 0.0}, "comments": {"value": null, "prob": 0.05672738328576088, "mask": 0.0}}], "prob": 0.27021291851997375, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.45115432143211365, "mask": 0.7272727489471436}, "terminate": {"prob": 0.025249646976590157, "mask": 0, "value": null}}, "cls_probs": [0.30134347081184387, 0.4681627154350281, 0.23049385845661163], "s_value": 0.47466525435447693, "orig_id": 772, "true_y": 2, "last_cost": 0.20000000298023224, "total_cost": 6.300000011920929}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.36, "prob": 0.25406700372695923, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.06267295777797699, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.06291211396455765, "mask": 0.0, "selected": true}, "score": {"value": 0.05, "prob": 0.06742177158594131, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.06409238278865814, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.07075349241495132, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.06684041768312454, "mask": 0.0}, "tags": {"value": null, "prob": 0.07673297077417374, "mask": 0.0}, "comments": {"value": null, "prob": 0.05989646911621094, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.06291311979293823, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06743087619543076, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06409159302711487, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0707796961069107, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06686446070671082, "mask": 0.0}, "tags": {"value": null, "prob": 0.07675480842590332, "mask": 0.0}, "comments": {"value": null, "prob": 0.059842854738235474, "mask": 0.0}}], "prob": 0.2726266086101532, "mask": 0.0625, "selected": true}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4436933398246765, "mask": 0.7272727489471436}, "terminate": {"prob": 0.029613016173243523, "mask": 0, "value": null}}, "cls_probs": [0.30664893984794617, 0.4571179449558258, 0.23623313009738922], "s_value": 0.47179144620895386, "orig_id": 772, "true_y": 2, "last_cost": 0.5, "total_cost": 6.500000014901161}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.36, "prob": 0.25378692150115967, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.06682038307189941, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.05, "prob": 0.0718144103884697, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.06852168589830399, "mask": 0.0, "selected": true}, "answers": {"value": 0.4, "prob": 0.07548092305660248, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0715041235089302, "mask": 0.0}, "tags": {"value": null, "prob": 0.08255721628665924, "mask": 0.0}, "comments": {"value": null, "prob": 0.06320745497941971, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.06702287495136261, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.07180878520011902, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06852473318576813, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07547049969434738, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.07150303572416306, "mask": 0.0}, "tags": {"value": null, "prob": 0.08255324512720108, "mask": 0.0}, "comments": {"value": null, "prob": 0.06321053206920624, "mask": 0.0}}], "prob": 0.2822243571281433, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4296959340572357, "mask": 0.7272727489471436}, "terminate": {"prob": 0.0342927910387516, "mask": 0, "value": null}}, "cls_probs": [0.30749818682670593, 0.44218820333480835, 0.2503136098384857], "s_value": 0.46942201256752014, "orig_id": 772, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 7.000000014901161}, {"sample": {"about_me": {"value": "<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\n\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.36, "prob": 0.2514667212963104, "mask": 0.0}, "reputation": {"value": 0.0163, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.079, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Reducing the number of variables in a multiple regression", "prob": 0.07162100821733475, "mask": 0.0}, "body": {"value": "<p>I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  <em>Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.</em></p>\n\n<p>Some (probably naive) thoughts on how to do this:</p>\n\n<ol>\n<li>Perform a simple linear regression with each variable and choose the ten with the largest $R^2$ values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.</li>\n<li>Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  </li>\n</ol>\n\n<p>I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.</p>\n\n<blockquote>\n  <p>Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  </p>\n</blockquote>\n\n<p>It seems like this would be a sufficiently common problem that there would be a standard approach.</p>\n\n<p><strike>A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.</strike></p>\n\n<p>whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.05, "prob": 0.07675740122795105, "mask": 0.0}, "views": {"value": 0.3354, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.4, "prob": 0.08106222748756409, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.07709182053804398, "mask": 0.0}, "tags": {"value": null, "prob": 0.08969061076641083, "mask": 0.0}, "comments": {"value": null, "prob": 0.06684200465679169, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>Another way is to use the combinatorial identity $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$.  (See end of post for a proof.)</p>\n\n<p>Dividing by $4^m$, setting $m = n-1$, and extracting the last term, the OP's sum becomes $$ \\\\binom{2n-1}{n} \\\\frac{1}{4^n} + \\\\frac{1}{2} \\\\sum_{j=0}^{n-1}{{j+n-1} \\\\choose j}\\\\frac{1}{2^{j+n-1}} = \\\\binom{2n}{n} \\\\frac{1}{2 \\\\cdot 4^n} + \\\\frac{1}{2}.$$</p>\n\n<p>Since the <a href=\"http://en.wikipedia.org/wiki/Central_binomial_coefficient\" rel=\"nofollow\">central binomial coefficient has asymptotic</a> $\\\\binom{2n}{n} \\\\sim \\\\frac{4^n}{\\\\sqrt{\\\\pi n}}$, the last expression approaches $\\\\frac{1}{2}$ as $n \\\\to \\\\infty$.</p>\n\n<p><HR></p>\n\n<p>Combinatorial proof that $\\\\displaystyle \\\\sum_{j=0}^m \\\\binom{m+j}{j} 2^{m-j}= 4^m$ (borrowed from an <a href=\"http://math.stackexchange.com/a/95255/2370\">answer of mine</a> on math.SE):</p>\n\n<p>Suppose you flip coins until you obtain either $m+1$ heads or $m+1$ tails.  After either heads or tails \"wins\" you keep flipping until you have a total of $2m+1$ coin flips.  The two sides count the number of ways for heads to win.</p>\n\n<p><em>For the left side</em>: Condition on the number of tails $j$ obtained before head $m+1$.  There are $\\\\binom{m+j}{j}$ ways to choose the positions at which these $j$ tails occurred from the $m+j$ total options, and then $2^{m-j}$ possibilities for the remaining flips after head $m+1$.  Summing up yields the left side.</p>\n\n<p><em>For the right side</em>: Heads wins on half of the total number of sequences; i.e., $\\\\frac{1}{2}(2^{2m+1}) = 4^m$.\n<HR>\n<em>Added</em>: Byron Schmuland has <a href=\"http://math.stackexchange.com/a/287663/2370\">recently answered</a> this question on math.SE as well.  My answer is similar to his.</p>\n", "prob": 0.07182168960571289, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.07675299793481827, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07375600934028625, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0810311958193779, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.07705949246883392, "mask": 0.0}, "tags": {"value": null, "prob": 0.08960546553134918, "mask": 0.0}, "comments": {"value": null, "prob": 0.06690796464681625, "mask": 0.0}}], "prob": 0.29789990186691284, "mask": 0.1875}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Constituent", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Notable Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4168749451637268, "mask": 0.7272727489471436}, "terminate": {"prob": 0.03375841677188873, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3108658492565155, 0.43422654271125793, 0.25490763783454895], "s_value": 0.46745288372039795, "orig_id": 772, "true_y": 2, "last_cost": 0.0, "total_cost": 7.100000016391277}], [{"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6375, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.042067524045705795, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 1, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 6375, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.04391631484031677, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.05027197301387787, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.056341566145420074, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06461223214864731, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.037622831761837006, "mask": 0.0}, "website": {"value": 1, "prob": 0.6556787490844727, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.07927537709474564, "mask": 0.0}, "terminate": {"prob": 0.012280893512070179, "mask": 0, "value": null}}, "cls_probs": [0.46822142601013184, 0.3963750898838043, 0.13540349900722504], "s_value": 0.5218657851219177, "orig_id": 6375, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.09018012136220932, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.12140507996082306, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.14582252502441406, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.18462489545345306, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08716776967048645, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.36402860283851624, "mask": 0.0}, "terminate": {"prob": 0.0067710005678236485, "mask": 0, "value": null}}, "cls_probs": [0.42937490344047546, 0.4097268581390381, 0.16089825332164764], "s_value": 0.5135658979415894, "orig_id": 6375, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.1384030282497406, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.15711882710456848, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.17732319235801697, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1253311038017273, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.23306603729724884, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.16875775158405304, "mask": 0, "value": null}}, "cls_probs": [0.4821184277534485, 0.3558279871940613, 0.162053644657135], "s_value": 0.508072018623352, "orig_id": 6375, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Colombian developer. Studied Computer and Systems Engineering at <em>Universidad de los Andes</em> in my home city, Bogot\u00e1 D.C.</p>\n", "prob": 0.12073533236980438, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.14504460990428925, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.15745475888252258, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.11325578391551971, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3287198543548584, "mask": 0.0}, "terminate": {"prob": 0.1347896158695221, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4936491847038269, 0.34294945001602173, 0.16340139508247375], "s_value": 0.5052444338798523, "orig_id": 6375, "true_y": 0, "last_cost": 0.0, "total_cost": 3.5}], [{"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.10069730132818222, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13613314926624298, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11296160519123077, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1270637810230255, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09370279312133789, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.12692825496196747, "mask": 0.0}, "badges": {"value": null, "prob": 0.14839661121368408, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.15411652624607086, "mask": 0, "value": null}}, "cls_probs": [0.4678768515586853, 0.36892351508140564, 0.16319963335990906], "s_value": 0.5223966836929321, "orig_id": 4721, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.09285223484039307, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1355995386838913, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11104028671979904, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.11943906545639038, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.08974386006593704, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.10861422121524811, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.23342055082321167, "mask": 0.0}, "terminate": {"prob": 0.1092902347445488, "mask": 0, "value": null}}, "cls_probs": [0.47724610567092896, 0.3551044464111328, 0.1676495373249054], "s_value": 0.5168143510818481, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.09946420043706894, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.14871437847614288, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.12220072001218796, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09945562481880188, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.11982812732458115, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.271849125623703, "mask": 0.0}, "terminate": {"prob": 0.13848784565925598, "mask": 0, "value": null}}, "cls_probs": [0.4860782325267792, 0.3461034595966339, 0.1678183376789093], "s_value": 0.5176160335540771, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.10663005709648132, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16518361866474152, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.13378028571605682, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13170070946216583, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.34426817297935486, "mask": 0.0}, "terminate": {"prob": 0.11843711137771606, "mask": 0, "value": null}}, "cls_probs": [0.4837115406990051, 0.35306036472320557, 0.16322804987430573], "s_value": 0.5176160931587219, "orig_id": 4721, "true_y": 0, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.12673915922641754, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1924448162317276, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.15718016028404236, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3965432643890381, "mask": 0.0}, "terminate": {"prob": 0.12709259986877441, "mask": 0, "value": null}}, "cls_probs": [0.4882301986217499, 0.3430747985839844, 0.1686950922012329], "s_value": 0.5090709328651428, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.13917092978954315, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.2227565348148346, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.4931984841823578, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.14487400650978088, "mask": 0, "value": null}}, "cls_probs": [0.49363359808921814, 0.35386261343955994, 0.15250371396541595], "s_value": 0.5059800148010254, "orig_id": 4721, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.2410597950220108, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.31915929913520813, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.4397808611392975, "mask": 0, "value": null}}, "cls_probs": [0.487070232629776, 0.3374798893928528, 0.17544983327388763], "s_value": 0.4972792863845825, "orig_id": 4721, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Hi.</p>\n\n<hr>\n\n<p>SO Milestones:</p>\n\n<ul>\n<li>2nd user to get the <a href=\"http://stackoverflow.com/badges/158/stl?userid=87234\">silver stl badge</a>.  </li>\n<li>3rd user to get the <a href=\"http://stackoverflow.com/badges/109/templates?userid=87234\">silver templates badge</a>.   </li>\n<li>15th user to get the <a href=\"http://stackoverflow.com/badges/49/c?userid=87234\">gold c++ badge</a>.  </li>\n<li>27th user to get the <a href=\"http://stackoverflow.com/badges/52/c?userid=87234\">silver c badge</a>.</li>\n<li>102nd user to get the <a href=\"http://stackoverflow.com/badges/146/legendary?userid=87234\">legendary badge</a>.</li>\n</ul>\n\n<hr>\n\n<p>Favorite answers:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550\">What is the copy-and-swap idiom?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3181766/how-far-to-go-with-a-strongly-typed-language/3181803#3181803\">How far to go with a strongly typed language?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/3280410/c-why-doesnt-delete-destroy-anything/3280465#3280465\">[C++] Why doesn't delete destroy anything ?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3582001/advantages-of-using-forward/3582313#3582313\">Advantage of using forward</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/2474018/when-does-invoking-a-member-function-on-a-null-instance-result-in-undefined-behav/2474021#2474021\">When does invoking a member function on a null instance result in undefined behavior?</a>  </li>\n<li><a href=\"http://stackoverflow.com/a/14169897/87234\">Is it possible to hash pointers in portable C++03 code?</a> (Value vs. value representation)</li>\n<li><a href=\"http://stackoverflow.com/questions/5547852/string-literals-not-allowed-as-non-type-template-parameters/5548016#5548016\">String literals not allowed as non type template parameters</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/3489138/c-how-do-i-prevent-a-function-from-accepting-a-pointer-that-is-allocated-in-li/3489248#3489248\">C++: How do I prevent a function from accepting a pointer that is allocated in-line?</a></li>\n<li><a href=\"http://stackoverflow.com/questions/4259885/why-do-i-see-strange-values-when-i-print-uninitialized-variables/4259991#4259991\">Why do I see strange values when I print uninitialized variables?</a>  </li>\n<li><a href=\"http://stackoverflow.com/questions/5610298/question-about-pointer-increment/5610311#5610311\">Question about pointer increment</a></li>\n<li><a href=\"http://stackoverflow.com/questions/6122094/building-boostoptions-from-a-string-boostany-map/6123962#6123962\">Building boost::options from a string/boost::any map</a> (Type-erasure)</li>\n</ul>\n", "prob": 0.3600725829601288, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.6399273872375488, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4969930052757263, 0.33425530791282654, 0.16875167191028595], "s_value": 0.49048227071762085, "orig_id": 4721, "true_y": 0, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07299014180898666, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 1, "prob": 0.44816169142723083, "mask": 0.0}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.06295530498027802, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.10522046685218811, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.07615853101015091, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05862271785736084, "mask": 0.0}, "website": {"value": 1, "prob": 0.4521936774253845, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.09785228222608566, "mask": 0.0}, "badges": {"value": null, "prob": 0.1337452083826065, "mask": 0.0}, "terminate": {"prob": 0.013251753523945808, "mask": 0, "value": null}}, "cls_probs": [0.4319295287132263, 0.427225798368454, 0.1408446729183197], "s_value": 0.5171808004379272, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.08859717100858688, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.16703610122203827, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.1169433668255806, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08857203274965286, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21760335564613342, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.31400683522224426, "mask": 0.0}, "terminate": {"prob": 0.007241107057780027, "mask": 0, "value": null}}, "cls_probs": [0.40443795919418335, 0.4317300319671631, 0.16383197903633118], "s_value": 0.5111697316169739, "orig_id": 798, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.11207418888807297, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.21190635859966278, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.14827516674995422, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10919298976659775, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.4108511209487915, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007700175046920776, "mask": 0, "value": null}}, "cls_probs": [0.4109278917312622, 0.4235374331474304, 0.1655346304178238], "s_value": 0.5099714994430542, "orig_id": 798, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.09632296860218048, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.1869199573993683, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.13138839602470398, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09610584378242493, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.48156091570854187, "mask": 0.0}, "terminate": {"prob": 0.007701976224780083, "mask": 0, "value": null}}, "cls_probs": [0.420354962348938, 0.41720423102378845, 0.16244079172611237], "s_value": 0.5087480545043945, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.11435388028621674, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1563294678926468, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11405779421329498, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.6047041416168213, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010554749518632889, "mask": 0, "value": null}}, "cls_probs": [0.4282747507095337, 0.4119039475917816, 0.1598212569952011], "s_value": 0.5004346370697021, "orig_id": 798, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.2939987778663635, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.3764704167842865, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.2898120582103729, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03971872478723526, "mask": 0, "value": null}}, "cls_probs": [0.4309205114841461, 0.40386655926704407, 0.16521292924880981], "s_value": 0.48712825775146484, "orig_id": 798, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.5138235092163086, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.4113784730434418, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.07479795813560486, "mask": 0, "value": null}}, "cls_probs": [0.4288327693939209, 0.4081666171550751, 0.16300052404403687], "s_value": 0.4792153835296631, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.8812556266784668, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.1187443733215332, "mask": 0, "value": null}}, "cls_probs": [0.4150020480155945, 0.4189769923686981, 0.166020929813385], "s_value": 0.4762350618839264, "orig_id": 798, "true_y": 0, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>I've been working for a public school district since 2006 where I am the Technology Administrator of an RTI/Intervention program.</p>\n\n<p>Starting in June 2010, I began working a second full time job at an educational materials publishing company.</p>\n\n<p>Some of my interests are PHP, K-12 education, ed-tech, ancient history &amp; language (particularly Egypt, and Pre-Columbian Mesoamerica), and non-fiction books.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4168553054332733, 0.4308035671710968, 0.1523410826921463], "s_value": 0.47047990560531616, "orig_id": 798, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0153, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5044, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0153, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.05408487096428871, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 5044, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.06445568054914474, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.09038347750902176, "mask": 0.0}, "reputation": {"value": 0.0153, "prob": 0.07403357326984406, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0690668448805809, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.07780817896127701, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06319841742515564, "mask": 0.0}, "badges": {"value": null, "prob": 0.07056259363889694, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.49049118161201477, "mask": 0, "value": null}}, "cls_probs": [0.5421879291534424, 0.3691498339176178, 0.0886622741818428], "s_value": 0.5583254098892212, "orig_id": 5044, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.07681529223918915, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12479124218225479, "mask": 0.0}, "reputation": {"value": 0.0153, "prob": 0.09846535325050354, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.08665137737989426, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.1025218740105629, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.08096524327993393, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.17453356087207794, "mask": 0.0}, "terminate": {"prob": 0.25525590777397156, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5562678575515747, 0.35482901334762573, 0.08890307694673538], "s_value": 0.5543678402900696, "orig_id": 5044, "true_y": 0, "last_cost": 0.0, "total_cost": 2.0}], [{"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.03980446234345436, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4742, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0758814811706543, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05668792128562927, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06153849884867668, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07084273546934128, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04428190737962723, "mask": 0.0}, "website": {"value": 1, "prob": 0.5222911238670349, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06703559309244156, "mask": 0.0}, "badges": {"value": null, "prob": 0.08349686115980148, "mask": 0.0}, "terminate": {"prob": 0.0179438553750515, "mask": 0, "value": null}}, "cls_probs": [0.45079395174980164, 0.4141537547111511, 0.13505227863788605], "s_value": 0.5266971588134766, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.13734787702560425, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0983215868473053, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11403992772102356, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14131930470466614, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.075611412525177, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17829416692256927, "mask": 0.0}, "badges": {"value": null, "prob": 0.24711892008781433, "mask": 0.0}, "terminate": {"prob": 0.007946811616420746, "mask": 0, "value": null}}, "cls_probs": [0.416081041097641, 0.42081525921821594, 0.16310371458530426], "s_value": 0.5135489702224731, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.11444112658500671, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.13067130744457245, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16121478378772736, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08792131394147873, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19852562248706818, "mask": 0.0}, "badges": {"value": null, "prob": 0.29675865173339844, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010467209853231907, "mask": 0, "value": null}}, "cls_probs": [0.4217439293861389, 0.4186169505119324, 0.15963910520076752], "s_value": 0.5059183835983276, "orig_id": 4742, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1093202456831932, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11865118891000748, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1512615829706192, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08365894109010696, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17032168805599213, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.35544005036354065, "mask": 0.0}, "terminate": {"prob": 0.011346286162734032, "mask": 0, "value": null}}, "cls_probs": [0.42540979385375977, 0.4197789430618286, 0.15481124818325043], "s_value": 0.5064992308616638, "orig_id": 4742, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.12944646179676056, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.14525523781776428, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.18473531305789948, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.09604915976524353, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.43381789326667786, "mask": 0.0}, "terminate": {"prob": 0.01069593895226717, "mask": 0, "value": null}}, "cls_probs": [0.4337429702281952, 0.41125962138175964, 0.15499739348888397], "s_value": 0.5034114122390747, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1584828943014145, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1720559298992157, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.12052424252033234, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5315107703208923, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01742604561150074, "mask": 0, "value": null}}, "cls_probs": [0.44466137886047363, 0.3943439722061157, 0.16099460422992706], "s_value": 0.5013931393623352, "orig_id": 4742, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.3215627372264862, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.36239156126976013, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.2559584975242615, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.060087189078330994, "mask": 0, "value": null}}, "cls_probs": [0.44843748211860657, 0.38730019330978394, 0.16426235437393188], "s_value": 0.49201515316963196, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.5242194533348083, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.3791036605834961, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.09667689353227615, "mask": 0, "value": null}}, "cls_probs": [0.45063090324401855, 0.39457589387893677, 0.15479320287704468], "s_value": 0.4851929247379303, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.8682743906974792, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.13172565400600433, "mask": 0, "value": null}}, "cls_probs": [0.4402303099632263, 0.4018961489200592, 0.15787355601787567], "s_value": 0.48005786538124084, "orig_id": 4742, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>Linux and Death metal, that explains everything about me.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4154738485813141, 0.43056994676589966, 0.15395621955394745], "s_value": 0.47040224075317383, "orig_id": 4742, "true_y": 1, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0771675556898117, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.14489252865314484, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.10301674902439117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12164411693811417, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07795343548059464, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19800105690956116, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.2697715163230896, "mask": 0.0}, "terminate": {"prob": 0.007553049363195896, "mask": 0, "value": null}}, "cls_probs": [0.4237002730369568, 0.4112336337566376, 0.16506606340408325], "s_value": 0.5202891826629639, "orig_id": 1388, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0949157252907753, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.17909090220928192, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.12706594169139862, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1541946977376938, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09352602064609528, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3434055745601654, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007801218889653683, "mask": 0, "value": null}}, "cls_probs": [0.43020501732826233, 0.4035753607749939, 0.16621962189674377], "s_value": 0.5186383128166199, "orig_id": 1388, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.08236602693796158, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.15962271392345428, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.11417411267757416, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12795153260231018, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08348909020423889, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.42421457171440125, "mask": 0.0}, "terminate": {"prob": 0.008181929588317871, "mask": 0, "value": null}}, "cls_probs": [0.44208869338035583, 0.39281198382377625, 0.16509927809238434], "s_value": 0.5172298550605774, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.09546758234500885, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.13257957994937897, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.14495179057121277, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09672586619853973, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5192908048629761, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01098443940281868, "mask": 0, "value": null}}, "cls_probs": [0.4499441683292389, 0.38770461082458496, 0.16235123574733734], "s_value": 0.5090348124504089, "orig_id": 1388, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.20025716722011566, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.2638700008392334, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.29963794350624084, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.20147210359573364, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03476279228925705, "mask": 0, "value": null}}, "cls_probs": [0.45305025577545166, 0.3814181983470917, 0.16553154587745667], "s_value": 0.4996069371700287, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.2708655893802643, "mask": 0.0, "selected": true}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.4011715054512024, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.27631720900535583, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.05164564028382301, "mask": 0, "value": null}}, "cls_probs": [0.45513856410980225, 0.3887125253677368, 0.15614894032478333], "s_value": 0.49249279499053955, "orig_id": 1388, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.523063600063324, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.37851428985595703, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.09842215478420258, "mask": 0, "value": null}}, "cls_probs": [0.450908899307251, 0.3955877125263214, 0.1535034030675888], "s_value": 0.4852537512779236, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.8662078380584717, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.13379211723804474, "mask": 0, "value": null}}, "cls_probs": [0.43950963020324707, 0.4039565622806549, 0.15653379261493683], "s_value": 0.48036158084869385, "orig_id": 1388, "true_y": 0, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>MSc in Advanced Computing (Machine Learning and Data Mining - University of Bristol, UK)\nBSc in Computer Science (University of Ioannina, Greece)</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.41496339440345764, 0.4323996305465698, 0.15263696014881134], "s_value": 0.47062623500823975, "orig_id": 1388, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Computer scientist passionate about how computers can make life better !</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6030, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Computer scientist passionate about how computers can make life better !</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 6030, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Computer scientist passionate about how computers can make life better !</p>\n", "prob": 0.10069730132818222, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.13613314926624298, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.11296160519123077, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1270637810230255, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.09370279312133789, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.12692825496196747, "mask": 0.0}, "badges": {"value": null, "prob": 0.14839661121368408, "mask": 0.0}, "terminate": {"prob": 0.15411652624607086, "mask": 0, "value": null}}, "cls_probs": [0.4678768515586853, 0.36892351508140564, 0.16319963335990906], "s_value": 0.5223966836929321, "orig_id": 6030, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Computer scientist passionate about how computers can make life better !</p>\n", "prob": 0.10989890992641449, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.15248490869998932, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.12665943801403046, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10572978109121323, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.14281430840492249, "mask": 0.0}, "badges": {"value": null, "prob": 0.1787022203207016, "mask": 0.0}, "terminate": {"prob": 0.18371044099330902, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.47654932737350464, 0.35726213455200195, 0.1661885380744934], "s_value": 0.5204572081565857, "orig_id": 6030, "true_y": 0, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3932, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.042067524045705795, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 0, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 3932, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.02999476157128811, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.03465066850185394, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.02927839756011963, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.03309743478894234, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.03524508327245712, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.024411147460341454, "mask": 0.0}, "website": {"value": 0, "prob": 0.7870315313339233, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02629101276397705, "mask": 0, "value": null}}, "cls_probs": [0.47834542393684387, 0.37077370285987854, 0.15088093280792236], "s_value": 0.5253109931945801, "orig_id": 3932, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.02773447148501873, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.026666726917028427, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.025389060378074646, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.02430094964802265, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.024299193173646927, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.023280587047338486, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8483290672302246, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5524676442146301, 0.3324041962623596, 0.11512810736894608], "s_value": 0.5358121991157532, "orig_id": 3932, "true_y": 0, "last_cost": 0.0, "total_cost": 2.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.01, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.05086076632142067, "mask": 0.0, "selected": true}, "views": {"value": 0.02, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 945, "true_y": 2, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0877307653427124, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.06501756608486176, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.08418399095535278, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.050847720354795456, "mask": 0.0}, "website": {"value": 1, "prob": 0.5178436040878296, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0773482546210289, "mask": 0.0}, "badges": {"value": null, "prob": 0.10046575218439102, "mask": 0.0}, "terminate": {"prob": 0.01656230352818966, "mask": 0, "value": null}}, "cls_probs": [0.4403305649757385, 0.4212026000022888, 0.13846680521965027], "s_value": 0.5129270553588867, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.15777775645256042, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.11110181361436844, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.162913516163826, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08445532619953156, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18937227129936218, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.2858344614505768, "mask": 0.0}, "terminate": {"prob": 0.008544918149709702, "mask": 0, "value": null}}, "cls_probs": [0.4063943326473236, 0.43865859508514404, 0.15494704246520996], "s_value": 0.5041645765304565, "orig_id": 945, "true_y": 2, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.1590745896100998, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.1130480095744133, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.1598260998725891, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08786636590957642, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030608389526605606, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030620677396655083, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03202956169843674, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.03015667013823986, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03270278498530388, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03105890564620495, "mask": 0.0}, "tags": {"value": null, "prob": 0.03187672048807144, "mask": 0.0}, "comments": {"value": null, "prob": 0.030946290120482445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030608389526605606, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030620677396655083, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.03202956169843674, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.03015667013823986, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03270278498530388, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03105890564620495, "mask": 0.0}, "tags": {"value": null, "prob": 0.03187672048807144, "mask": 0.0}, "comments": {"value": null, "prob": 0.030946290120482445, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030608389526605606, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030620677396655083, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03202956169843674, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.03015667013823986, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03270278498530388, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03105890564620495, "mask": 0.0}, "tags": {"value": null, "prob": 0.03187672048807144, "mask": 0.0}, "comments": {"value": null, "prob": 0.030946290120482445, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030608389526605606, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030620677396655083, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03202956169843674, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.03015667013823986, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03270278498530388, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03105890564620495, "mask": 0.0}, "tags": {"value": null, "prob": 0.03187672048807144, "mask": 0.0}, "comments": {"value": null, "prob": 0.030946290120482445, "mask": 0.0}}], "prob": 0.17849834263324738, "mask": 0.0}, "badges": {"value": null, "prob": 0.29127877950668335, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010407933965325356, "mask": 0, "value": null}}, "cls_probs": [0.4097227156162262, 0.441092312335968, 0.14918500185012817], "s_value": 0.5029442310333252, "orig_id": 945, "true_y": 2, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.1507377326488495, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.10685035586357117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.14755131304264069, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08325836062431335, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030470585450530052, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030513154342770576, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.032222285866737366, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.02998829260468483, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03298155218362808, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03103744611144066, "mask": 0.0}, "tags": {"value": null, "prob": 0.03194226324558258, "mask": 0.0}, "comments": {"value": null, "prob": 0.030844440683722496, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030470585450530052, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030513154342770576, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.032222285866737366, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02998829260468483, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03298155218362808, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03103744611144066, "mask": 0.0}, "tags": {"value": null, "prob": 0.03194226324558258, "mask": 0.0}, "comments": {"value": null, "prob": 0.030844440683722496, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030470585450530052, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030513154342770576, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.032222285866737366, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.02998829260468483, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03298155218362808, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03103744611144066, "mask": 0.0}, "tags": {"value": null, "prob": 0.03194226324558258, "mask": 0.0}, "comments": {"value": null, "prob": 0.030844440683722496, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030470585450530052, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030513154342770576, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.032222285866737366, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02998829260468483, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03298155218362808, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03103744611144066, "mask": 0.0}, "tags": {"value": null, "prob": 0.03194226324558258, "mask": 0.0}, "comments": {"value": null, "prob": 0.030844440683722496, "mask": 0.0}}], "prob": 0.15741322934627533, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}], "prob": 0.34328991174697876, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01089906133711338, "mask": 0, "value": null}}, "cls_probs": [0.4151965379714966, 0.4408724904060364, 0.14393097162246704], "s_value": 0.5058245658874512, "orig_id": 945, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.15327204763889313, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.10871516913175583, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.14960630238056183, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.08524016290903091, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.03045235201716423, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030506491661071777, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.032195571810007095, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.02998146414756775, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.032959211617708206, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031040657311677933, "mask": 0.0}, "tags": {"value": null, "prob": 0.032020773738622665, "mask": 0.0}, "comments": {"value": null, "prob": 0.0308434646576643, "mask": 0.0}}, {"title": {"value": null, "prob": 0.03045235201716423, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030506491661071777, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.032195571810007095, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02998146414756775, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.032959211617708206, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031040657311677933, "mask": 0.0}, "tags": {"value": null, "prob": 0.032020773738622665, "mask": 0.0}, "comments": {"value": null, "prob": 0.0308434646576643, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.03045235201716423, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030506491661071777, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.032195571810007095, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.02998146414756775, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.032959211617708206, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031040657311677933, "mask": 0.0}, "tags": {"value": null, "prob": 0.032020773738622665, "mask": 0.0}, "comments": {"value": null, "prob": 0.0308434646576643, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.03045235201716423, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030506491661071777, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.032195571810007095, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02998146414756775, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.032959211617708206, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031040657311677933, "mask": 0.0}, "tags": {"value": null, "prob": 0.032020773738622665, "mask": 0.0}, "comments": {"value": null, "prob": 0.0308434646576643, "mask": 0.0}}], "prob": 0.16119325160980225, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.3299514949321747, "mask": 0.1666666716337204}, "terminate": {"prob": 0.012021657079458237, "mask": 0, "value": null}}, "cls_probs": [0.41625872254371643, 0.4396819770336151, 0.14405925571918488], "s_value": 0.5019004344940186, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.18172161281108856, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.13021346926689148, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10405121743679047, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.0303718950599432, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030483540147542953, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03219330683350563, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.029933389276266098, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03299412131309509, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031062308698892593, "mask": 0.0}, "tags": {"value": null, "prob": 0.032191142439842224, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077028878033161, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0303718950599432, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030483540147542953, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.03219330683350563, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.029933389276266098, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03299412131309509, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031062308698892593, "mask": 0.0}, "tags": {"value": null, "prob": 0.032191142439842224, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077028878033161, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.0303718950599432, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030483540147542953, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03219330683350563, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.029933389276266098, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03299412131309509, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031062308698892593, "mask": 0.0}, "tags": {"value": null, "prob": 0.032191142439842224, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077028878033161, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.0303718950599432, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030483540147542953, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03219330683350563, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.029933389276266098, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03299412131309509, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031062308698892593, "mask": 0.0}, "tags": {"value": null, "prob": 0.032191142439842224, "mask": 0.0}, "comments": {"value": null, "prob": 0.03077028878033161, "mask": 0.0}}], "prob": 0.1814310997724533, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.3838336765766144, "mask": 0.1666666716337204, "selected": true}, "terminate": {"prob": 0.01874898560345173, "mask": 0, "value": null}}, "cls_probs": [0.4173847436904907, 0.44010743498802185, 0.14250773191452026], "s_value": 0.5007069110870361, "orig_id": 945, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.18819791078567505, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.13342240452766418, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10893910378217697, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030274419113993645, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030418431386351585, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03214576840400696, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.02989274077117443, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03306405618786812, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031117094680666924, "mask": 0.0}, "tags": {"value": null, "prob": 0.032495081424713135, "mask": 0.0}, "comments": {"value": null, "prob": 0.03059239126741886, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030274419113993645, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030418431386351585, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.03214576840400696, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02989274077117443, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03306405618786812, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031117094680666924, "mask": 0.0}, "tags": {"value": null, "prob": 0.032495081424713135, "mask": 0.0}, "comments": {"value": null, "prob": 0.03059239126741886, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030274419113993645, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030418431386351585, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03214576840400696, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.02989274077117443, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03306405618786812, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031117094680666924, "mask": 0.0}, "tags": {"value": null, "prob": 0.032495081424713135, "mask": 0.0}, "comments": {"value": null, "prob": 0.03059239126741886, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030274419113993645, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030418431386351585, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03214576840400696, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02989274077117443, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03306405618786812, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031117094680666924, "mask": 0.0}, "tags": {"value": null, "prob": 0.032495081424713135, "mask": 0.0}, "comments": {"value": null, "prob": 0.03059239126741886, "mask": 0.0}}], "prob": 0.18932105600833893, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.3604351878166199, "mask": 0.3333333432674408}, "terminate": {"prob": 0.019684335216879845, "mask": 0, "value": null}}, "cls_probs": [0.40849369764328003, 0.4529103934764862, 0.13859589397907257], "s_value": 0.4988193213939667, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.2069060355424881, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.14537739753723145, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.03017478808760643, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030330225825309753, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0321720726788044, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.029851609840989113, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03318142145872116, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031127994880080223, "mask": 0.0}, "tags": {"value": null, "prob": 0.032735031098127365, "mask": 0.0}, "comments": {"value": null, "prob": 0.030426856130361557, "mask": 0.0}}, {"title": {"value": null, "prob": 0.03017478808760643, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030330225825309753, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0321720726788044, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.029851609840989113, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03318142145872116, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031127994880080223, "mask": 0.0}, "tags": {"value": null, "prob": 0.032735031098127365, "mask": 0.0}, "comments": {"value": null, "prob": 0.030426856130361557, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.03017478808760643, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030330225825309753, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0321720726788044, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.029851609840989113, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03318142145872116, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031127994880080223, "mask": 0.0}, "tags": {"value": null, "prob": 0.032735031098127365, "mask": 0.0}, "comments": {"value": null, "prob": 0.030426856130361557, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.03017478808760643, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030330225825309753, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0321720726788044, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.029851609840989113, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03318142145872116, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031127994880080223, "mask": 0.0}, "tags": {"value": null, "prob": 0.032735031098127365, "mask": 0.0}, "comments": {"value": null, "prob": 0.030426856130361557, "mask": 0.0}}], "prob": 0.21168974041938782, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.41636449098587036, "mask": 0.3333333432674408, "selected": true}, "terminate": {"prob": 0.019662348553538322, "mask": 0, "value": null}}, "cls_probs": [0.40305081009864807, 0.4564591348171234, 0.14049004018306732], "s_value": 0.4995158910751343, "orig_id": 945, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 5.200000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.20152603089809418, "mask": 0.0}, "reputation": {"value": 0.0135, "prob": 0.1558847427368164, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030693233013153076, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.031050918623805046, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03209249675273895, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.0299830362200737, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03271935135126114, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031327102333307266, "mask": 0.0}, "tags": {"value": null, "prob": 0.03101195953786373, "mask": 0.0}, "comments": {"value": null, "prob": 0.031121892854571342, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030693233013153076, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.031050918623805046, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.03209249675273895, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0299830362200737, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03271935135126114, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031327102333307266, "mask": 0.0}, "tags": {"value": null, "prob": 0.03101195953786373, "mask": 0.0}, "comments": {"value": null, "prob": 0.031121892854571342, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030693233013153076, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.031050918623805046, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03209249675273895, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.0299830362200737, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03271935135126114, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.031327102333307266, "mask": 0.0}, "tags": {"value": null, "prob": 0.03101195953786373, "mask": 0.0}, "comments": {"value": null, "prob": 0.031121892854571342, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030693233013153076, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.031050918623805046, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03209249675273895, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.0299830362200737, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03271935135126114, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031327102333307266, "mask": 0.0}, "tags": {"value": null, "prob": 0.03101195953786373, "mask": 0.0}, "comments": {"value": null, "prob": 0.031121892854571342, "mask": 0.0}}], "prob": 0.11153490096330643, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.2550368905067444, "mask": 0.5}, "terminate": {"prob": 0.276017427444458, "mask": 0, "value": null}}, "cls_probs": [0.49378982186317444, 0.40430036187171936, 0.1019098311662674], "s_value": 0.5220112800598145, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 5.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.23459170758724213, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0135, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030553266406059265, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.03095730021595955, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.03220517560839653, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.029806163161993027, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.032981764525175095, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03135538846254349, "mask": 0.0}, "tags": {"value": null, "prob": 0.031234432011842728, "mask": 0.0}, "comments": {"value": null, "prob": 0.03090648725628853, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030553266406059265, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.03095730021595955, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.03220517560839653, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.029806163161993027, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.032981764525175095, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03135538846254349, "mask": 0.0}, "tags": {"value": null, "prob": 0.031234432011842728, "mask": 0.0}, "comments": {"value": null, "prob": 0.03090648725628853, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030553266406059265, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.03095730021595955, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03220517560839653, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.029806163161993027, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.032981764525175095, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03135538846254349, "mask": 0.0}, "tags": {"value": null, "prob": 0.031234432011842728, "mask": 0.0}, "comments": {"value": null, "prob": 0.03090648725628853, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030553266406059265, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.03095730021595955, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03220517560839653, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.029806163161993027, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.032981764525175095, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03135538846254349, "mask": 0.0}, "tags": {"value": null, "prob": 0.031234432011842728, "mask": 0.0}, "comments": {"value": null, "prob": 0.03090648725628853, "mask": 0.0}}], "prob": 0.12194281816482544, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.32053816318511963, "mask": 0.5}, "terminate": {"prob": 0.3229272663593292, "mask": 0, "value": null}}, "cls_probs": [0.5002822875976562, 0.41050222516059875, 0.08921550959348679], "s_value": 0.5222763419151306, "orig_id": 945, "true_y": 2, "last_cost": 0.5, "total_cost": 5.800000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0135, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.030538994818925858, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030957508832216263, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.032263241708278656, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.029763871803879738, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03309415280818939, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03140120208263397, "mask": 0.0}, "tags": {"value": null, "prob": 0.031193170696496964, "mask": 0.0}, "comments": {"value": null, "prob": 0.030787859112024307, "mask": 0.0}}, {"title": {"value": null, "prob": 0.030538994818925858, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030957508832216263, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.032263241708278656, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.029763871803879738, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03309415280818939, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03140120208263397, "mask": 0.0}, "tags": {"value": null, "prob": 0.031193170696496964, "mask": 0.0}, "comments": {"value": null, "prob": 0.030787859112024307, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.030538994818925858, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030957508832216263, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.032263241708278656, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.029763871803879738, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03309415280818939, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03140120208263397, "mask": 0.0}, "tags": {"value": null, "prob": 0.031193170696496964, "mask": 0.0}, "comments": {"value": null, "prob": 0.030787859112024307, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.030538994818925858, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030957508832216263, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.032263241708278656, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.029763871803879738, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03309415280818939, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03140120208263397, "mask": 0.0}, "tags": {"value": null, "prob": 0.031193170696496964, "mask": 0.0}, "comments": {"value": null, "prob": 0.030787859112024307, "mask": 0.0}}], "prob": 0.1440349817276001, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.41258668899536133, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.4433782696723938, "mask": 0, "value": null}}, "cls_probs": [0.5174866914749146, 0.40125396847724915, 0.0812593623995781], "s_value": 0.5238761901855469, "orig_id": 945, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 6.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0135, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.03022482618689537, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.030630920082330704, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.032128557562828064, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.02978414297103882, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.033187635242938995, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03145162761211395, "mask": 0.0}, "tags": {"value": null, "prob": 0.03245993331074715, "mask": 0.0}, "comments": {"value": null, "prob": 0.030132368206977844, "mask": 0.0}}, {"title": {"value": null, "prob": 0.03022482618689537, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.030630920082330704, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.032128557562828064, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02978414297103882, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033187635242938995, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03145162761211395, "mask": 0.0}, "tags": {"value": null, "prob": 0.03245993331074715, "mask": 0.0}, "comments": {"value": null, "prob": 0.030132368206977844, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.03022482618689537, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.030630920082330704, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.032128557562828064, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.02978414297103882, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.033187635242938995, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03145162761211395, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.03245993331074715, "mask": 0.0}, "comments": {"value": null, "prob": 0.030132368206977844, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.03022482618689537, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.030630920082330704, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.032128557562828064, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02978414297103882, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.033187635242938995, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03145162761211395, "mask": 0.0}, "tags": {"value": null, "prob": 0.03245993331074715, "mask": 0.0}, "comments": {"value": null, "prob": 0.030132368206977844, "mask": 0.0}}], "prob": 0.16732528805732727, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.39258599281311035, "mask": 0.6666666865348816}, "terminate": {"prob": 0.44008874893188477, "mask": 0, "value": null}}, "cls_probs": [0.4700978398323059, 0.42732158303260803, 0.10258063673973083], "s_value": 0.5023500323295593, "orig_id": 945, "true_y": 2, "last_cost": 0.10000000149011612, "total_cost": 6.4000000059604645}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0135, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What software is used for maps of the US (or other arbitrary areas)?", "prob": 0.03118566796183586, "mask": 0.0}, "body": {"value": "<p>What software does one use to create a map that describes conditions over an arbitrary area ? The example I am thinking of would be a map of the US where states that voted one way or another would be red or blue. Obviously I can get a map of the US and use a graphics program to manually paint in the states, but I am figuring there should be some sort of software to do that in a somewhat automated fashion. Apart from a for-pay web app, I haven't been able to find anything. I am very new to the field so I'm not familiar with the available tools.</p>\n", "prob": 0.03159366548061371, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.033175162971019745, "mask": 0.0}, "views": {"value": 0.0236, "prob": 0.0307500958442688, "mask": 0.0}, "answers": {"value": 0.4, "prob": 0.03429558128118515, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.03248431161046028, "mask": 0.0}, "tags": {"value": null, "prob": 0.03362258896231651, "mask": 0.0}, "comments": {"value": null, "prob": 0.03101077303290367, "mask": 0.0}}, {"title": {"value": null, "prob": 0.03118566796183586, "mask": 0.0}, "body": {"value": "<p>Don't hate the plyr! Hate the game!</p>\n", "prob": 0.03159366548061371, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.033175162971019745, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0307500958442688, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03429558128118515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03248431161046028, "mask": 0.0}, "tags": {"value": null, "prob": 0.03362258896231651, "mask": 0.0}, "comments": {"value": null, "prob": 0.03101077303290367, "mask": 0.0}}, {"title": {"value": "Completely random or structured selection of receipients for an A/B mailing", "prob": 0.031183039769530296, "mask": 0.0}, "body": {"value": "<p>and apologies if this is an overly simple question. \nI am doing an A/B mailing (where we send out two different solicitation letters). It's going out to past donors so we have some information on them and I have a master list of about 18,000 eligible donors. The point is to see which letter produces more and/or greater donations. Should I generate the two groups completely randomly, or should I sort them by amount given in the past year and manually assign them to two groups taking care that each group had the same distribution of people with various donation histories (same number of 'high donors', 'medium donors' and 'low donors'). What is the best way to create two equivalent groups so that examining the amounts donated later will have meaning ?</p>\n", "prob": 0.03159211575984955, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03318062052130699, "mask": 0.0}, "views": {"value": 0.0148, "prob": 0.030748244374990463, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.03430783376097679, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.03364262729883194, "mask": 0.0}, "comments": {"value": null, "prob": 0.03099200688302517, "mask": 0.0}}, {"title": {"value": "Hypothesis testing for more than two samples?", "prob": 0.03118566796183586, "mask": 0.0}, "body": {"value": "<p>We do a monthly fundraising letter at my nonprofit. I know how to set up tests in R to compare whether the proportion of response and amount received between two mailings is significantly different, but I would like to compare all twelve for the last year. I figure there is a test to do that, but I don't know what it would be. </p>\n\n<p>Can someone suggest a way to look at 12 samples at once?</p>\n", "prob": 0.03159366548061371, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.033175162971019745, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.0307500958442688, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03429558128118515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03248431161046028, "mask": 0.0}, "tags": {"value": null, "prob": 0.03362258896231651, "mask": 0.0}, "comments": {"value": null, "prob": 0.03101077303290367, "mask": 0.0}}], "prob": 0.16979758441448212, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.399444580078125, "mask": 0.6666666865348816}, "terminate": {"prob": 0.4307578206062317, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4694259464740753, 0.4267878532409668, 0.1037861630320549], "s_value": 0.5028973817825317, "orig_id": 945, "true_y": 2, "last_cost": 0.0, "total_cost": 6.500000007450581}], [{"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6309, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.042067524045705795, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 1, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 6309, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.05894787237048149, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09721963852643967, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.07257969677448273, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07740547508001328, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.09351205825805664, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05404208227992058, "mask": 0.0}, "website": {"value": 1, "prob": 0.36736834049224854, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.16350437700748444, "mask": 0.0}, "terminate": {"prob": 0.015420478768646717, "mask": 0, "value": null}}, "cls_probs": [0.468865305185318, 0.3843379318714142, 0.14679673314094543], "s_value": 0.5223132371902466, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.070609450340271, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13829325139522552, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09733446687459946, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11277060955762863, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14828462898731232, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06973160058259964, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.3576032519340515, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005372782703489065, "mask": 0, "value": null}}, "cls_probs": [0.4309535324573517, 0.40981411933898926, 0.15923236310482025], "s_value": 0.5181341767311096, "orig_id": 6309, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.11686007678508759, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.20557241141796112, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.14998863637447357, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1788380742073059, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.22089381515979767, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.11334424465894699, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.014502735808491707, "mask": 0, "value": null}}, "cls_probs": [0.4373914897441864, 0.3924471437931061, 0.1701613962650299], "s_value": 0.5081716775894165, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 2.600000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.14768999814987183, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.2635481059551239, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.19266505539417267, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.2258695811033249, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1482059806585312, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.022021234035491943, "mask": 0, "value": null}}, "cls_probs": [0.44457677006721497, 0.38667407631874084, 0.16874921321868896], "s_value": 0.5055890083312988, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.17975889146327972, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.33315739035606384, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.2742854058742523, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.18414533138275146, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.028652949258685112, "mask": 0, "value": null}}, "cls_probs": [0.44827789068222046, 0.39543622732162476, 0.15628582239151], "s_value": 0.4984554350376129, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.2194472849369049, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.4157877564430237, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.3335252106189728, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.031239690259099007, "mask": 0, "value": null}}, "cls_probs": [0.4391236901283264, 0.40005967020988464, 0.16081665456295013], "s_value": 0.4941849112510681, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.37808528542518616, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.5597963929176331, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06211838126182556, "mask": 0, "value": null}}, "cls_probs": [0.4473061263561249, 0.39317646622657776, 0.15951740741729736], "s_value": 0.4871675372123718, "orig_id": 6309, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.3600725829601288, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.6399273872375488, "mask": 0, "value": null}}, "cls_probs": [0.4969930052757263, 0.33425530791282654, 0.16875167191028595], "s_value": 0.49048227071762085, "orig_id": 6309, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Erasmus Mundus in Computational Logic</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4849376082420349, 0.3456471860408783, 0.1694151610136032], "s_value": 0.48671969771385193, "orig_id": 6309, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>I am currently a student at the University of Virginia studying Computer Science and Computer Engineering. I'll be graduating in May 2016. [Will add more later.]</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7632, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am currently a student at the University of Virginia studying Computer Science and Computer Engineering. I'll be graduating in May 2016. [Will add more later.]</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 7632, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am currently a student at the University of Virginia studying Computer Science and Computer Engineering. I'll be graduating in May 2016. [Will add more later.]</p>\n", "prob": 0.10069730132818222, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13613314926624298, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.11296160519123077, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1270637810230255, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09370279312133789, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.12692825496196747, "mask": 0.0}, "badges": {"value": null, "prob": 0.14839661121368408, "mask": 0.0}, "terminate": {"prob": 0.15411652624607086, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4678768515586853, 0.36892351508140564, 0.16319963335990906], "s_value": 0.5223966836929321, "orig_id": 7632, "true_y": 0, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": "<p>I have been a software developer for 30 years, using a variety of programming languages like COBOL, Pascal, C, assembly, and some scripting languages. Now very interested in following the development of the new C++11 standard.</p>\n\n<p>Currently working for a bank, providing file services for corporate customers.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1117, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I have been a software developer for 30 years, using a variety of programming languages like COBOL, Pascal, C, assembly, and some scripting languages. Now very interested in following the development of the new C++11 standard.</p>\n\n<p>Currently working for a bank, providing file services for corporate customers.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 1117, "true_y": 2, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I have been a software developer for 30 years, using a variety of programming languages like COBOL, Pascal, C, assembly, and some scripting languages. Now very interested in following the development of the new C++11 standard.</p>\n\n<p>Currently working for a bank, providing file services for corporate customers.</p>\n", "prob": 0.09414400905370712, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.1390436738729477, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.1099778413772583, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.12478432804346085, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08778016269207001, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.10426583886146545, "mask": 0.0}, "badges": {"value": null, "prob": 0.13248629868030548, "mask": 0.0}, "terminate": {"prob": 0.2075178623199463, "mask": 0, "value": null}}, "cls_probs": [0.4944605827331543, 0.3901350796222687, 0.11540437489748001], "s_value": 0.5293203592300415, "orig_id": 1117, "true_y": 2, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I have been a software developer for 30 years, using a variety of programming languages like COBOL, Pascal, C, assembly, and some scripting languages. Now very interested in following the development of the new C++11 standard.</p>\n\n<p>Currently working for a bank, providing file services for corporate customers.</p>\n", "prob": 0.10634126514196396, "mask": 0.0, "selected": true}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1269155740737915, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.13853169977664948, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10019329190254211, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.11563274264335632, "mask": 0.0}, "badges": {"value": null, "prob": 0.16562098264694214, "mask": 0.0}, "terminate": {"prob": 0.2467644363641739, "mask": 0, "value": null}}, "cls_probs": [0.5135640501976013, 0.38272368907928467, 0.10371221601963043], "s_value": 0.5310320854187012, "orig_id": 1117, "true_y": 2, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I have been a software developer for 30 years, using a variety of programming languages like COBOL, Pascal, C, assembly, and some scripting languages. Now very interested in following the development of the new C++11 standard.</p>\n\n<p>Currently working for a bank, providing file services for corporate customers.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.12407569587230682, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.13077472150325775, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10171672701835632, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.10549657791852951, "mask": 0.0}, "badges": {"value": null, "prob": 0.14035627245903015, "mask": 0.0}, "terminate": {"prob": 0.39757999777793884, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5083661079406738, 0.38787969946861267, 0.1037542074918747], "s_value": 0.5264249444007874, "orig_id": 1117, "true_y": 2, "last_cost": 0.0, "total_cost": 2.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.029, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1278, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.029, "prob": 0.08534761518239975, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 0, "prob": 0.3274863362312317, "mask": 0.0}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}], "prob": 0.15204817056655884, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 1278, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.08222529292106628, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.11558158695697784, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.0887361392378807, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.10134071856737137, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.029, "prob": 0.11187542974948883, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07821159809827805, "mask": 0.0}, "website": {"value": 0, "prob": 0.11650394648313522, "mask": 0.0}, "posts": {"value": null, "prob": 0.0971793681383133, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.19588889181613922, "mask": 0.1666666716337204}, "terminate": {"prob": 0.012457004748284817, "mask": 0, "value": null}}, "cls_probs": [0.36740267276763916, 0.40339019894599915, 0.2292071431875229], "s_value": 0.519029974937439, "orig_id": 1278, "true_y": 1, "last_cost": 0.5, "total_cost": 1.1000000014901161}, {"sample": {"about_me": {"value": null, "prob": 0.09205058217048645, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.13078051805496216, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.09827014803886414, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.13062399625778198, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0866299644112587, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.12421516329050064, "mask": 0.0}, "posts": {"value": null, "prob": 0.1132107526063919, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.2136385291814804, "mask": 0.1666666716337204}, "terminate": {"prob": 0.010580343194305897, "mask": 0, "value": null}}, "cls_probs": [0.3482891917228699, 0.4251660704612732, 0.2265448421239853], "s_value": 0.5084520578384399, "orig_id": 1278, "true_y": 1, "last_cost": 0.5, "total_cost": 1.6000000014901161}, {"sample": {"about_me": {"value": null, "prob": 0.0990288034081459, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.14335817098617554, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.1071215346455574, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.1443740278482437, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.11629621684551239, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.12647421658039093, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.2529505789279938, "mask": 0.1666666716337204}, "terminate": {"prob": 0.01039651408791542, "mask": 0, "value": null}}, "cls_probs": [0.3405407965183258, 0.42714568972587585, 0.23231349885463715], "s_value": 0.5065534710884094, "orig_id": 1278, "true_y": 1, "last_cost": 0.5, "total_cost": 2.100000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.11026739329099655, "mask": 0.0}, "views": {"value": 0.14, "prob": 0.16199198365211487, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.1248454824090004, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.1515258252620697, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13342146575450897, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Enthusiast", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.2805098593235016, "mask": 0.1666666716337204, "selected": true}, "terminate": {"prob": 0.037437960505485535, "mask": 0, "value": null}}, "cls_probs": [0.3945757746696472, 0.4121372699737549, 0.19328701496124268], "s_value": 0.5037010908126831, "orig_id": 1278, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 2.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.11299148947000504, "mask": 0.0, "selected": true}, "views": {"value": 0.14, "prob": 0.16389600932598114, "mask": 0.0}, "reputation": {"value": 0.0171, "prob": 0.1265195906162262, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.1544724553823471, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13659489154815674, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.2658992409706116, "mask": 0.3333333432674408}, "terminate": {"prob": 0.03962641581892967, "mask": 0, "value": null}}, "cls_probs": [0.3934801518917084, 0.4145662188529968, 0.191953644156456], "s_value": 0.5021193027496338, "orig_id": 1278, "true_y": 1, "last_cost": 1.0, "total_cost": 2.7000000029802322}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.14, "prob": 0.18724070489406586, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0171, "prob": 0.1483532041311264, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.1688988208770752, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.14545735716819763, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.24319788813591003, "mask": 0.3333333432674408}, "terminate": {"prob": 0.10685205459594727, "mask": 0, "value": null}}, "cls_probs": [0.4008638262748718, 0.4196229577064514, 0.17951320111751556], "s_value": 0.49810484051704407, "orig_id": 1278, "true_y": 1, "last_cost": 0.5, "total_cost": 3.7000000029802322}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.14, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0171, "prob": 0.1789213865995407, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.1993459314107895, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17342300713062286, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.3397708535194397, "mask": 0.3333333432674408}, "terminate": {"prob": 0.10853883624076843, "mask": 0, "value": null}}, "cls_probs": [0.40768909454345703, 0.41909825801849365, 0.17321263253688812], "s_value": 0.4930669069290161, "orig_id": 1278, "true_y": 1, "last_cost": 0.5, "total_cost": 4.200000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.14, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0171, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.23264601826667786, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20244769752025604, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.4406949579715729, "mask": 0.3333333432674408, "selected": true}, "terminate": {"prob": 0.12421132624149323, "mask": 0, "value": null}}, "cls_probs": [0.41195935010910034, 0.4292716383934021, 0.15876899659633636], "s_value": 0.488941490650177, "orig_id": 1278, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.14, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0171, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.029, "prob": 0.23016317188739777, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2176879495382309, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Enthusiast", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Caucus", "prob": 0.0, "mask": 1.0}}], "prob": 0.41867849230766296, "mask": 0.5}, "terminate": {"prob": 0.133470356464386, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.34821051359176636, 0.44353440403938293, 0.20825505256652832], "s_value": 0.4715976119041443, "orig_id": 1278, "true_y": 1, "last_cost": 0.0, "total_cost": 4.800000004470348}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5681, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.042067524045705795, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 1, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 5681, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.06662319600582123, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.05167224258184433, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05702978000044823, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06387162208557129, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04002488777041435, "mask": 0.0}, "website": {"value": 1, "prob": 0.6303839683532715, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06947559863328934, "mask": 0.0}, "terminate": {"prob": 0.02091871201992035, "mask": 0, "value": null}}, "cls_probs": [0.4663267731666565, 0.3957247734069824, 0.1379484236240387], "s_value": 0.5241990089416504, "orig_id": 5681, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.16579581797122955, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.11857020109891891, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.13938888907432556, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1726287454366684, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.08754266798496246, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.30701252818107605, "mask": 0.0}, "terminate": {"prob": 0.009061187505722046, "mask": 0, "value": null}}, "cls_probs": [0.4324747920036316, 0.4096691608428955, 0.15785610675811768], "s_value": 0.5123176574707031, "orig_id": 5681, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.20327280461788177, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.1469482183456421, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1718290150165558, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1112503781914711, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.35249799489974976, "mask": 0.0}, "terminate": {"prob": 0.014201649464666843, "mask": 0, "value": null}}, "cls_probs": [0.43817898631095886, 0.4014558494091034, 0.16036513447761536], "s_value": 0.5111439228057861, "orig_id": 5681, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.1826571524143219, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.210174098610878, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1381080150604248, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.44842228293418884, "mask": 0.0}, "terminate": {"prob": 0.020638441666960716, "mask": 0, "value": null}}, "cls_probs": [0.44403693079948425, 0.3990388512611389, 0.1569242626428604], "s_value": 0.5039722919464111, "orig_id": 5681, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.20474787056446075, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.2344890832901001, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5406833291053772, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02007959969341755, "mask": 0, "value": null}}, "cls_probs": [0.4359041750431061, 0.4068414270877838, 0.1572543829679489], "s_value": 0.49983829259872437, "orig_id": 5681, "true_y": 1, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.44133591651916504, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.5030684471130371, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.05559564009308815, "mask": 0, "value": null}}, "cls_probs": [0.44170886278152466, 0.39860963821411133, 0.15968142449855804], "s_value": 0.49326467514038086, "orig_id": 5681, "true_y": 1, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.08195291459560394, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.9180471301078796, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5162574648857117, 0.3304857015609741, 0.1532568633556366], "s_value": 0.5017420053482056, "orig_id": 5681, "true_y": 1, "last_cost": 0.0, "total_cost": 5.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0145, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4709, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0451396144926548, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.0725313052535057, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0145, "prob": 0.05382183939218521, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05918215960264206, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.07011939585208893, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.531177282333374, "mask": 0.0}, "posts": {"value": null, "prob": 0.06768590956926346, "mask": 0.0}, "badges": {"value": null, "prob": 0.08940514922142029, "mask": 0.0}, "terminate": {"prob": 0.010937291197478771, "mask": 0, "value": null}}, "cls_probs": [0.45435237884521484, 0.41012728214263916, 0.1355203092098236], "s_value": 0.5306205749511719, "orig_id": 4709, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.04989733174443245, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0145, "prob": 0.061337608844041824, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06471563130617142, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.07778343558311462, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.5482240915298462, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07324302941560745, "mask": 0.0}, "badges": {"value": null, "prob": 0.11073573678731918, "mask": 0.0}, "terminate": {"prob": 0.01406315341591835, "mask": 0, "value": null}}, "cls_probs": [0.47294917702674866, 0.40643733739852905, 0.12061350047588348], "s_value": 0.5301374197006226, "orig_id": 4709, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.06912443041801453, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0145, "prob": 0.08259233087301254, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07402826845645905, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.08397553861141205, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0672217383980751, "mask": 0.0}, "badges": {"value": null, "prob": 0.0899166613817215, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.533141016960144, "mask": 0, "value": null}}, "cls_probs": [0.5589068531990051, 0.3618159294128418, 0.07927723973989487], "s_value": 0.5571296215057373, "orig_id": 4709, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.0840916782617569, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0145, "prob": 0.11108212918043137, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09436602145433426, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.11147492378950119, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0870283767580986, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.2199985235929489, "mask": 0.0}, "terminate": {"prob": 0.29195836186408997, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5745725035667419, 0.3465985059738159, 0.07882893085479736], "s_value": 0.554265022277832, "orig_id": 4709, "true_y": 0, "last_cost": 0.0, "total_cost": 2.5}], [{"sample": {"about_me": {"value": "<p>Bachelor in Applied Mathematics with minor of Computer Science, Linn\u00e9universitetet, Sweden</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7371, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Bachelor in Applied Mathematics with minor of Computer Science, Linn\u00e9universitetet, Sweden</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 7371, "true_y": 0, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0347, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.022, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0347, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.022, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0347, "prob": 0.09319149702787399, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11058338731527328, "mask": 0.0}, "up_votes": {"value": 0.022, "prob": 0.14358055591583252, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 3735, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.06393192708492279, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.12605567276477814, "mask": 0.0}, "reputation": {"value": 0.0347, "prob": 0.08850457519292831, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09802098572254181, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.022, "prob": 0.13112056255340576, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1466534584760666, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.3399604856967926, "mask": 0.0}, "terminate": {"prob": 0.005752329248934984, "mask": 0, "value": null}}, "cls_probs": [0.415014386177063, 0.4198645353317261, 0.16512107849121094], "s_value": 0.5162007808685303, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.0710844174027443, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.14111016690731049, "mask": 0.0}, "reputation": {"value": 0.0347, "prob": 0.09737872332334518, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.1496697962284088, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16918617486953735, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.36655429005622864, "mask": 0.0}, "terminate": {"prob": 0.0050164684653282166, "mask": 0, "value": null}}, "cls_probs": [0.39762547612190247, 0.4375667870044708, 0.16480770707130432], "s_value": 0.5092275738716125, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.08352576941251755, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.16429315507411957, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0347, "prob": 0.11483292281627655, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18378308415412903, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.44570332765579224, "mask": 0.0}, "terminate": {"prob": 0.007861712016165257, "mask": 0, "value": null}}, "cls_probs": [0.40243852138519287, 0.4302836060523987, 0.16727784276008606], "s_value": 0.5080493688583374, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.11068440228700638, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.14723889529705048, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20314985513687134, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0, "selected": true}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.5218874216079712, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.017039310187101364, "mask": 0, "value": null}}, "cls_probs": [0.38262274861335754, 0.4433181583881378, 0.17405912280082703], "s_value": 0.509635329246521, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.11321886628866196, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.1498952955007553, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2104235291481018, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.07692307978868484, "mask": 0.0, "selected": true}}, {"badge": {"value": "Tumbleweed", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.07692307978868484, "mask": 0.0}}], "prob": 0.5090623497962952, "mask": 0.0714285746216774, "selected": true}, "terminate": {"prob": 0.017399996519088745, "mask": 0, "value": null}}, "cls_probs": [0.3798857033252716, 0.4483098089694977, 0.1718045324087143], "s_value": 0.5101537704467773, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.11506286263465881, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.15166403353214264, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21474623680114746, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0833333358168602, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.0833333358168602, "mask": 0.0}}], "prob": 0.5006126165390015, "mask": 0.1428571492433548, "selected": true}, "terminate": {"prob": 0.017914319410920143, "mask": 0, "value": null}}, "cls_probs": [0.37786534428596497, 0.4493662714958191, 0.17276832461357117], "s_value": 0.5091678500175476, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.7000000029802322}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.11663910746574402, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.15338018536567688, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21886833012104034, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Organizer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.09090909361839294, "mask": 0.0}}], "prob": 0.4926464855670929, "mask": 0.2142857164144516}, "terminate": {"prob": 0.0184659194201231, "mask": 0, "value": null}}, "cls_probs": [0.3783760666847229, 0.44919732213020325, 0.17242659628391266], "s_value": 0.5082165598869324, "orig_id": 3735, "true_y": 0, "last_cost": 1.0, "total_cost": 3.8000000044703484}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.11600833386182785, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.15392537415027618, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.005248067434877157, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.005273130722343922, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005590209737420082, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.0052169570699334145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005755132529884577, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054040090180933475, "mask": 0.0}, "tags": {"value": null, "prob": 0.005708958953619003, "mask": 0.0}, "comments": {"value": null, "prob": 0.005281790159642696, "mask": 0.0}}], "prob": 0.20323175191879272, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0, "selected": true}}, {"badge": {"value": "Organizer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.09090909361839294, "mask": 0.0}}], "prob": 0.5044201612472534, "mask": 0.2142857164144516, "selected": true}, "terminate": {"prob": 0.022414354607462883, "mask": 0, "value": null}}, "cls_probs": [0.38353431224823, 0.4545036852359772, 0.16196201741695404], "s_value": 0.5119277238845825, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.800000004470348}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.12755534052848816, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.16607359051704407, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.005263729486614466, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.0052989753894507885, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005602195858955383, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.00520434882491827, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005756056401878595, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0054143150337040424, "mask": 0.0}, "tags": {"value": null, "prob": 0.005632421001791954, "mask": 0.0}, "comments": {"value": null, "prob": 0.005306214094161987, "mask": 0.0}}], "prob": 0.190404012799263, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.10000000149011612, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Organizer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.10000000149011612, "mask": 0.0}}], "prob": 0.4765240252017975, "mask": 0.2857142984867096, "selected": true}, "terminate": {"prob": 0.039443086832761765, "mask": 0, "value": null}}, "cls_probs": [0.4043766260147095, 0.45272761583328247, 0.14289583265781403], "s_value": 0.5105412006378174, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.9000000059604645}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.12965022027492523, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.16806048154830933, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.005261485930532217, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.005297104828059673, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599783267825842, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.005203778389841318, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057531665079295635, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.00541351642459631, "mask": 0.0}, "tags": {"value": null, "prob": 0.005642791744321585, "mask": 0.0}, "comments": {"value": null, "prob": 0.00530663039535284, "mask": 0.0}}], "prob": 0.1940436214208603, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Organizer", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.1111111119389534, "mask": 0.0}}], "prob": 0.4664892256259918, "mask": 0.3571428656578064}, "terminate": {"prob": 0.04175642877817154, "mask": 0, "value": null}}, "cls_probs": [0.4025251567363739, 0.4527164101600647, 0.14475838840007782], "s_value": 0.507036030292511, "orig_id": 3735, "true_y": 0, "last_cost": 0.5, "total_cost": 5.000000007450581}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.1548939347267151, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.0052511924877762794, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.0052941469475626945, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005607780534774065, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.00519002228975296, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057745035737752914, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420756991952658, "mask": 0.0}, "tags": {"value": null, "prob": 0.005653913598507643, "mask": 0.0}, "comments": {"value": null, "prob": 0.005285944323986769, "mask": 0.0}}], "prob": 0.21664226055145264, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.1111111119389534, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Organizer", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.1111111119389534, "mask": 0.0}}], "prob": 0.5687422156333923, "mask": 0.3571428656578064, "selected": true}, "terminate": {"prob": 0.05972156673669815, "mask": 0, "value": null}}, "cls_probs": [0.4103944003582001, 0.45544904470443726, 0.13415652513504028], "s_value": 0.5055583119392395, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.500000007450581}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.15725868940353394, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.0052510593086481094, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.005294228903949261, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005606279708445072, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.0051894765347242355, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0057718693278729916, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005420316476374865, "mask": 0.0}, "tags": {"value": null, "prob": 0.005656080320477486, "mask": 0.0}, "comments": {"value": null, "prob": 0.005288953892886639, "mask": 0.0}}], "prob": 0.22081656754016876, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Organizer", "prob": 0.125, "mask": 0.0, "selected": true}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.125, "mask": 0.0}}], "prob": 0.559563934803009, "mask": 0.4285714328289032, "selected": true}, "terminate": {"prob": 0.06236080825328827, "mask": 0, "value": null}}, "cls_probs": [0.4101180136203766, 0.4553736448287964, 0.1345083862543106], "s_value": 0.503185510635376, "orig_id": 3735, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.600000008940697}, {"sample": {"about_me": {"value": "<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\n", "prob": 0.1611155867576599, "mask": 0.0}, "views": {"value": 0.84, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0347, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.022, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Is discriminant analysis supervised learning?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Is linear discriminant analysis, specifically Linear Programming Discriminant Analysis (LPDA), supervised learning? Can you provide a valid reference that states so if possible.</p>\n\n<p>My study supervisor and I have been disagreeing about it. I'm convinced linear discriminant analysis, whether Fisher LDA or LPDA, is supervised learning. Both techniques use a labelled set of objects to derive a function which can be used to predict class labels for unlabelled objects. </p>\n\n<p>My study supervisor does not agree, stating that nothing is \"learned\" when using discriminant analysis. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0196, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Selecting optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\n\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0184, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Random search for the optimal number of input features and optimal number of hidden layers for a MLP?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I've performed a random search in hypothesis space $$\\\\{(c,h)| c \\\\in U[1,256]; h\\\\in U[1,100];c \\\\in \\\\mathrm{Z} \\\\text{ and } h \\\\in \\\\mathrm{Z}\\\\}$$  that defines the parameters of a standard multilayer perceptron (MLP) neural network. </p>\n\n<p>In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair.</p>\n\n<p>The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped.</p>\n\n<p>The results over the hypothesis space in the training data is:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ITpEf.png\" alt=\"Training sample estimated errors\"></p>\n\n<p>The results over the hypothesis space in the hold-out data is,</p>\n\n<p><img src=\"http://i.stack.imgur.com/kXlYE.png\" alt=\"Hold out sample estimated errors\"></p>\n\n<p>This question relates to work I've done as part of my masters dissertation, and is related to the question here <a href=\"http://stats.stackexchange.com/questions/43648/selecting-optimal-number-of-input-features-and-optimal-number-of-hidden-layers-f\">this</a></p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0097, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Overfitting a linear Linear Discriminant Function", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I am estimating a Linear Discriminant function with 250 input variables over 4000 data records. Should I consider feature selection, am I over fitting the model? How do I know when feature selection should be considered? I'm using a Linear Programming Discriminant Analysis model proposed by  <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/1520-6750%28199206%2939:4%3C545%3a%3aAID-NAV3220390408%3E3.0.CO;2-A/abstract\" rel=\"nofollow\">Stam and Ragsdale</a></p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0077, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>You might also be interested in the answers of these questions.</p>\n\n<ul>\n<li><a href=\"http://stats.stackexchange.com/questions/166/how-do-you-decide-the-sample-size-when-polling-a-large-population\">How do you decide the sample size when polling a large\npopulation?</a> </li>\n<li><a href=\"http://stats.stackexchange.com/questions/276/how-large-should-a-sample-be-for-a-given-estimation-technique-and-parameters\">How large should a sample be for a given estimation\ntechnique and parameters?</a> </li>\n<li><a href=\"http://math.stackexchange.com/questions/3490/optimal-sample-size\">Optimal sample size?</a></li>\n</ul>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Is \"discriminant function\" a synonym for \"classification function\"", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>\"discriminant function\" and \"classification function\" are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.</p>\n\n<p>I presume \"discriminant function\" has it's origin in statistics and \"classification function\" in machine learning. Is there any reason these terms cant be used as synonyms? </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0081, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "How to statistically compare the performance of machine learning classifiers?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Based on  estimated classification accuracy, I want to test whether one classifier is statistically better on a base set than another classifier . For each classifier, I select a training and testing sample randomly from the base set, train the model, and test the model. I do this ten times for each classifier. I therefore have ten estimate classification accuracy measurements for each classifier.  How do I statistically test whether the $classifier 1$ is a better classifier than the $classifier 2$ on the base dataset. What t-test is appropriate to use? </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.1838, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.05, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>A review and critique of some t-test approaches is given in <a href=\"http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf\" rel=\"nofollow\">Choosing between two learning algorithms based on calibrated tests</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</a>, and <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</a></p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Statistical comparisons of multiple classifiers performance?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>If the accuracy of $classifier1$ is statistically significantly better than $classifier2$ as per some hypothesis test, and likewise the accuracy of $classifier2$ is  statistically significantly better than $classifier3$, does it follow that $classifier1$ is  statistically significantly better than $classifier3$. This question relates to my question asked <a href=\"http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers\">here</a>.</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0099, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "How to preform 1NN with single centroid per class in SAS?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I've computer a single centroid per class using PROC fastclus in SAS,</p>\n\n<pre><code>proc fastclus data=sample.samples_train mean=knn.clusters\nmaxc=1 *number of clusters*\nmaxiter=100;\nvar &amp;predictors; \nby class;\nrun;\n</code></pre>\n\n<p>I'm trying to classify a test set based on the closest one of these centroids created. This i'm doing using PROC discrim in also in SAS.</p>\n\n<pre><code>proc discrim data=knn.clusters   \n             test=sample.samples_test\n             method=NPAR k=1 metric=identity noclassify;\n      class class; \n      var &amp;predictors; \n      ods output ErrorTestClass=knn.error;\nrun; \n</code></pre>\n\n<p>I'm using the euclidean distance measure with the option <code>metric=identity</code>.</p>\n\n<p>The following error is returned.</p>\n\n<pre><code>ERROR: There are not enough observations to evaluate the pooled covariance matrix in DATA= data set or BY group.\n</code></pre>\n\n<p>This works if I set the number of cluster in fastproc equal to 2. </p>\n\n<p>How do I however preform a 1NN with single centroid per class in SAS?</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Where does the definition of the hyperplane in a simple SVM come from?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I'm trying to figure out support vector machines using this <a href=\"http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\" rel=\"nofollow\">resource</a>. On page 2 it is stated that for linearly separable data the SVM problem is to select a hyperplane such that $\\\\vec{x}_i\\\\vec{w} + b \\\\geq 1$ for $y_i \\\\in 1$ and $\\\\vec{x}_i\\\\vec{w} + b \\\\leq -1$ for $y_i \\\\in -1$. I'm having trouble to understand where the right-hand side of the constraints come from?</p>\n\n<p>P.S The next question would be how to show that the SVM's margin is equal to $\\\\frac{1}{||\\\\vec{w}||}$.</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0443, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>In,</p>\n\n<p>$I(x,y)= \\\\sum_{x \\\\in X} \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$</p>\n\n<p>$CI(x,y)= \\\\sum_{y \\\\in Y} p(x,y) \\\\log_2 (\\\\frac{p(x , y)}{p(x)p(y)})$ for $x \\\\in X$</p>\n\n<p>we have,</p>\n\n<p>$p(\\\\cdot) \\\\in [0,1]$</p>\n\n<p>$\\\\rightarrow\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in (0...\\\\infty ]$</p>\n\n<p>$\\\\rightarrow p(\\\\cdot) log_2\\\\frac{p(\\\\cdot)}{p(\\\\cdot)p(\\\\cdot)} \\\\in [0...\\\\infty ]$</p>\n\n<p>the codomain of $I$ and $CI$ is defined only on positive real values. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Defintion for model diversity?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Two models are diverse if they make prediction errors on different instances. I know there are different measures to quantify diversity, however, I'm looking for formal conceptual definition of what we are trying to measure. Any suggestions? </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0059, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Can OLS be considered an optimization technique?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Can ordinary least squares estimation be considered an optimization technique? If so, how can I explain this?</p>\n\n<p>Note:</p>\n\n<p>From an AI perspective, supervised learning involves finding a hypothesis function $h_\\\\vec{w}(\\\\vec{x})$ that approximates the true nature between predictor variables and the predicted variable. Let some set of functions with the same model representation define the hypothesis space $\\\\mathbb{H}$ (That is we hypothesise the true relationship to be a linear function of inputs or a quadratic function of inputs and so forth). The objective is to find the model $h\\\\in\\\\mathbb{H}$ that optimally maps inputs to outputs. This is done by application of some technique to finds optimal values for the adjustable parameters $\\\\vec{w}$ that defines the function $h_w(\\\\vec{x})$. In AI we call this parameter optimization. A parameter optimization technique/model inducer/learning algorithm would for example be the back propagation algorithm.</p>\n\n<p>OLS is used to find/estimate for $\\\\beta$ parameters that defines the linear regression line that <em>optimally</em> maps predictor variables to output variables. This would be parameter optimization in the scenario above.</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Reasons for transforming multiple class classification problem into a set of binary sub-problems?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Does anyone know of a good reference that list the reasons for transforming multiple class classification problem into a set of binary sub-problems?</p>\n\n<p>In response to comment: One reason to transform a multiple class classification problem into a set of binary sub-problems are that a binary network is usually simpler and therefore better suited to fit small data sets (Less prone to over fitting).</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.1318&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Furnkranz</a> argues that a set of simpler binary classifiers can also provide better performance than one complex multiple class classifier because it is often easier to learn how to distinguish between two classes, than it is to learn how to distinguish between multiple classes. Simpler models reduce training time also. </p>\n\n<p><a href=\"http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf\" rel=\"nofollow\">Milgram</a> shows that a set of binary SVM can preform better than a multi-class MLP. Though this might be due to choice of classifier rather than binarization. </p>\n\n<p>If we focus on a MLP classifier, training a set of binary one-against-all MLPs and combining using softmax function would be very much equal the the multi-class MLP however this would allow us to use different hyper-paramater settings for each class. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0163, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "One-against-all probability values into a multiple class probability value?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I have a 10-class classification problem. I've approached the problem as a set of one-against-all binary problems. For each class I've built a MLP neural network that provides a probability estimate [0,1] as to whether the input pattern does belong to the target class. How do I combine the individual one-against-all MLP  probability estimates into a single multi-class probability estimate. </p>\n\n<p>I have the idea of using $\\\\hat{p}_j = \\\\frac{p_j}{\\\\sum_{i=1}^{10} p_i}$ where $\\\\hat{p}_j$ is multi-class probability estimate for class i, and $p_j$ is the probability estimate of the one-against-all MLP for class $j$. Would this be satisfactory? </p>\n\n<p>Any references would be appreciated.</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0055, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "How do I calculate the probabilities in this decision making scenario?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I have the following problem: A decision has to be made. Paradoxically, there is value in only being marginally correct in making this decision, say I want to be correct 60 percent of the time. That is $p(\\\\text{adjusted correct})=0.6$. I know from experience that I'm actually able to make the decision correctly 90 percent of the time, say $p(\\\\text{I'm correct})=0.9$.  After making my decision, I can ask someone else to make the decision instead. This person is very naive, and is only able to make the correct decision 10 percent of the time, that is $p(\\\\text{Alternative opinion correct})=0.1$. What percentage of decisions should I let the naive person make?</p>\n\n<p>My solution:</p>\n\n<p>Let $x$ be the proportion of decision made by the naive person. </p>\n\n<p>$0.1x+0.9(1-x)=0.6$</p>\n\n<p>$x=0.375$</p>\n\n<p>Is this correct? If the two decisions are not independent of one another how will this change? That is, what if the accuracy of the naive persons choice somehow depends on the choice made my myself? Say if I make decision A, then the naive person is $30$ percent correct in it's choice, if I make decision B, then the naive person is $20$ correct in its choice, and if I make decision C, then the naive person is $10$ correct in its choice. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0346, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>What you might actually be looking for is some way to remove the cyclical component in the data from the long term trend. </p>\n\n<p>Any smoothing technique will be able to remove noise and the cyclical component in the data. A simple moving average or exponential smoothing technique is sometimes very useful. The difference between the predicted smoothed value and the actual value is the cyclical component in the data plus noise.</p>\n\n<p>You can use a more complicated techniques such as a Hodrick\u2013Prescott filter (usually applied to business cycles). I image you could also fit some function with a aggressive regularization component to encourage smoothness. The prediction by the smooth model fitted will be the trend component and the difference the seasonal component. </p>\n\n<p>Alternatively, simple technique such as seasonality indexes might be used. </p>\n\n<p>A seasonal index (quarterly sales data) is calculated in the following table.</p>\n\n<p><img src=\"http://i.stack.imgur.com/NwHp8.png\" alt=\"example\"></p>\n\n<p>Calculate what the hypothetical constant demand was in each quarter $i.e 403/4$. Calculate in what ratio the actual demand in quarter was above or below the constant demand $i.e 80/100.75$. Decompose data into the trend component $i.e 80/0.89$ and seasonal component $i.e 80-80/0.89$. If you have a monthly data consider converting the 12 data points per year to 4 data points with a weighted average. Basically taking the average of every three month to compute one data point. Weighing the start and end of season less than the middle of the season.</p>\n\n<p>Alternatively, you can also build a simple OLS regression to separate the effect of each season from the long term trend. One independent variable is time, and other independent variables are dummy variables to indicate the season. Use $y=\\\\alpha + \\\\beta t+\\\\beta_1 d_1+\\\\beta_2 d_2 +\\\\beta_2 d_3$ where $t$ is the time period starting from 1, and $d_1$, $d_2$ and $d_3$ is dummy variables for the quarter 2, 3, 4. Using quoter 1 as the base model, add the  coefficient $\\\\beta_2$ to all data-points in quoter 2, $\\\\beta_3$ to all data-points in quarter 3, and $\\\\beta_4$ to all data-points in quarter 4. All the data points will now effectively be in the same quarter (quarter 1). That is, seasonal component of different quarter has been removed.  </p>\n\n<p>The different between approaches comes down to effort and complexity. As in any modelling task, no penance exists. The most appropriate technique can only be chosen after examining the objective of what you are trying to do. A very comprehensive standard for seasonal adjustment of time series data is used in the EU. This document gives some very good guidelines. See <a href=\"http://www.cmfb.org/pdf/ESS%20Guidelines%20on%20SA.pdf\" rel=\"nofollow\">ESS guidelines</a></p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Just because the results look good, doesn\u2019t mean they are statistically significant.  A hypothesis test tries to determine whether, statistically, one can expect the same results from a repeated study. Whether the null hypothesis will be rejected or not, will depend on the estimated variance in your experiment, the size of the difference in proportion as well as the number of samples used. </p>\n\n<p>The null hypothesis states the results obtained are purely random, whereas the alternative hypothesis states that the results are extraordinary. That is, at a 95 percent confidence level ($\\\\alpha=5$ percent), the null hypothesis will only be rejected if results is expected to be ''that good'' or better in just 5 out of 100 cases. That is, the null hypothesis is rejected in favour of the  one-tailed alternative hypothesis is if $p(t  \\\\ge t_d | H_0)\\\\le \\\\alpha$ where $t_d$ is your test statistic.</p>\n\n<p>This can also be interpreted as, the zero hypothesis will correctly rejected the null hypothesis 95 percent of the times. Incorrectly rejecting the null hypothesis is called a type I error. The probability of a Type I error = $\\\\alpha$.</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": -0.02, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Two statisticians went duck hunting. A duck flew overhead and one statistician fired just to the right of the bird. The other statistician fired just to the left of the bird. They turned to each other in glee, and congratulated each other... \"On average, he's dead!\", they cried! The duck continued his migration. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": -0.07, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": null, "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in <a href=\"http://hal.inria.fr/inria-00103955/en/\" rel=\"nofollow\">Milgram</a>.</p>\n\n<p>In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See <a href=\"http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf\" rel=\"nofollow\">Chapter 5</a> for a comprehensive study of fusing classifier outputs. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.3365\" rel=\"nofollow\">Kittler</a> argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. </p>\n\n<p>I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?</p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Confusion matrices with percentages rather than number of instances?", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>Most of the confusion matrices I've seen contain the number of instances in each cell. Isn't a confusion matrix with the percentage of instances in each cell easier to read? Is this approach wrong or does it go against some unwritten rule with regards to confusion matrices?</p>\n\n<p>Such an confusion matrix will look like this, where each of the 10 class labels makes up 10 percent of the dataset and the total is 100 percent. 9.06 percent of the dataset belonged to class 1 and was assigned to class 1. Therefore 90.60 percent of class 1 instances are classified correctly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/pReOK.png\" alt=\"example\"></p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0345, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}, {"title": {"value": "Data exploration tools for Excel", "prob": 0.005237746983766556, "mask": 0.0}, "body": {"value": "<p>I have a sample of records, with string and numerical columns. My sample is currently hosted in an excel spreadsheet. I need a tool that wil produce discripte statistics for each colomn, such as max and min values, number of unique values, max string lenght. Are there any such tools available. Any tips? The ultimate goal is to derive a data dictionary. </p>\n", "prob": 0.005280685145407915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.005599361378699541, "mask": 0.0}, "views": {"value": 0.0193, "prob": 0.005190787836909294, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.005774194374680519, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.005422449670732021, "mask": 0.0}, "tags": {"value": null, "prob": 0.005712108686566353, "mask": 0.0}, "comments": {"value": null, "prob": 0.005260928068310022, "mask": 0.0}}], "prob": 0.22707390785217285, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Organizer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Disciplined", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Self-Learner", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.1428571492433548, "mask": 0.0}}], "prob": 0.546887993812561, "mask": 0.5}, "terminate": {"prob": 0.0649225115776062, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.40023019909858704, 0.4583059251308441, 0.14146395027637482], "s_value": 0.498282790184021, "orig_id": 3735, "true_y": 0, "last_cost": 0.0, "total_cost": 5.700000010430813}], [{"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 2300, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.0769987404346466, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.148077592253685, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10395338386297226, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12679272890090942, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16148799657821655, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.07459203153848648, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3030097186565399, "mask": 0.0}, "terminate": {"prob": 0.005087730009108782, "mask": 0, "value": null}}, "cls_probs": [0.4247566759586334, 0.41208311915397644, 0.16316016018390656], "s_value": 0.5213593244552612, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.0949157252907753, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.17909090220928192, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.12706594169139862, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1541946977376938, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09352602064609528, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3434055745601654, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007801218889653683, "mask": 0, "value": null}}, "cls_probs": [0.43020501732826233, 0.4035753607749939, 0.16621962189674377], "s_value": 0.5186383128166199, "orig_id": 2300, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.08236602693796158, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.15962271392345428, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11417411267757416, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.12795153260231018, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08348909020423889, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.42421457171440125, "mask": 0.0}, "terminate": {"prob": 0.008181929588317871, "mask": 0, "value": null}}, "cls_probs": [0.44208869338035583, 0.39281198382377625, 0.16509927809238434], "s_value": 0.5172298550605774, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.09281718730926514, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.17874886095523834, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.13991813361644745, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09447262436151505, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.4832319915294647, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010811276733875275, "mask": 0, "value": null}}, "cls_probs": [0.4418289065361023, 0.3989016115665436, 0.15926943719387054], "s_value": 0.5111525654792786, "orig_id": 2300, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.17975889146327972, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.33315739035606384, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2742854058742523, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.18414533138275146, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.028652949258685112, "mask": 0, "value": null}}, "cls_probs": [0.44827789068222046, 0.39543622732162476, 0.15628582239151], "s_value": 0.4984554350376129, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.27086639404296875, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.40109601616859436, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.27632346749305725, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.051714058965444565, "mask": 0, "value": null}}, "cls_probs": [0.45503735542297363, 0.3888016641139984, 0.15616105496883392], "s_value": 0.492492139339447, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.3782595098018646, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.5591263771057129, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06261410564184189, "mask": 0, "value": null}}, "cls_probs": [0.446893572807312, 0.3934958875179291, 0.15961050987243652], "s_value": 0.48728591203689575, "orig_id": 2300, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.8822324275970459, "mask": 0.0, "selected": true}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.11776763945817947, "mask": 0, "value": null}}, "cls_probs": [0.42386263608932495, 0.4203208386898041, 0.15581649541854858], "s_value": 0.4775775969028473, "orig_id": 2300, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Working as System Administrator and Web Developer for fun and profit, free software and opensource enthusiast, messing around with any kind of servers, great fan of the simplicity of Debian, Ubuntu and ArchLinux, hateful of any rpm-based distribution, secret love for *BSD.</p>\n\n<p><strong>Well-know technologies:</strong>\nDebian, Ubuntu, Linux-KVM, OpenNebula, Proxmox, GlusterFS, MooseFS, PHP, Symfony, Java, PlayFramework.</p>\n\n<p><strong>Don't mind:</strong>\nevery commercial software/hardware solution</p>\n\n<p><strong>Hates:</strong>\nWindows*, Oracle</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4128223955631256, 0.4327051043510437, 0.1544725000858307], "s_value": 0.4709084630012512, "orig_id": 2300, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4881, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.08377869427204132, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 0, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 4881, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.05636784806847572, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.06805414706468582, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.09166625142097473, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.051644835621118546, "mask": 0.0}, "website": {"value": 0, "prob": 0.5113903284072876, "mask": 0.0}, "posts": {"value": null, "prob": 0.0858428105711937, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.12395503371953964, "mask": 0.0}, "terminate": {"prob": 0.0110786659643054, "mask": 0, "value": null}}, "cls_probs": [0.4449361562728882, 0.4188414216041565, 0.1362224519252777], "s_value": 0.5166609883308411, "orig_id": 4881, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.059630900621414185, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.07058391720056534, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0979575514793396, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05217543616890907, "mask": 0.0}, "website": {"value": 0, "prob": 0.5792972445487976, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.13068100810050964, "mask": 0.0}, "terminate": {"prob": 0.009673980996012688, "mask": 0, "value": null}}, "cls_probs": [0.4418531656265259, 0.41351795196533203, 0.1446288526058197], "s_value": 0.509600043296814, "orig_id": 4881, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.08073467016220093, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1072835847735405, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06160963326692581, "mask": 0.0}, "website": {"value": 0, "prob": 0.5980746150016785, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.13658753037452698, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01570991612970829, "mask": 0, "value": null}}, "cls_probs": [0.43711259961128235, 0.4182545840740204, 0.14463280141353607], "s_value": 0.5026743412017822, "orig_id": 4881, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.07260623574256897, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.09280460327863693, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.057551443576812744, "mask": 0.0}, "website": {"value": 0, "prob": 0.7499372363090515, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.027100523933768272, "mask": 0, "value": null}}, "cls_probs": [0.44750097393989563, 0.39765578508377075, 0.15484316647052765], "s_value": 0.4970362186431885, "orig_id": 4881, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.07070586085319519, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.07304226607084274, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06135167181491852, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.7949002981185913, "mask": 0, "value": null}}, "cls_probs": [0.5126544237136841, 0.3631800711154938, 0.12416555732488632], "s_value": 0.5083276033401489, "orig_id": 4881, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<h3> About Me </h3>\\\\n<p>\\\\nResearch Scientist with an e-commerce company near Reading, UK. Looking  for work in the near future :D\\\\n</p> \\\\n\\\\n<h3> Specialities: </h3>\\\\n<ul>\\\\n<li> C/C++ </li>\\\\n<li> System Architecture </li>\\\\n<li> Data modelling and large scale data architectures. </li>\\\\n</ul>", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.08213142305612564, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08684474974870682, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8310237526893616, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5160323977470398, 0.36381804943084717, 0.12014950066804886], "s_value": 0.509584367275238, "orig_id": 4881, "true_y": 1, "last_cost": 0.0, "total_cost": 5.0}], [{"sample": {"about_me": {"value": "<p>I'm a psychology PhD student at Trinity College, Dublin doing my research on dangerous driving among young people. I'm a part time lecturer in statistics and research methods.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0026, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2972, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a psychology PhD student at Trinity College, Dublin doing my research on dangerous driving among young people. I'm a part time lecturer in statistics and research methods.</p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0026, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08429394662380219, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 0, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 2972, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a psychology PhD student at Trinity College, Dublin doing my research on dangerous driving among young people. I'm a part time lecturer in statistics and research methods.</p>\n", "prob": 0.06295530498027802, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.10522046685218811, "mask": 0.0}, "reputation": {"value": 0.0026, "prob": 0.07615853101015091, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05862271785736084, "mask": 0.0}, "website": {"value": 0, "prob": 0.4521936774253845, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.09785228222608566, "mask": 0.0}, "badges": {"value": null, "prob": 0.1337452083826065, "mask": 0.0}, "terminate": {"prob": 0.013251753523945808, "mask": 0, "value": null}}, "cls_probs": [0.4319295287132263, 0.427225798368454, 0.1408446729183197], "s_value": 0.5171808004379272, "orig_id": 2972, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm a psychology PhD student at Trinity College, Dublin doing my research on dangerous driving among young people. I'm a part time lecturer in statistics and research methods.</p>\n", "prob": 0.10536523163318634, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.1602829098701477, "mask": 0.0}, "reputation": {"value": 0.0026, "prob": 0.1259562373161316, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10059438645839691, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.12324418872594833, "mask": 0.0}, "badges": {"value": null, "prob": 0.15578506886959076, "mask": 0.0}, "terminate": {"prob": 0.22877196967601776, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.49861302971839905, 0.3887646496295929, 0.11262232810258865], "s_value": 0.5298835635185242, "orig_id": 2972, "true_y": 0, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2554, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.042067524045705795, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 1, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0557045079767704, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.08198747038841248, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.06372874230146408, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07148997485637665, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0494525171816349, "mask": 0.0}, "website": {"value": 1, "prob": 0.5640727281570435, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.09650393575429916, "mask": 0.0}, "terminate": {"prob": 0.017060095444321632, "mask": 0, "value": null}}, "cls_probs": [0.4667429029941559, 0.38635069131851196, 0.14690642058849335], "s_value": 0.5250242948532104, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0637304037809372, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.07539690285921097, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.08198732137680054, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05709376186132431, "mask": 0.0}, "website": {"value": 1, "prob": 0.573580801486969, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.12758514285087585, "mask": 0.0}, "terminate": {"prob": 0.020625673234462738, "mask": 0, "value": null}}, "cls_probs": [0.4757453203201294, 0.38485807180404663, 0.1393965482711792], "s_value": 0.5191958546638489, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.11385341733694077, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.15240198373794556, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.181316077709198, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11203891038894653, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.42970722913742065, "mask": 0.0}, "terminate": {"prob": 0.010682409629225731, "mask": 0, "value": null}}, "cls_probs": [0.43679895997047424, 0.39998024702072144, 0.16322079300880432], "s_value": 0.510771632194519, "orig_id": 2554, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.18090230226516724, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.2117839753627777, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1385480761528015, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.45186951756477356, "mask": 0.0}, "terminate": {"prob": 0.016896164044737816, "mask": 0, "value": null}}, "cls_probs": [0.4331631064414978, 0.4027022123336792, 0.164134681224823], "s_value": 0.5030711889266968, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2524438500404358, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1691463142633438, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5548501014709473, "mask": 0.0}, "terminate": {"prob": 0.02355971746146679, "mask": 0, "value": null}}, "cls_probs": [0.4363684356212616, 0.40663617849349976, 0.15699543058872223], "s_value": 0.49549832940101624, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.29151976108551025, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.6834536790847778, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02502656728029251, "mask": 0, "value": null}}, "cls_probs": [0.42548713088035583, 0.4166155755519867, 0.15789736807346344], "s_value": 0.49336302280426025, "orig_id": 2554, "true_y": 1, "last_cost": 1.0, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.21200518310070038, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.7642545700073242, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.023740287870168686, "mask": 0, "value": null}}, "cls_probs": [0.4376789927482605, 0.4060972332954407, 0.15622374415397644], "s_value": 0.494961678981781, "orig_id": 2554, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.8711576461791992, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.1288423240184784, "mask": 0, "value": null}}, "cls_probs": [0.43990373611450195, 0.40117010474205017, 0.15892617404460907], "s_value": 0.4803135097026825, "orig_id": 2554, "true_y": 1, "last_cost": 0.5, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>\"When the going gets tough, the tough get empirical\"</p>\n\n<p>-- Jon Carroll</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.415151983499527, 0.42972901463508606, 0.15511901676654816], "s_value": 0.4706268012523651, "orig_id": 2554, "true_y": 1, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 243, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.0451396144926548, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.0725313052535057, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05382183939218521, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05918215960264206, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07011939585208893, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.531177282333374, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06768590956926346, "mask": 0.0}, "badges": {"value": null, "prob": 0.08940514922142029, "mask": 0.0}, "terminate": {"prob": 0.010937291197478771, "mask": 0, "value": null}}, "cls_probs": [0.45435237884521484, 0.41012728214263916, 0.1355203092098236], "s_value": 0.5306205749511719, "orig_id": 243, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09319149702787399, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11058338731527328, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14358055591583252, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 243, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.08143333345651627, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.1558399200439453, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10981719940900803, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.12809358537197113, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2070697695016861, "mask": 0.0}, "badges": {"value": null, "prob": 0.3102585971355438, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007487614173442125, "mask": 0, "value": null}}, "cls_probs": [0.4140949249267578, 0.41732847690582275, 0.16857662796974182], "s_value": 0.5163363218307495, "orig_id": 243, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.07274115830659866, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.14305557310581207, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10170059651136398, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.10825134068727493, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1618088185787201, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}], "prob": 0.4040200710296631, "mask": 0.0}, "terminate": {"prob": 0.008422376587986946, "mask": 0, "value": null}}, "cls_probs": [0.42349231243133545, 0.4072098433971405, 0.16929779946804047], "s_value": 0.5162491798400879, "orig_id": 243, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.10663005709648132, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.16518361866474152, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.13378028571605682, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13170070946216583, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}], "prob": 0.34426817297935486, "mask": 0.0}, "terminate": {"prob": 0.11843711137771606, "mask": 0, "value": null}}, "cls_probs": [0.4837115406990051, 0.35306036472320557, 0.16322804987430573], "s_value": 0.5176160931587219, "orig_id": 243, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.12016082555055618, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.15361113846302032, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.14203372597694397, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}], "prob": 0.42811211943626404, "mask": 0.0}, "terminate": {"prob": 0.15608210861682892, "mask": 0, "value": null}}, "cls_probs": [0.497407466173172, 0.3518105745315552, 0.15078192949295044], "s_value": 0.5139645338058472, "orig_id": 243, "true_y": 1, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.14631055295467377, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.18421755731105804, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}], "prob": 0.5040507912635803, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.16542111337184906, "mask": 0, "value": null}}, "cls_probs": [0.500406801700592, 0.34154611825942993, 0.15804719924926758], "s_value": 0.5054543614387512, "orig_id": 243, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>I'm a software developer working on a social-networking site.  I work mainly in J2EE, SQL, HTML, JavaScript and CSS.  My free time is spent raising a daughter and a son.</p>\n", "prob": 0.16382728517055511, "mask": 0.0}, "views": {"value": 0.1, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.19469374418258667, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 1.0, "mask": 0.0}}], "prob": 0.4010354280471802, "mask": 0.5}, "terminate": {"prob": 0.24044354259967804, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4951147139072418, 0.33393633365631104, 0.17094889283180237], "s_value": 0.49827876687049866, "orig_id": 243, "true_y": 1, "last_cost": 0.0, "total_cost": 4.600000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3314, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 3314, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.0718502551317215, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.11221335083246231, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.08943933993577957, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07996772974729538, "mask": 0.0}, "up_votes": {"value": 0.011, "prob": 0.09119528532028198, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06943032890558243, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.07170018553733826, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.13948626816272736, "mask": 0.0}, "terminate": {"prob": 0.2747172713279724, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5469009876251221, 0.35995760560035706, 0.09314139932394028], "s_value": 0.5479375123977661, "orig_id": 3314, "true_y": 1, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0114, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 810, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.12516337633132935, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0114, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 810, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.07595883309841156, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.10171500593423843, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11853548884391785, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.002, "prob": 0.15048938989639282, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07515018433332443, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17892484366893768, "mask": 0.0}, "badges": {"value": null, "prob": 0.2922736406326294, "mask": 0.0}, "terminate": {"prob": 0.006952641531825066, "mask": 0, "value": null}}, "cls_probs": [0.42199626564979553, 0.4193255603313446, 0.15867824852466583], "s_value": 0.5158883929252625, "orig_id": 810, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.08640659600496292, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.11442991346120834, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.17340299487113953, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08484718948602676, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1998176872730255, "mask": 0.0}, "badges": {"value": null, "prob": 0.33449915051460266, "mask": 0.0}, "terminate": {"prob": 0.006596422288566828, "mask": 0, "value": null}}, "cls_probs": [0.4049684405326843, 0.43538713455200195, 0.1596444994211197], "s_value": 0.5062559843063354, "orig_id": 810, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.09854934364557266, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.19360202550888062, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09726625680923462, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2192050963640213, "mask": 0.0}, "badges": {"value": null, "prob": 0.3828146755695343, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008562527596950531, "mask": 0, "value": null}}, "cls_probs": [0.4104250967502594, 0.43574878573417664, 0.1538260579109192], "s_value": 0.49904531240463257, "orig_id": 810, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.0891173928976059, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.174806609749794, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09060381352901459, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18807700276374817, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.4484240710735321, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008971132338047028, "mask": 0, "value": null}}, "cls_probs": [0.41447216272354126, 0.4330673813819885, 0.1524604856967926], "s_value": 0.5010337829589844, "orig_id": 810, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.09270166605710983, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.17841164767742157, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09487085044384003, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1953677088022232, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.4280272424221039, "mask": 0.20000000298023224}, "terminate": {"prob": 0.010620799846947193, "mask": 0, "value": null}}, "cls_probs": [0.41650286316871643, 0.431792676448822, 0.1517045497894287], "s_value": 0.4963420033454895, "orig_id": 810, "true_y": 1, "last_cost": 1.0, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.09389760345220566, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.17276360094547272, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09971802681684494, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Normalizing rating in a group of people [finding effectiveness]", "prob": 0.06081597879528999, "mask": 0.0}, "body": {"value": "<p>I'm a new guy here. Hopefully, I'm asking this question to right forum.</p>\n\n<h2>Problem:</h2>\n\n<p>We have data of a group of people (P1, P2, P3). They rank their expertise (1-10, where higher number is better) in a list of components (G1, G2, G3).</p>\n\n<pre><code>    P1  P2  P3\n--------------\nG1 | 8   4   7\nG2 | 7   3   7\nG3 | 9   6   5\n</code></pre>\n\n<p>Also, we have some data regarding work done by each person in each component. Example:</p>\n\n<p>For P1,</p>\n\n<pre><code>     W  WD\n----------\nG1 | 0   0\nG2 | 2   0\nG3 | 8   2\n</code></pre>\n\n<p>where <code>W</code> is total work allotted to user P1, and <code>WD</code> is actual work done. W >= WD >= 0.\nWe have similar data for P2, and P3 users.</p>\n\n<p><strong>Point to note:</strong> The user might have some level of expertise regardless of work done in a component. Example: P1 has ranked himself 8/10 even though he has not been given any task in G1 component (W = 0). P1 also has ranked himself 7/10 for G2 even though he has not finished any task in that component (WD = 0).</p>\n\n<p>Now, we want to calculate effective rating of all users relative to the group of users, not self-ranking, considering their work data and self-ranking.</p>\n\n<p>Can anyone suggest some mechanism to achieve this?</p>\n\n<p>Thanks much in advance!</p>\n", "prob": 0.06090331822633743, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06450601667165756, "mask": 0.0}, "views": {"value": 0.0756, "prob": 0.059971392154693604, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.06612507998943329, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.06208660826086998, "mask": 0.0}, "tags": {"value": null, "prob": 0.06425069272518158, "mask": 0.0}, "comments": {"value": null, "prob": 0.06134091317653656, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06081597879528999, "mask": 0.0}, "body": {"value": "<p>I worked on it and thought this Bayesian equation will be useful.</p>\n\n<p>Rating<sub>TM</sub> = SR<sub>TM</sub>/(1 + AWD/WD<sub>TM</sub>) + MSRT/(1 + WD<sub>TM</sub>/AWD)</p>\n\n<p>The variables are:<br/>\nSR = Team member's self rating<br/>\nAWD = Average work done by team<br/>\nWD = Work done by team member<br/>\nMSRT = Mean self rating of team<br/></p>\n\n<p>(TM: TeamMember)</p>\n\n<p>Please comment if you think this is not right. Thanks!</p>\n", "prob": 0.06090331822633743, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06450601667165756, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.059971392154693604, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06612507998943329, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06208660826086998, "mask": 0.0}, "tags": {"value": null, "prob": 0.06425069272518158, "mask": 0.0}, "comments": {"value": null, "prob": 0.06134091317653656, "mask": 0.0}}], "prob": 0.18096695840358734, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.4389948546886444, "mask": 0.20000000298023224}, "terminate": {"prob": 0.013658923096954823, "mask": 0, "value": null}}, "cls_probs": [0.41747790575027466, 0.435658723115921, 0.1468634158372879], "s_value": 0.5003097057342529, "orig_id": 810, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.09907757490873337, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.1852167844772339, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Normalizing rating in a group of people [finding effectiveness]", "prob": 0.06059993803501129, "mask": 0.0}, "body": {"value": "<p>I'm a new guy here. Hopefully, I'm asking this question to right forum.</p>\n\n<h2>Problem:</h2>\n\n<p>We have data of a group of people (P1, P2, P3). They rank their expertise (1-10, where higher number is better) in a list of components (G1, G2, G3).</p>\n\n<pre><code>    P1  P2  P3\n--------------\nG1 | 8   4   7\nG2 | 7   3   7\nG3 | 9   6   5\n</code></pre>\n\n<p>Also, we have some data regarding work done by each person in each component. Example:</p>\n\n<p>For P1,</p>\n\n<pre><code>     W  WD\n----------\nG1 | 0   0\nG2 | 2   0\nG3 | 8   2\n</code></pre>\n\n<p>where <code>W</code> is total work allotted to user P1, and <code>WD</code> is actual work done. W >= WD >= 0.\nWe have similar data for P2, and P3 users.</p>\n\n<p><strong>Point to note:</strong> The user might have some level of expertise regardless of work done in a component. Example: P1 has ranked himself 8/10 even though he has not been given any task in G1 component (W = 0). P1 also has ranked himself 7/10 for G2 even though he has not finished any task in that component (WD = 0).</p>\n\n<p>Now, we want to calculate effective rating of all users relative to the group of users, not self-ranking, considering their work data and self-ranking.</p>\n\n<p>Can anyone suggest some mechanism to achieve this?</p>\n\n<p>Thanks much in advance!</p>\n", "prob": 0.060747139155864716, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06455370038747787, "mask": 0.0}, "views": {"value": 0.0756, "prob": 0.05987073481082916, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.06638465821743011, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.062111519277095795, "mask": 0.0}, "tags": {"value": null, "prob": 0.06473509967327118, "mask": 0.0}, "comments": {"value": null, "prob": 0.06099719926714897, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06059993803501129, "mask": 0.0}, "body": {"value": "<p>I worked on it and thought this Bayesian equation will be useful.</p>\n\n<p>Rating<sub>TM</sub> = SR<sub>TM</sub>/(1 + AWD/WD<sub>TM</sub>) + MSRT/(1 + WD<sub>TM</sub>/AWD)</p>\n\n<p>The variables are:<br/>\nSR = Team member's self rating<br/>\nAWD = Average work done by team<br/>\nWD = Work done by team member<br/>\nMSRT = Mean self rating of team<br/></p>\n\n<p>(TM: TeamMember)</p>\n\n<p>Please comment if you think this is not right. Thanks!</p>\n", "prob": 0.060747139155864716, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06455370038747787, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05987073481082916, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06638465821743011, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062111519277095795, "mask": 0.0}, "tags": {"value": null, "prob": 0.06473509967327118, "mask": 0.0}, "comments": {"value": null, "prob": 0.06099719926714897, "mask": 0.0}}], "prob": 0.19389867782592773, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0, "selected": true}}], "prob": 0.5077528953552246, "mask": 0.20000000298023224, "selected": true}, "terminate": {"prob": 0.014054049737751484, "mask": 0, "value": null}}, "cls_probs": [0.41847410798072815, 0.4350241422653198, 0.14650171995162964], "s_value": 0.5007866621017456, "orig_id": 810, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.12688404321670532, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.14909781515598297, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Normalizing rating in a group of people [finding effectiveness]", "prob": 0.06181861460208893, "mask": 0.0}, "body": {"value": "<p>I'm a new guy here. Hopefully, I'm asking this question to right forum.</p>\n\n<h2>Problem:</h2>\n\n<p>We have data of a group of people (P1, P2, P3). They rank their expertise (1-10, where higher number is better) in a list of components (G1, G2, G3).</p>\n\n<pre><code>    P1  P2  P3\n--------------\nG1 | 8   4   7\nG2 | 7   3   7\nG3 | 9   6   5\n</code></pre>\n\n<p>Also, we have some data regarding work done by each person in each component. Example:</p>\n\n<p>For P1,</p>\n\n<pre><code>     W  WD\n----------\nG1 | 0   0\nG2 | 2   0\nG3 | 8   2\n</code></pre>\n\n<p>where <code>W</code> is total work allotted to user P1, and <code>WD</code> is actual work done. W >= WD >= 0.\nWe have similar data for P2, and P3 users.</p>\n\n<p><strong>Point to note:</strong> The user might have some level of expertise regardless of work done in a component. Example: P1 has ranked himself 8/10 even though he has not been given any task in G1 component (W = 0). P1 also has ranked himself 7/10 for G2 even though he has not finished any task in that component (WD = 0).</p>\n\n<p>Now, we want to calculate effective rating of all users relative to the group of users, not self-ranking, considering their work data and self-ranking.</p>\n\n<p>Can anyone suggest some mechanism to achieve this?</p>\n\n<p>Thanks much in advance!</p>\n", "prob": 0.06244504824280739, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06427814066410065, "mask": 0.0}, "views": {"value": 0.0756, "prob": 0.060234736651182175, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.06528156995773315, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.06272353231906891, "mask": 0.0}, "tags": {"value": null, "prob": 0.060873162001371384, "mask": 0.0}, "comments": {"value": null, "prob": 0.06234516203403473, "mask": 0.0}}, {"title": {"value": null, "prob": 0.06181861460208893, "mask": 0.0}, "body": {"value": "<p>I worked on it and thought this Bayesian equation will be useful.</p>\n\n<p>Rating<sub>TM</sub> = SR<sub>TM</sub>/(1 + AWD/WD<sub>TM</sub>) + MSRT/(1 + WD<sub>TM</sub>/AWD)</p>\n\n<p>The variables are:<br/>\nSR = Team member's self rating<br/>\nAWD = Average work done by team<br/>\nWD = Work done by team member<br/>\nMSRT = Mean self rating of team<br/></p>\n\n<p>(TM: TeamMember)</p>\n\n<p>Please comment if you think this is not right. Thanks!</p>\n", "prob": 0.06244504824280739, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06427814066410065, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060234736651182175, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06528156995773315, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06272353231906891, "mask": 0.0}, "tags": {"value": null, "prob": 0.060873162001371384, "mask": 0.0}, "comments": {"value": null, "prob": 0.06234516203403473, "mask": 0.0}}], "prob": 0.09072187542915344, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}], "prob": 0.2631004750728607, "mask": 0.4000000059604645, "selected": true}, "terminate": {"prob": 0.37019580602645874, "mask": 0, "value": null}}, "cls_probs": [0.5250114798545837, 0.37679576873779297, 0.09819278120994568], "s_value": 0.5293691754341125, "orig_id": 810, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": "<p>Coding junkie</p>\n", "prob": 0.1320262998342514, "mask": 0.0}, "views": {"value": 0.09, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0114, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.002, "prob": 0.16445527970790863, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Normalizing rating in a group of people [finding effectiveness]", "prob": 0.061487436294555664, "mask": 0.0}, "body": {"value": "<p>I'm a new guy here. Hopefully, I'm asking this question to right forum.</p>\n\n<h2>Problem:</h2>\n\n<p>We have data of a group of people (P1, P2, P3). They rank their expertise (1-10, where higher number is better) in a list of components (G1, G2, G3).</p>\n\n<pre><code>    P1  P2  P3\n--------------\nG1 | 8   4   7\nG2 | 7   3   7\nG3 | 9   6   5\n</code></pre>\n\n<p>Also, we have some data regarding work done by each person in each component. Example:</p>\n\n<p>For P1,</p>\n\n<pre><code>     W  WD\n----------\nG1 | 0   0\nG2 | 2   0\nG3 | 8   2\n</code></pre>\n\n<p>where <code>W</code> is total work allotted to user P1, and <code>WD</code> is actual work done. W >= WD >= 0.\nWe have similar data for P2, and P3 users.</p>\n\n<p><strong>Point to note:</strong> The user might have some level of expertise regardless of work done in a component. Example: P1 has ranked himself 8/10 even though he has not been given any task in G1 component (W = 0). P1 also has ranked himself 7/10 for G2 even though he has not finished any task in that component (WD = 0).</p>\n\n<p>Now, we want to calculate effective rating of all users relative to the group of users, not self-ranking, considering their work data and self-ranking.</p>\n\n<p>Can anyone suggest some mechanism to achieve this?</p>\n\n<p>Thanks much in advance!</p>\n", "prob": 0.06220075115561485, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06432970613241196, "mask": 0.0}, "views": {"value": 0.0756, "prob": 0.05996593460440636, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.06565796583890915, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0627417340874672, "mask": 0.0}, "tags": {"value": null, "prob": 0.06159442290663719, "mask": 0.0}, "comments": {"value": null, "prob": 0.06202200800180435, "mask": 0.0}}, {"title": {"value": null, "prob": 0.061487436294555664, "mask": 0.0}, "body": {"value": "<p>I worked on it and thought this Bayesian equation will be useful.</p>\n\n<p>Rating<sub>TM</sub> = SR<sub>TM</sub>/(1 + AWD/WD<sub>TM</sub>) + MSRT/(1 + WD<sub>TM</sub>/AWD)</p>\n\n<p>The variables are:<br/>\nSR = Team member's self rating<br/>\nAWD = Average work done by team<br/>\nWD = Work done by team member<br/>\nMSRT = Mean self rating of team<br/></p>\n\n<p>(TM: TeamMember)</p>\n\n<p>Please comment if you think this is not right. Thanks!</p>\n", "prob": 0.06220075115561485, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06432970613241196, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05996593460440636, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06565796583890915, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0627417340874672, "mask": 0.0}, "tags": {"value": null, "prob": 0.06159442290663719, "mask": 0.0}, "comments": {"value": null, "prob": 0.06202200800180435, "mask": 0.0}}], "prob": 0.10370827466249466, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}], "prob": 0.28710079193115234, "mask": 0.6000000238418579}, "terminate": {"prob": 0.31270933151245117, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.516811192035675, 0.39168256521224976, 0.09150631725788116], "s_value": 0.5259040594100952, "orig_id": 810, "true_y": 1, "last_cost": 0.0, "total_cost": 4.800000004470348}], [{"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 402, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.09110599011182785, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.16393209993839264, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11823386698961258, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.13953357934951782, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.17636369168758392, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08842766284942627, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21292395889759064, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.00947917252779007, "mask": 0, "value": null}}, "cls_probs": [0.4253544807434082, 0.40841978788375854, 0.16622570157051086], "s_value": 0.5181680917739868, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.10137580335140228, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.19226180016994476, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.15655674040317535, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.2000499963760376, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10118298977613449, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.23775209486484528, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.010820597410202026, "mask": 0, "value": null}}, "cls_probs": [0.4288017451763153, 0.41936713457107544, 0.15183115005493164], "s_value": 0.5104514956474304, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.12183625251054764, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.22727596759796143, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.24147050082683563, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.12009184062480927, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.27820199728012085, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01112336479127407, "mask": 0, "value": null}}, "cls_probs": [0.41332125663757324, 0.4310896098613739, 0.15558911859989166], "s_value": 0.4983746111392975, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.1585351526737213, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.2982320189476013, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1597132384777069, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.3663695454597473, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.017150012776255608, "mask": 0, "value": null}}, "cls_probs": [0.41522613167762756, 0.4319338798522949, 0.15283995866775513], "s_value": 0.4964163303375244, "orig_id": 402, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.2549252510070801, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.47279784083366394, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.24834580719470978, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.023931140080094337, "mask": 0, "value": null}}, "cls_probs": [0.42182332277297974, 0.4215008318424225, 0.15667586028575897], "s_value": 0.49509307742118835, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.4809558093547821, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.4691742956638336, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04986982420086861, "mask": 0, "value": null}}, "cls_probs": [0.427267849445343, 0.42000898718833923, 0.15272314846515656], "s_value": 0.4894181787967682, "orig_id": 402, "true_y": 1, "last_cost": 1.0, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.8762783408164978, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.12372169643640518, "mask": 0, "value": null}}, "cls_probs": [0.4264083504676819, 0.4214745759963989, 0.15211710333824158], "s_value": 0.4811522364616394, "orig_id": 402, "true_y": 1, "last_cost": 0.5, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>My first computer was a TI-99/4a -- just like Jeff Atwood ;-)</p>\n\n<p>I like coding with</p>\n\n<ul>\n<li>erlang</li>\n<li>javascript</li>\n<li>lua</li>\n<li>php</li>\n<li>shell (bash)</li>\n<li>sql</li>\n</ul>\n\n<p>I am Mac OS X user, my favorite editor is Textmate</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.41260430216789246, 0.43310338258743286, 0.1542922705411911], "s_value": 0.47802603244781494, "orig_id": 402, "true_y": 1, "last_cost": 0.0, "total_cost": 6.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0365, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.26, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0365, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.26, "prob": 0.1336667537689209, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.07859871536493301, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.1456049233675003, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0365, "prob": 0.10452106595039368, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12196975201368332, "mask": 0.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07967399060726166, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19765090942382812, "mask": 0.0}, "badges": {"value": null, "prob": 0.2627951502799988, "mask": 0.0}, "terminate": {"prob": 0.009185617789626122, "mask": 0, "value": null}}, "cls_probs": [0.4285087287425995, 0.40587735176086426, 0.16561384499073029], "s_value": 0.5201252102851868, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.09546833485364914, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.125407412648201, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.14048254489898682, "mask": 0.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09646527469158173, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2083549052476883, "mask": 0.0}, "badges": {"value": null, "prob": 0.319269597530365, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.014552028849720955, "mask": 0, "value": null}}, "cls_probs": [0.4281969666481018, 0.4071234464645386, 0.16467967629432678], "s_value": 0.5135248303413391, "orig_id": 250, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.08515875786542892, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.11586686223745346, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11817895621061325, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08905892819166183, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16186721622943878, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0833333358168602, "mask": 0.0}}], "prob": 0.41329425573349, "mask": 0.0}, "terminate": {"prob": 0.016575051471590996, "mask": 0, "value": null}}, "cls_probs": [0.432954341173172, 0.4014165699481964, 0.16562910377979279], "s_value": 0.5146220922470093, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.09932681918144226, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.13284483551979065, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10222199559211731, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18525300920009613, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0833333358168602, "mask": 0.0, "selected": true}}], "prob": 0.46483418345451355, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.015519227832555771, "mask": 0, "value": null}}, "cls_probs": [0.4100641906261444, 0.42684629559516907, 0.16308946907520294], "s_value": 0.5065563917160034, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.10259340703487396, "mask": 0.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.13578744232654572, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10513446480035782, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1883804202079773, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4509629011154175, "mask": 0.0833333358168602}, "terminate": {"prob": 0.01714138500392437, "mask": 0, "value": null}}, "cls_probs": [0.4101020395755768, 0.4273337721824646, 0.16256411373615265], "s_value": 0.505439043045044, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.11955460906028748, "mask": 0.0, "selected": true}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.12343663722276688, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20708121359348297, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Excavator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.5264649391174316, "mask": 0.0833333358168602}, "terminate": {"prob": 0.02346249297261238, "mask": 0, "value": null}}, "cls_probs": [0.41098752617836, 0.4311898946762085, 0.1578225940465927], "s_value": 0.49975156784057617, "orig_id": 250, "true_y": 1, "last_cost": 1.0, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.15600648522377014, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2433178424835205, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0, "selected": true}}, {"badge": {"value": "Excavator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.5571228861808777, "mask": 0.0833333358168602, "selected": true}, "terminate": {"prob": 0.043552856892347336, "mask": 0, "value": null}}, "cls_probs": [0.415008544921875, 0.4356105923652649, 0.1493808627128601], "s_value": 0.49159878492355347, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16696079075336456, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.24074816703796387, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.5260345935821533, "mask": 0.1666666716337204}, "terminate": {"prob": 0.06625648587942123, "mask": 0, "value": null}}, "cls_probs": [0.4206467568874359, 0.43667399883270264, 0.14267925918102264], "s_value": 0.49262070655822754, "orig_id": 250, "true_y": 1, "last_cost": 0.5, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2798239290714264, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.660891056060791, "mask": 0.1666666716337204, "selected": true}, "terminate": {"prob": 0.05928501486778259, "mask": 0, "value": null}}, "cls_probs": [0.41784176230430603, 0.4378385841846466, 0.14431963860988617], "s_value": 0.4917541444301605, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.200000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.29317355155944824, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1111111119389534, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.1111111119389534, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.6484978795051575, "mask": 0.25, "selected": true}, "terminate": {"prob": 0.05832855403423309, "mask": 0, "value": null}}, "cls_probs": [0.4147549569606781, 0.44509780406951904, 0.14014729857444763], "s_value": 0.49063634872436523, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.3018966615200043, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.125, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.125, "mask": 0.0, "selected": true}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.637397825717926, "mask": 0.3333333432674408, "selected": true}, "terminate": {"prob": 0.06070553511381149, "mask": 0, "value": null}}, "cls_probs": [0.41401246190071106, 0.4451877474784851, 0.14079976081848145], "s_value": 0.48837074637413025, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.4000000059604645}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.3111988604068756, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.6267553567886353, "mask": 0.4166666567325592}, "terminate": {"prob": 0.06204582378268242, "mask": 0, "value": null}}, "cls_probs": [0.4038863182067871, 0.44742459058761597, 0.1486891210079193], "s_value": 0.4832795262336731, "orig_id": 250, "true_y": 1, "last_cost": 1.0, "total_cost": 5.500000007450581}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Introduction to index decomposition analysis", "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Could you recommend an introductory reference to index decomposition analysis, including</p>\n\n<ul>\n<li>different methods (e.g. methods linked to the Laspeyre index and methods linked to the Divisa index)</li>\n<li>properties of decomposition methods which can be used to compare the different methods</li>\n<li>implementations of methods, e.g. in R</li>\n</ul>\n\n<p>? Any hint appreciated.</p>\n\n<p>(could not tag as index-decomposition due to missing reputation)</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.1199, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>You can get user reviews of packages on <a href=\"http://crantastic.org/reviews\" rel=\"nofollow\">crantastic</a></p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>You could try to employ the <a href=\"http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\" rel=\"nofollow\">theory</a> and <a href=\"http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\" rel=\"nofollow\">praxis</a> of association analysis or market basket analysis to your problem (just read \"items\" as \"keywords\" / \"cited reference\" and \"market basket\" as \"journal article\").</p>\n\n<p>Disclaimer - this is just an idea, I did not do anything like that myself. Just my 2Cents.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Here is a german blog with some posts on R:\n<a href=\"http://blog.berndweiss.net/tag/r/\" rel=\"nofollow\">http://blog.berndweiss.net/tag/r/</a></p>\n\n<p>Recently started, with no posts on R yet, but focused on open data, is this blog:\n<a href=\"http://blog.zeit.de/open-data\" rel=\"nofollow\">http://blog.zeit.de/open-data</a></p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Regarding the measurement of your knowledge: You could attend some data mining / data analysis competitions, such as <a href=\"http://www.kdnuggets.com/datasets/competitions.html\" rel=\"nofollow\">1</a>, <a href=\"http://www.kaggle.com\" rel=\"nofollow\">2</a>, <a href=\"http://www.research-garden.de\" rel=\"nofollow\">3</a>, <a href=\"http://www.innocentive.com/\" rel=\"nofollow\">4</a>, and see how you score compared to others.</p>\n\n<p>There are a lot of pointers to textbooks on mathematical statistics in the answers. I would like to add as relevant topics:</p>\n\n<ul>\n<li>the empirical social research component, which comprise sampling theory, socio-demographic and regional standards</li>\n<li>data management, which includes knowlegde on databases (writing SQL queries, common database schemes)</li>\n<li>communication, how to present results in a way the audience stays awake (visualization methods)</li>\n</ul>\n\n<p>Disclaimer: I am not a statistician, this are just my 2cents</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Recently I started to keep the data in a sqlite database, access the database directly from R using sqldf and view / edit with a database tool named <a href=\"http://wiki.tcl.tk/17603\" rel=\"nofollow\">tksqlite</a></p>\n\n<p>Another option is to export the data and view / edit with <a href=\"http://code.google.com/p/google-refine/\" rel=\"nofollow\">Google Refine</a></p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": "What do I gain if I consider the outcome as ordinal instead of categorical?", "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>There are different methods for prediction of ordinal and categorical variables.</p>\n\n<p>What I do not understand, is how this distinction matters. Is there a simple example which can make clear what goes wrong if I drop the order? Under what circumstances does it not matter? For instance, if the independent variables are all categorical/ordinal, too, would there be a difference?</p>\n\n<p><a href=\"http://stats.stackexchange.com/questions/6481/consequence-of-ignoring-the-order-of-a-categorical-variable-with-different-levels\">This related question</a> focuses on the type of the independent variables. Here I am asking about outcome variables.</p>\n\n<p><strong>Edit:</strong>\nI see the point that using the order structure reduces the number of model parameters, but I am still not really convinced.</p>\n\n<p>Here is an example (taken from an <a href=\"http://www.ats.ucla.edu/stat/r/dae/ologit.htm\" rel=\"nofollow\">introduction to ordered logistic regression</a> where as far as I can see ordinal logistic regression does not perform better than multinomial logistic regression:</p>\n\n<pre><code>library(nnet)\nlibrary(MASS)\ngradapply &lt;- read.csv(url(\"http://www.ats.ucla.edu/stat/r/dae/ologit.csv\"), colClasses=c(\"factor\", \"factor\", \"factor\", \"numeric\"))\n\nordered_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- polr(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nmultinomial_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- multinom(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nn &lt;- 100\n\npolr_res &lt;- replicate(n, ordered_result())\nmultinom_res &lt;- replicate(n, multinomial_result())\nboxplot(data.frame(polr=polr_res, multinom=multinom_res))\n</code></pre>\n\n<p>which shows the distribution of the number of right guesses (out of 40) of both algorithms. </p>\n\n<p><img src=\"http://i.stack.imgur.com/fUKhS.png\" alt=\"polr_vs_multinom\"></p>\n\n<p><strong>Edit2:</strong> When I use as scoring method the following</p>\n\n<pre><code>return(sum(abs(as.numeric(pred)-as.numeric(test_data$apply)))\n</code></pre>\n\n<p>and penalize \"very wrong\" predictions, polr still looks bad, i.e. the plot above does not change very much.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.09, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0362, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Just for reference, an <a href=\"http://factbased.blogspot.com/2011/07/index-decomposition-with-r_09.html\" rel=\"nofollow\">implementation</a> of two index decomposition methods is now available.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": "Combine / analyse different exposure variables in count data model", "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>The outcome $y$ in my dataset is count data. There are three possible exposure variables $e_1, e_2, e_3$ conceivable. These exposure variables are mutually exclusive, i.e. refer to different physical entities, they are different groups. However, the exposure variables are quite correlated.</p>\n\n<p>I would like to estimate the influence of the groups on the outcome. That is, I would like make a statement like \"The outcome consists of 10% $e_1$, 70% $e_2$ and 20% $e_3$.\"</p>\n\n<p>What modelling approaches can I use to address this problem? So far, I have two ideas (the model type would be negative binomial or Poisson)</p>\n\n<ol>\n<li><p>Model averaging. Here I would fit three models $y \\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e_i)$ and combine these models using for instance the <code>model.avg</code> function in the <code>MuMIn</code> package in R. However, I am not familiar with model averaging and would like to know if this approach is sensible before I look into it.</p></li>\n<li><p>I could use the sum $e=e_1+e_2+e_3$ as exposure variable and add the fractions $f_i=e_i/e$ as additional parameters. That is, $y\\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e)+ f_1 + f_2$. Here I have the problem of interpreting the parameters to derive a statement on the groups' influence.</p></li>\n</ol>\n\n<p>Any hint appreciated -- also on how to find out if the data does not help to make such a statement.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0117, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Another option seems to be <a href=\"http://en.wikipedia.org/wiki/Dataverse\" rel=\"nofollow\">Dataverse</a>, which is available as a service and as open source software. I did not try it, though.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.009914940223097801, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>The histogram of a sample of a normally distributed random variable. </p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Given explanatory variable $x_i$ and observed variable $y_i$, and a linear regression model $y = a + b x$, OLS is the minimum of: \n$$\n\\\\sum_i (y_i - b x_i - a)^2\n$$\nover all $a,b$.</p>\n\n<p>In contrast to this, quantile regression is the minimum of:\n$$\n\\\\sum_i |y_i - b x_i - a|\n$$\nover all $a,b$.</p>\n\n<p>EDIT\nAs a minimum, the parameter estimates for $a$ and $b$ are some kind of optimum. And the <a href=\"http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example\" rel=\"nofollow\">calculation method</a>, i.e. solving the normal equations, to actually find the estimates can be considered as technique. </p>\n\n<p>So I vote for yes, OLS is a kind of optimization technique. If someone says \"I solved the problem using OLS\" I would conclude he used the euclidean norm and a linear solver on the normal equations formulation of the problem. I do not know which solver, though.</p>\n\n<p>So the term \"OLS\" is more similar to \"MLE\" than to numerical methods like \"Simplex algorithm\" or \"Conjugate Gradient method\". </p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009271567687392235, "mask": 0.0}, "body": {"value": "<p>Maybe you could plot the model with <code>plot(monb)</code> and look at the QQ plot and the residual vs. fitted plot.</p>\n\n<p>The P-values (column \"Pr(>|z|)\") are quite large, which is sometimes interpreted as bad model.</p>\n\n<p>If you have an alternative model, for instance <code>glm.nb(Count~1, data=modtab)</code>, you can compare the AIC of both models.</p>\n\n<p>If you look at the coefficient estimates +/- standard error, you find that the estimates are sometimes not distinguishable from 0, in which case they have no influence.</p>\n", "prob": 0.009351562708616257, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.009914940223097801, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009164082817733288, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01022996287792921, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009598512202501297, "mask": 0.0}, "tags": {"value": null, "prob": 0.01006466243416071, "mask": 0.0}, "comments": {"value": null, "prob": 0.009327784180641174, "mask": 0.0}}], "prob": 0.27671170234680176, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.6467688679695129, "mask": 0.4166666567325592}, "terminate": {"prob": 0.07651939988136292, "mask": 0, "value": null}}, "cls_probs": [0.41874629259109497, 0.4504395127296448, 0.13081416487693787], "s_value": 0.4915463328361511, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.500000007450581}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Introduction to index decomposition analysis", "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Could you recommend an introductory reference to index decomposition analysis, including</p>\n\n<ul>\n<li>different methods (e.g. methods linked to the Laspeyre index and methods linked to the Divisa index)</li>\n<li>properties of decomposition methods which can be used to compare the different methods</li>\n<li>implementations of methods, e.g. in R</li>\n</ul>\n\n<p>? Any hint appreciated.</p>\n\n<p>(could not tag as index-decomposition due to missing reputation)</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.1199, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>You can get user reviews of packages on <a href=\"http://crantastic.org/reviews\" rel=\"nofollow\">crantastic</a></p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>You could try to employ the <a href=\"http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\" rel=\"nofollow\">theory</a> and <a href=\"http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\" rel=\"nofollow\">praxis</a> of association analysis or market basket analysis to your problem (just read \"items\" as \"keywords\" / \"cited reference\" and \"market basket\" as \"journal article\").</p>\n\n<p>Disclaimer - this is just an idea, I did not do anything like that myself. Just my 2Cents.</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Here is a german blog with some posts on R:\n<a href=\"http://blog.berndweiss.net/tag/r/\" rel=\"nofollow\">http://blog.berndweiss.net/tag/r/</a></p>\n\n<p>Recently started, with no posts on R yet, but focused on open data, is this blog:\n<a href=\"http://blog.zeit.de/open-data\" rel=\"nofollow\">http://blog.zeit.de/open-data</a></p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0, "selected": true}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Regarding the measurement of your knowledge: You could attend some data mining / data analysis competitions, such as <a href=\"http://www.kdnuggets.com/datasets/competitions.html\" rel=\"nofollow\">1</a>, <a href=\"http://www.kaggle.com\" rel=\"nofollow\">2</a>, <a href=\"http://www.research-garden.de\" rel=\"nofollow\">3</a>, <a href=\"http://www.innocentive.com/\" rel=\"nofollow\">4</a>, and see how you score compared to others.</p>\n\n<p>There are a lot of pointers to textbooks on mathematical statistics in the answers. I would like to add as relevant topics:</p>\n\n<ul>\n<li>the empirical social research component, which comprise sampling theory, socio-demographic and regional standards</li>\n<li>data management, which includes knowlegde on databases (writing SQL queries, common database schemes)</li>\n<li>communication, how to present results in a way the audience stays awake (visualization methods)</li>\n</ul>\n\n<p>Disclaimer: I am not a statistician, this are just my 2cents</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Recently I started to keep the data in a sqlite database, access the database directly from R using sqldf and view / edit with a database tool named <a href=\"http://wiki.tcl.tk/17603\" rel=\"nofollow\">tksqlite</a></p>\n\n<p>Another option is to export the data and view / edit with <a href=\"http://code.google.com/p/google-refine/\" rel=\"nofollow\">Google Refine</a></p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": "What do I gain if I consider the outcome as ordinal instead of categorical?", "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>There are different methods for prediction of ordinal and categorical variables.</p>\n\n<p>What I do not understand, is how this distinction matters. Is there a simple example which can make clear what goes wrong if I drop the order? Under what circumstances does it not matter? For instance, if the independent variables are all categorical/ordinal, too, would there be a difference?</p>\n\n<p><a href=\"http://stats.stackexchange.com/questions/6481/consequence-of-ignoring-the-order-of-a-categorical-variable-with-different-levels\">This related question</a> focuses on the type of the independent variables. Here I am asking about outcome variables.</p>\n\n<p><strong>Edit:</strong>\nI see the point that using the order structure reduces the number of model parameters, but I am still not really convinced.</p>\n\n<p>Here is an example (taken from an <a href=\"http://www.ats.ucla.edu/stat/r/dae/ologit.htm\" rel=\"nofollow\">introduction to ordered logistic regression</a> where as far as I can see ordinal logistic regression does not perform better than multinomial logistic regression:</p>\n\n<pre><code>library(nnet)\nlibrary(MASS)\ngradapply &lt;- read.csv(url(\"http://www.ats.ucla.edu/stat/r/dae/ologit.csv\"), colClasses=c(\"factor\", \"factor\", \"factor\", \"numeric\"))\n\nordered_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- polr(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nmultinomial_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- multinom(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nn &lt;- 100\n\npolr_res &lt;- replicate(n, ordered_result())\nmultinom_res &lt;- replicate(n, multinomial_result())\nboxplot(data.frame(polr=polr_res, multinom=multinom_res))\n</code></pre>\n\n<p>which shows the distribution of the number of right guesses (out of 40) of both algorithms. </p>\n\n<p><img src=\"http://i.stack.imgur.com/fUKhS.png\" alt=\"polr_vs_multinom\"></p>\n\n<p><strong>Edit2:</strong> When I use as scoring method the following</p>\n\n<pre><code>return(sum(abs(as.numeric(pred)-as.numeric(test_data$apply)))\n</code></pre>\n\n<p>and penalize \"very wrong\" predictions, polr still looks bad, i.e. the plot above does not change very much.</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.09, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0362, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Just for reference, an <a href=\"http://factbased.blogspot.com/2011/07/index-decomposition-with-r_09.html\" rel=\"nofollow\">implementation</a> of two index decomposition methods is now available.</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": "Combine / analyse different exposure variables in count data model", "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>The outcome $y$ in my dataset is count data. There are three possible exposure variables $e_1, e_2, e_3$ conceivable. These exposure variables are mutually exclusive, i.e. refer to different physical entities, they are different groups. However, the exposure variables are quite correlated.</p>\n\n<p>I would like to estimate the influence of the groups on the outcome. That is, I would like make a statement like \"The outcome consists of 10% $e_1$, 70% $e_2$ and 20% $e_3$.\"</p>\n\n<p>What modelling approaches can I use to address this problem? So far, I have two ideas (the model type would be negative binomial or Poisson)</p>\n\n<ol>\n<li><p>Model averaging. Here I would fit three models $y \\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e_i)$ and combine these models using for instance the <code>model.avg</code> function in the <code>MuMIn</code> package in R. However, I am not familiar with model averaging and would like to know if this approach is sensible before I look into it.</p></li>\n<li><p>I could use the sum $e=e_1+e_2+e_3$ as exposure variable and add the fractions $f_i=e_i/e$ as additional parameters. That is, $y\\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e)+ f_1 + f_2$. Here I have the problem of interpreting the parameters to derive a statement on the groups' influence.</p></li>\n</ol>\n\n<p>Any hint appreciated -- also on how to find out if the data does not help to make such a statement.</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0117, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009362899698317051, "mask": 0.0}, "body": {"value": "<p>Another option seems to be <a href=\"http://en.wikipedia.org/wiki/Dataverse\" rel=\"nofollow\">Dataverse</a>, which is available as a service and as open source software. I did not try it, though.</p>\n", "prob": 0.009443005546927452, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0092566953971982, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010338163003325462, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009700055234134197, "mask": 0.0}, "tags": {"value": null, "prob": 0.01017774548381567, "mask": 0.0}, "comments": {"value": null, "prob": 0.009402531199157238, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>The histogram of a sample of a normally distributed random variable. </p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Given explanatory variable $x_i$ and observed variable $y_i$, and a linear regression model $y = a + b x$, OLS is the minimum of: \n$$\n\\\\sum_i (y_i - b x_i - a)^2\n$$\nover all $a,b$.</p>\n\n<p>In contrast to this, quantile regression is the minimum of:\n$$\n\\\\sum_i |y_i - b x_i - a|\n$$\nover all $a,b$.</p>\n\n<p>EDIT\nAs a minimum, the parameter estimates for $a$ and $b$ are some kind of optimum. And the <a href=\"http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example\" rel=\"nofollow\">calculation method</a>, i.e. solving the normal equations, to actually find the estimates can be considered as technique. </p>\n\n<p>So I vote for yes, OLS is a kind of optimization technique. If someone says \"I solved the problem using OLS\" I would conclude he used the euclidean norm and a linear solver on the normal equations formulation of the problem. I do not know which solver, though.</p>\n\n<p>So the term \"OLS\" is more similar to \"MLE\" than to numerical methods like \"Simplex algorithm\" or \"Conjugate Gradient method\". </p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009363343939185143, "mask": 0.0}, "body": {"value": "<p>Maybe you could plot the model with <code>plot(monb)</code> and look at the QQ plot and the residual vs. fitted plot.</p>\n\n<p>The P-values (column \"Pr(>|z|)\") are quite large, which is sometimes interpreted as bad model.</p>\n\n<p>If you have an alternative model, for instance <code>glm.nb(Count~1, data=modtab)</code>, you can compare the AIC of both models.</p>\n\n<p>If you look at the coefficient estimates +/- standard error, you find that the estimates are sometimes not distinguishable from 0, in which case they have no influence.</p>\n", "prob": 0.009443383663892746, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010013513267040253, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009257130324840546, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010333556681871414, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009697130881249905, "mask": 0.0}, "tags": {"value": null, "prob": 0.010174864903092384, "mask": 0.0}, "comments": {"value": null, "prob": 0.009410315193235874, "mask": 0.0}}], "prob": 0.2769026756286621, "mask": 0.009615384973585606, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.6455270051956177, "mask": 0.4166666567325592}, "terminate": {"prob": 0.07757024466991425, "mask": 0, "value": null}}, "cls_probs": [0.41899600625038147, 0.4497503638267517, 0.13125362992286682], "s_value": 0.49181225895881653, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.600000008940697}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Introduction to index decomposition analysis", "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Could you recommend an introductory reference to index decomposition analysis, including</p>\n\n<ul>\n<li>different methods (e.g. methods linked to the Laspeyre index and methods linked to the Divisa index)</li>\n<li>properties of decomposition methods which can be used to compare the different methods</li>\n<li>implementations of methods, e.g. in R</li>\n</ul>\n\n<p>? Any hint appreciated.</p>\n\n<p>(could not tag as index-decomposition due to missing reputation)</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.1199, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>You can get user reviews of packages on <a href=\"http://crantastic.org/reviews\" rel=\"nofollow\">crantastic</a></p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>You could try to employ the <a href=\"http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\" rel=\"nofollow\">theory</a> and <a href=\"http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\" rel=\"nofollow\">praxis</a> of association analysis or market basket analysis to your problem (just read \"items\" as \"keywords\" / \"cited reference\" and \"market basket\" as \"journal article\").</p>\n\n<p>Disclaimer - this is just an idea, I did not do anything like that myself. Just my 2Cents.</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009451150894165039, "mask": 0.0}, "body": {"value": "<p>Here is a german blog with some posts on R:\n<a href=\"http://blog.berndweiss.net/tag/r/\" rel=\"nofollow\">http://blog.berndweiss.net/tag/r/</a></p>\n\n<p>Recently started, with no posts on R yet, but focused on open data, is this blog:\n<a href=\"http://blog.zeit.de/open-data\" rel=\"nofollow\">http://blog.zeit.de/open-data</a></p>\n", "prob": 0.00953138992190361, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010104727931320667, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.010431279428303242, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0097927525639534, "mask": 0.0}, "tags": {"value": null, "prob": 0.01028500683605671, "mask": 0.0}, "comments": {"value": null, "prob": 0.00948377512395382, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Regarding the measurement of your knowledge: You could attend some data mining / data analysis competitions, such as <a href=\"http://www.kdnuggets.com/datasets/competitions.html\" rel=\"nofollow\">1</a>, <a href=\"http://www.kaggle.com\" rel=\"nofollow\">2</a>, <a href=\"http://www.research-garden.de\" rel=\"nofollow\">3</a>, <a href=\"http://www.innocentive.com/\" rel=\"nofollow\">4</a>, and see how you score compared to others.</p>\n\n<p>There are a lot of pointers to textbooks on mathematical statistics in the answers. I would like to add as relevant topics:</p>\n\n<ul>\n<li>the empirical social research component, which comprise sampling theory, socio-demographic and regional standards</li>\n<li>data management, which includes knowlegde on databases (writing SQL queries, common database schemes)</li>\n<li>communication, how to present results in a way the audience stays awake (visualization methods)</li>\n</ul>\n\n<p>Disclaimer: I am not a statistician, this are just my 2cents</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Recently I started to keep the data in a sqlite database, access the database directly from R using sqldf and view / edit with a database tool named <a href=\"http://wiki.tcl.tk/17603\" rel=\"nofollow\">tksqlite</a></p>\n\n<p>Another option is to export the data and view / edit with <a href=\"http://code.google.com/p/google-refine/\" rel=\"nofollow\">Google Refine</a></p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": "What do I gain if I consider the outcome as ordinal instead of categorical?", "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>There are different methods for prediction of ordinal and categorical variables.</p>\n\n<p>What I do not understand, is how this distinction matters. Is there a simple example which can make clear what goes wrong if I drop the order? Under what circumstances does it not matter? For instance, if the independent variables are all categorical/ordinal, too, would there be a difference?</p>\n\n<p><a href=\"http://stats.stackexchange.com/questions/6481/consequence-of-ignoring-the-order-of-a-categorical-variable-with-different-levels\">This related question</a> focuses on the type of the independent variables. Here I am asking about outcome variables.</p>\n\n<p><strong>Edit:</strong>\nI see the point that using the order structure reduces the number of model parameters, but I am still not really convinced.</p>\n\n<p>Here is an example (taken from an <a href=\"http://www.ats.ucla.edu/stat/r/dae/ologit.htm\" rel=\"nofollow\">introduction to ordered logistic regression</a> where as far as I can see ordinal logistic regression does not perform better than multinomial logistic regression:</p>\n\n<pre><code>library(nnet)\nlibrary(MASS)\ngradapply &lt;- read.csv(url(\"http://www.ats.ucla.edu/stat/r/dae/ologit.csv\"), colClasses=c(\"factor\", \"factor\", \"factor\", \"numeric\"))\n\nordered_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- polr(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nmultinomial_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- multinom(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nn &lt;- 100\n\npolr_res &lt;- replicate(n, ordered_result())\nmultinom_res &lt;- replicate(n, multinomial_result())\nboxplot(data.frame(polr=polr_res, multinom=multinom_res))\n</code></pre>\n\n<p>which shows the distribution of the number of right guesses (out of 40) of both algorithms. </p>\n\n<p><img src=\"http://i.stack.imgur.com/fUKhS.png\" alt=\"polr_vs_multinom\"></p>\n\n<p><strong>Edit2:</strong> When I use as scoring method the following</p>\n\n<pre><code>return(sum(abs(as.numeric(pred)-as.numeric(test_data$apply)))\n</code></pre>\n\n<p>and penalize \"very wrong\" predictions, polr still looks bad, i.e. the plot above does not change very much.</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.09, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0362, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Just for reference, an <a href=\"http://factbased.blogspot.com/2011/07/index-decomposition-with-r_09.html\" rel=\"nofollow\">implementation</a> of two index decomposition methods is now available.</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": "Combine / analyse different exposure variables in count data model", "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>The outcome $y$ in my dataset is count data. There are three possible exposure variables $e_1, e_2, e_3$ conceivable. These exposure variables are mutually exclusive, i.e. refer to different physical entities, they are different groups. However, the exposure variables are quite correlated.</p>\n\n<p>I would like to estimate the influence of the groups on the outcome. That is, I would like make a statement like \"The outcome consists of 10% $e_1$, 70% $e_2$ and 20% $e_3$.\"</p>\n\n<p>What modelling approaches can I use to address this problem? So far, I have two ideas (the model type would be negative binomial or Poisson)</p>\n\n<ol>\n<li><p>Model averaging. Here I would fit three models $y \\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e_i)$ and combine these models using for instance the <code>model.avg</code> function in the <code>MuMIn</code> package in R. However, I am not familiar with model averaging and would like to know if this approach is sensible before I look into it.</p></li>\n<li><p>I could use the sum $e=e_1+e_2+e_3$ as exposure variable and add the fractions $f_i=e_i/e$ as additional parameters. That is, $y\\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e)+ f_1 + f_2$. Here I have the problem of interpreting the parameters to derive a statement on the groups' influence.</p></li>\n</ol>\n\n<p>Any hint appreciated -- also on how to find out if the data does not help to make such a statement.</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0117, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450049139559269, "mask": 0.0}, "body": {"value": "<p>Another option seems to be <a href=\"http://en.wikipedia.org/wiki/Dataverse\" rel=\"nofollow\">Dataverse</a>, which is available as a service and as open source software. I did not try it, though.</p>\n", "prob": 0.0095301428809762, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.009345082566142082, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010433290153741837, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009791563265025616, "mask": 0.0}, "tags": {"value": null, "prob": 0.010279553942382336, "mask": 0.0}, "comments": {"value": null, "prob": 0.009485525079071522, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>The histogram of a sample of a normally distributed random variable. </p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Given explanatory variable $x_i$ and observed variable $y_i$, and a linear regression model $y = a + b x$, OLS is the minimum of: \n$$\n\\\\sum_i (y_i - b x_i - a)^2\n$$\nover all $a,b$.</p>\n\n<p>In contrast to this, quantile regression is the minimum of:\n$$\n\\\\sum_i |y_i - b x_i - a|\n$$\nover all $a,b$.</p>\n\n<p>EDIT\nAs a minimum, the parameter estimates for $a$ and $b$ are some kind of optimum. And the <a href=\"http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example\" rel=\"nofollow\">calculation method</a>, i.e. solving the normal equations, to actually find the estimates can be considered as technique. </p>\n\n<p>So I vote for yes, OLS is a kind of optimization technique. If someone says \"I solved the problem using OLS\" I would conclude he used the euclidean norm and a linear solver on the normal equations formulation of the problem. I do not know which solver, though.</p>\n\n<p>So the term \"OLS\" is more similar to \"MLE\" than to numerical methods like \"Simplex algorithm\" or \"Conjugate Gradient method\". </p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009450498037040234, "mask": 0.0}, "body": {"value": "<p>Maybe you could plot the model with <code>plot(monb)</code> and look at the QQ plot and the residual vs. fitted plot.</p>\n\n<p>The P-values (column \"Pr(>|z|)\") are quite large, which is sometimes interpreted as bad model.</p>\n\n<p>If you have an alternative model, for instance <code>glm.nb(Count~1, data=modtab)</code>, you can compare the AIC of both models.</p>\n\n<p>If you look at the coefficient estimates +/- standard error, you find that the estimates are sometimes not distinguishable from 0, in which case they have no influence.</p>\n", "prob": 0.009530523791909218, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010104785673320293, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009345521219074726, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010428640991449356, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00978861190378666, "mask": 0.0}, "tags": {"value": null, "prob": 0.010276644490659237, "mask": 0.0}, "comments": {"value": null, "prob": 0.00949337612837553, "mask": 0.0}}], "prob": 0.27835598587989807, "mask": 0.01923076994717121}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.6437848210334778, "mask": 0.4166666567325592, "selected": true}, "terminate": {"prob": 0.07785923779010773, "mask": 0, "value": null}}, "cls_probs": [0.4192756116390228, 0.44905009865760803, 0.13167425990104675], "s_value": 0.4914526343345642, "orig_id": 250, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.700000010430813}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.4, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0365, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.26, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Introduction to index decomposition analysis", "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Could you recommend an introductory reference to index decomposition analysis, including</p>\n\n<ul>\n<li>different methods (e.g. methods linked to the Laspeyre index and methods linked to the Divisa index)</li>\n<li>properties of decomposition methods which can be used to compare the different methods</li>\n<li>implementations of methods, e.g. in R</li>\n</ul>\n\n<p>? Any hint appreciated.</p>\n\n<p>(could not tag as index-decomposition due to missing reputation)</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.1199, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>You can get user reviews of packages on <a href=\"http://crantastic.org/reviews\" rel=\"nofollow\">crantastic</a></p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>You could try to employ the <a href=\"http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\" rel=\"nofollow\">theory</a> and <a href=\"http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\" rel=\"nofollow\">praxis</a> of association analysis or market basket analysis to your problem (just read \"items\" as \"keywords\" / \"cited reference\" and \"market basket\" as \"journal article\").</p>\n\n<p>Disclaimer - this is just an idea, I did not do anything like that myself. Just my 2Cents.</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.00952104665338993, "mask": 0.0}, "body": {"value": "<p>Here is a german blog with some posts on R:\n<a href=\"http://blog.berndweiss.net/tag/r/\" rel=\"nofollow\">http://blog.berndweiss.net/tag/r/</a></p>\n\n<p>Recently started, with no posts on R yet, but focused on open data, is this blog:\n<a href=\"http://blog.zeit.de/open-data\" rel=\"nofollow\">http://blog.zeit.de/open-data</a></p>\n", "prob": 0.009628274478018284, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.01010177657008171, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.010382657870650291, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009835398755967617, "mask": 0.0}, "tags": {"value": null, "prob": 0.010050201788544655, "mask": 0.0}, "comments": {"value": null, "prob": 0.00955192744731903, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Regarding the measurement of your knowledge: You could attend some data mining / data analysis competitions, such as <a href=\"http://www.kdnuggets.com/datasets/competitions.html\" rel=\"nofollow\">1</a>, <a href=\"http://www.kaggle.com\" rel=\"nofollow\">2</a>, <a href=\"http://www.research-garden.de\" rel=\"nofollow\">3</a>, <a href=\"http://www.innocentive.com/\" rel=\"nofollow\">4</a>, and see how you score compared to others.</p>\n\n<p>There are a lot of pointers to textbooks on mathematical statistics in the answers. I would like to add as relevant topics:</p>\n\n<ul>\n<li>the empirical social research component, which comprise sampling theory, socio-demographic and regional standards</li>\n<li>data management, which includes knowlegde on databases (writing SQL queries, common database schemes)</li>\n<li>communication, how to present results in a way the audience stays awake (visualization methods)</li>\n</ul>\n\n<p>Disclaimer: I am not a statistician, this are just my 2cents</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Recently I started to keep the data in a sqlite database, access the database directly from R using sqldf and view / edit with a database tool named <a href=\"http://wiki.tcl.tk/17603\" rel=\"nofollow\">tksqlite</a></p>\n\n<p>Another option is to export the data and view / edit with <a href=\"http://code.google.com/p/google-refine/\" rel=\"nofollow\">Google Refine</a></p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": "What do I gain if I consider the outcome as ordinal instead of categorical?", "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>There are different methods for prediction of ordinal and categorical variables.</p>\n\n<p>What I do not understand, is how this distinction matters. Is there a simple example which can make clear what goes wrong if I drop the order? Under what circumstances does it not matter? For instance, if the independent variables are all categorical/ordinal, too, would there be a difference?</p>\n\n<p><a href=\"http://stats.stackexchange.com/questions/6481/consequence-of-ignoring-the-order-of-a-categorical-variable-with-different-levels\">This related question</a> focuses on the type of the independent variables. Here I am asking about outcome variables.</p>\n\n<p><strong>Edit:</strong>\nI see the point that using the order structure reduces the number of model parameters, but I am still not really convinced.</p>\n\n<p>Here is an example (taken from an <a href=\"http://www.ats.ucla.edu/stat/r/dae/ologit.htm\" rel=\"nofollow\">introduction to ordered logistic regression</a> where as far as I can see ordinal logistic regression does not perform better than multinomial logistic regression:</p>\n\n<pre><code>library(nnet)\nlibrary(MASS)\ngradapply &lt;- read.csv(url(\"http://www.ats.ucla.edu/stat/r/dae/ologit.csv\"), colClasses=c(\"factor\", \"factor\", \"factor\", \"numeric\"))\n\nordered_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- polr(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nmultinomial_result &lt;- function() {\n  train_rows &lt;- sample(nrow(gradapply), round(nrow(gradapply)*0.9))\n  train_data &lt;- gradapply[train_rows,]\n  test_data &lt;- gradapply[setdiff(1:nrow(gradapply), train_rows),]\n  m &lt;- multinom(apply~pared+gpa, data=train_data)\n  pred &lt;- predict(m, test_data)\n  return(sum(pred==test_data$apply))\n}\n\nn &lt;- 100\n\npolr_res &lt;- replicate(n, ordered_result())\nmultinom_res &lt;- replicate(n, multinomial_result())\nboxplot(data.frame(polr=polr_res, multinom=multinom_res))\n</code></pre>\n\n<p>which shows the distribution of the number of right guesses (out of 40) of both algorithms. </p>\n\n<p><img src=\"http://i.stack.imgur.com/fUKhS.png\" alt=\"polr_vs_multinom\"></p>\n\n<p><strong>Edit2:</strong> When I use as scoring method the following</p>\n\n<pre><code>return(sum(abs(as.numeric(pred)-as.numeric(test_data$apply)))\n</code></pre>\n\n<p>and penalize \"very wrong\" predictions, polr still looks bad, i.e. the plot above does not change very much.</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.09, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0362, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Just for reference, an <a href=\"http://factbased.blogspot.com/2011/07/index-decomposition-with-r_09.html\" rel=\"nofollow\">implementation</a> of two index decomposition methods is now available.</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": "Combine / analyse different exposure variables in count data model", "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>The outcome $y$ in my dataset is count data. There are three possible exposure variables $e_1, e_2, e_3$ conceivable. These exposure variables are mutually exclusive, i.e. refer to different physical entities, they are different groups. However, the exposure variables are quite correlated.</p>\n\n<p>I would like to estimate the influence of the groups on the outcome. That is, I would like make a statement like \"The outcome consists of 10% $e_1$, 70% $e_2$ and 20% $e_3$.\"</p>\n\n<p>What modelling approaches can I use to address this problem? So far, I have two ideas (the model type would be negative binomial or Poisson)</p>\n\n<ol>\n<li><p>Model averaging. Here I would fit three models $y \\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e_i)$ and combine these models using for instance the <code>model.avg</code> function in the <code>MuMIn</code> package in R. However, I am not familiar with model averaging and would like to know if this approach is sensible before I look into it.</p></li>\n<li><p>I could use the sum $e=e_1+e_2+e_3$ as exposure variable and add the fractions $f_i=e_i/e$ as additional parameters. That is, $y\\\\;\\\\tilde\\\\;\\\\; \\\\text{offset}(\\\\log e)+ f_1 + f_2$. Here I have the problem of interpreting the parameters to derive a statement on the groups' influence.</p></li>\n</ol>\n\n<p>Any hint appreciated -- also on how to find out if the data does not help to make such a statement.</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0117, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009519936516880989, "mask": 0.0}, "body": {"value": "<p>Another option seems to be <a href=\"http://en.wikipedia.org/wiki/Dataverse\" rel=\"nofollow\">Dataverse</a>, which is available as a service and as open source software. I did not try it, though.</p>\n", "prob": 0.009627014398574829, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.009354139678180218, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.01038465928286314, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.00983420480042696, "mask": 0.0}, "tags": {"value": null, "prob": 0.01004487369209528, "mask": 0.0}, "comments": {"value": null, "prob": 0.009553689509630203, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>The histogram of a sample of a normally distributed random variable. </p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Given explanatory variable $x_i$ and observed variable $y_i$, and a linear regression model $y = a + b x$, OLS is the minimum of: \n$$\n\\\\sum_i (y_i - b x_i - a)^2\n$$\nover all $a,b$.</p>\n\n<p>In contrast to this, quantile regression is the minimum of:\n$$\n\\\\sum_i |y_i - b x_i - a|\n$$\nover all $a,b$.</p>\n\n<p>EDIT\nAs a minimum, the parameter estimates for $a$ and $b$ are some kind of optimum. And the <a href=\"http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example\" rel=\"nofollow\">calculation method</a>, i.e. solving the normal equations, to actually find the estimates can be considered as technique. </p>\n\n<p>So I vote for yes, OLS is a kind of optimization technique. If someone says \"I solved the problem using OLS\" I would conclude he used the euclidean norm and a linear solver on the normal equations formulation of the problem. I do not know which solver, though.</p>\n\n<p>So the term \"OLS\" is more similar to \"MLE\" than to numerical methods like \"Simplex algorithm\" or \"Conjugate Gradient method\". </p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}, {"title": {"value": null, "prob": 0.009520388208329678, "mask": 0.0}, "body": {"value": "<p>Maybe you could plot the model with <code>plot(monb)</code> and look at the QQ plot and the residual vs. fitted plot.</p>\n\n<p>The P-values (column \"Pr(>|z|)\") are quite large, which is sometimes interpreted as bad model.</p>\n\n<p>If you have an alternative model, for instance <code>glm.nb(Count~1, data=modtab)</code>, you can compare the AIC of both models.</p>\n\n<p>If you look at the coefficient estimates +/- standard error, you find that the estimates are sometimes not distinguishable from 0, in which case they have no influence.</p>\n", "prob": 0.009627399034798145, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.010101834312081337, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.009354579262435436, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.010380031540989876, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.009831239469349384, "mask": 0.0}, "tags": {"value": null, "prob": 0.010042030364274979, "mask": 0.0}, "comments": {"value": null, "prob": 0.009561598300933838, "mask": 0.0}}], "prob": 0.20724095404148102, "mask": 0.01923076994717121}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Precognitive", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Custodian", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Excavator", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}], "prob": 0.4909496307373047, "mask": 0.5}, "terminate": {"prob": 0.3018094301223755, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4577290117740631, 0.43475911021232605, 0.10751189291477203], "s_value": 0.5061392188072205, "orig_id": 250, "true_y": 1, "last_cost": 0.0, "total_cost": 6.800000011920929}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6963, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.046835899353027344, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07631810754537582, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.06163671240210533, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07105255126953125, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04347776994109154, "mask": 0.0}, "website": {"value": 1, "prob": 0.5253101587295532, "mask": 0.0}, "posts": {"value": null, "prob": 0.06713656336069107, "mask": 0.0}, "badges": {"value": null, "prob": 0.09422486275434494, "mask": 0.0}, "terminate": {"prob": 0.014007381163537502, "mask": 0, "value": null}}, "cls_probs": [0.4649169445037842, 0.4096432328224182, 0.12543979287147522], "s_value": 0.5320410132408142, "orig_id": 6963, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.050335593521595, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.06610987335443497, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07732782512903214, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.047522157430648804, "mask": 0.0}, "website": {"value": 1, "prob": 0.5574812889099121, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07207262516021729, "mask": 0.0}, "badges": {"value": null, "prob": 0.11219023913145065, "mask": 0.0}, "terminate": {"prob": 0.01696031540632248, "mask": 0, "value": null}}, "cls_probs": [0.48043981194496155, 0.40721574425697327, 0.1123444214463234], "s_value": 0.530654788017273, "orig_id": 6963, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.0849391296505928, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.13022947311401367, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16600608825683594, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08431260287761688, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19095264375209808, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.33489155769348145, "mask": 0.0}, "terminate": {"prob": 0.008668467402458191, "mask": 0, "value": null}}, "cls_probs": [0.42646971344947815, 0.4210495948791504, 0.15248064696788788], "s_value": 0.5075578689575195, "orig_id": 6963, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.103250652551651, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.16309000551700592, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.20653600990772247, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09997090697288513, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.41824981570243835, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008902632631361485, "mask": 0, "value": null}}, "cls_probs": [0.431520938873291, 0.413403183221817, 0.15507590770721436], "s_value": 0.5053294897079468, "orig_id": 6963, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.17951783537864685, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.2781037390232086, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.3504925072193146, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.17339712381362915, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01848869025707245, "mask": 0, "value": null}}, "cls_probs": [0.43876397609710693, 0.40700459480285645, 0.1542314738035202], "s_value": 0.5017052888870239, "orig_id": 6963, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.2154536247253418, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.33598223328590393, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.4280056655406952, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.020558485761284828, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.43205559253692627, 0.4112241864204407, 0.15672023594379425], "s_value": 0.4936330020427704, "orig_id": 6963, "true_y": 0, "last_cost": 0.0, "total_cost": 4.0}], [{"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 1, "prob": 0.44816169142723083, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.0771675556898117, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.14489252865314484, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.10301674902439117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12164411693811417, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07795343548059464, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19800105690956116, "mask": 0.0}, "badges": {"value": null, "prob": 0.2697715163230896, "mask": 0.0}, "terminate": {"prob": 0.007553049363195896, "mask": 0, "value": null}}, "cls_probs": [0.4237002730369568, 0.4112336337566376, 0.16506606340408325], "s_value": 0.5202891826629639, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.08143333345651627, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1558399200439453, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.10981719940900803, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.12809358537197113, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2070697695016861, "mask": 0.0}, "badges": {"value": null, "prob": 0.3102585971355438, "mask": 0.0}, "terminate": {"prob": 0.007487614173442125, "mask": 0, "value": null}}, "cls_probs": [0.4140949249267578, 0.41732847690582275, 0.16857662796974182], "s_value": 0.5163363218307495, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.092936210334301, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17677918076515198, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1422095149755478, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21963930130004883, "mask": 0.0}, "badges": {"value": null, "prob": 0.35855644941329956, "mask": 0.0}, "terminate": {"prob": 0.009879391640424728, "mask": 0, "value": null}}, "cls_probs": [0.41622090339660645, 0.42162859439849854, 0.16215045750141144], "s_value": 0.5103447437286377, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.11098797619342804, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1664920151233673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2495347112417221, "mask": 0.0}, "badges": {"value": null, "prob": 0.4595737159252167, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013411576859652996, "mask": 0, "value": null}}, "cls_probs": [0.42928197979927063, 0.415831059217453, 0.15488693118095398], "s_value": 0.5056186318397522, "orig_id": 4700, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.09426932036876678, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.13380879163742065, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18576882779598236, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5717883706092834, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.014364643022418022, "mask": 0, "value": null}}, "cls_probs": [0.44005927443504333, 0.4065932333469391, 0.1533474624156952], "s_value": 0.5076807141304016, "orig_id": 4700, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.20652811229228973, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.29675939679145813, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.4564018249511719, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04031066596508026, "mask": 0, "value": null}}, "cls_probs": [0.44035252928733826, 0.40089309215545654, 0.15875433385372162], "s_value": 0.48950648307800293, "orig_id": 4700, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.30128225684165955, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.6508897542953491, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04782802611589432, "mask": 0, "value": null}}, "cls_probs": [0.4172687232494354, 0.4282688796520233, 0.15446236729621887], "s_value": 0.4790191650390625, "orig_id": 4700, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.9150946140289307, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08490537852048874, "mask": 0, "value": null}}, "cls_probs": [0.4072248935699463, 0.4396248161792755, 0.153150275349617], "s_value": 0.47334277629852295, "orig_id": 4700, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Shen Zhun (Allen) is a regular stuff in IBM Global Delivery Center(Suzhou),which provides a broad spectrum of services, including consulting, systems integration, application services, maintenance, testing, solution management, business transformation outsourcing services and IT infrastructure services.</p>\n\n<p>I got dual Bachelor degrees from Liverpool University(UK) and Xi'an Jiaotong Liverpool University(China) in 2012. My main interest is Wireless Sensor Network, Information Retrieval, Clouding Computing and Mobile Computing.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4157842695713043, 0.43055322766304016, 0.1536625325679779], "s_value": 0.47045838832855225, "orig_id": 4700, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.04287828505039215, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.052264608442783356, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.05562107264995575, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.06584755331277847, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.03930831700563431, "mask": 0.0}, "website": {"value": 1, "prob": 0.5823312997817993, "mask": 0.0}, "posts": {"value": null, "prob": 0.061085935682058334, "mask": 0.0}, "badges": {"value": null, "prob": 0.08656222373247147, "mask": 0.0}, "terminate": {"prob": 0.014100611209869385, "mask": 0, "value": null}}, "cls_probs": [0.46837085485458374, 0.4088307321071625, 0.12279842048883438], "s_value": 0.5302755832672119, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.050459008663892746, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.06627295911312103, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.07751479744911194, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04764823988080025, "mask": 0.0}, "website": {"value": 1, "prob": 0.5562790632247925, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07234954833984375, "mask": 0.0}, "badges": {"value": null, "prob": 0.11253119260072708, "mask": 0.0}, "terminate": {"prob": 0.016945183277130127, "mask": 0, "value": null}}, "cls_probs": [0.4801673889160156, 0.4073464572429657, 0.11248614639043808], "s_value": 0.5305505990982056, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.08505354821681976, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.13030895590782166, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.16604776680469513, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.08441412448883057, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19079002737998962, "mask": 0.0}, "badges": {"value": null, "prob": 0.33467602729797363, "mask": 0.0}, "terminate": {"prob": 0.00870949774980545, "mask": 0, "value": null}}, "cls_probs": [0.42623406648635864, 0.42125847935676575, 0.1525074988603592], "s_value": 0.5076096653938293, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.10411527007818222, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.15665000677108765, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10530193895101547, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.23636531829833984, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.38411781191825867, "mask": 0.0}, "terminate": {"prob": 0.01344962790608406, "mask": 0, "value": null}}, "cls_probs": [0.43301764130592346, 0.4115489423274994, 0.15543338656425476], "s_value": 0.5062792897224426, "orig_id": 1699, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.134614497423172, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.2096085250377655, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13258574903011322, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5087434649467468, "mask": 0.0}, "terminate": {"prob": 0.014447716064751148, "mask": 0, "value": null}}, "cls_probs": [0.4395286440849304, 0.40450993180274963, 0.15596146881580353], "s_value": 0.5033881664276123, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.1706906110048294, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16608892381191254, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.648016631603241, "mask": 0.0}, "terminate": {"prob": 0.015203823335468769, "mask": 0, "value": null}}, "cls_probs": [0.41937166452407837, 0.42565807700157166, 0.1549701988697052], "s_value": 0.4948829114437103, "orig_id": 1699, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.19472503662109375, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.7892753481864929, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.015999577939510345, "mask": 0, "value": null}}, "cls_probs": [0.40826863050460815, 0.43454307317733765, 0.15718837082386017], "s_value": 0.4911505877971649, "orig_id": 1699, "true_y": 0, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.14409519731998444, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}], "prob": 0.8412498831748962, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01465489063411951, "mask": 0, "value": null}}, "cls_probs": [0.4213844835758209, 0.4191368818283081, 0.15947866439819336], "s_value": 0.49150550365448, "orig_id": 1699, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.18446408212184906, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.7968190908432007, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.018716750666499138, "mask": 0, "value": null}}, "cls_probs": [0.4013307988643646, 0.4618073105812073, 0.13686183094978333], "s_value": 0.4906696081161499, "orig_id": 1699, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.8902887105941772, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.10971126705408096, "mask": 0, "value": null}}, "cls_probs": [0.3975343704223633, 0.461158812046051, 0.14130674302577972], "s_value": 0.4802829623222351, "orig_id": 1699, "true_y": 0, "last_cost": 1.0, "total_cost": 5.200000002980232}, {"sample": {"about_me": {"value": "<p>Actionscript programmer dabbling in C++, C#, and various</p>\n\n<p>Holds a private pilot's licence</p>\n\n<p>Rides a little red motorbike with approximately the same power output as a combined 4x4 matrix of hamsters</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3876248896121979, 0.4720865488052368, 0.14028851687908173], "s_value": 0.4729149341583252, "orig_id": 1699, "true_y": 0, "last_cost": 0.0, "total_cost": 6.200000002980232}], [{"sample": {"about_me": {"value": "<p>I am not a native speaker of English. Please edit and improve my writings if you found something wrong or unnatural. It helps me a lot.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8166, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am not a native speaker of English. Please edit and improve my writings if you found something wrong or unnatural. It helps me a lot.</p>\n", "prob": 0.022596973925828934, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.021051395684480667, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.020138157531619072, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0185193233191967, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.018861593678593636, "mask": 0.0}, "website": {"value": 0, "prob": 0.45873865485191345, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.013075222261250019, "mask": 0.0}, "badges": {"value": null, "prob": 0.011445035226643085, "mask": 0.0}, "terminate": {"prob": 0.41557368636131287, "mask": 0, "value": null}}, "cls_probs": [0.535433292388916, 0.32578715682029724, 0.13877953588962555], "s_value": 0.5434094071388245, "orig_id": 8166, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am not a native speaker of English. Please edit and improve my writings if you found something wrong or unnatural. It helps me a lot.</p>\n", "prob": 0.0016501593636348844, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.00129901641048491, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0013868999667465687, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0010316113475710154, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0014120794367045164, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0006314671481959522, "mask": 0.0}, "badges": {"value": null, "prob": 0.00041446503018960357, "mask": 0.0}, "terminate": {"prob": 0.9921742677688599, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.6228641271591187, 0.28880879282951355, 0.08832705020904541], "s_value": 0.5674750804901123, "orig_id": 8166, "true_y": 0, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": "<p>Name: SUBHABRATA BANERJEE.\nDate of Birth: 16.08.1971\nAddress: B2-115, Jalvayu Towers, Gurgaon, Haryana. India.\nTelephone: +91-0124-4142594.\nQualification: Post Doctoral from Indian Institute of Science,Bangalore.\nVisiting Scientist: IBM India Research Laboratory, Bangalore.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.79, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0059, "prob": 0.04720594361424446, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 805, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Name: SUBHABRATA BANERJEE.\nDate of Birth: 16.08.1971\nAddress: B2-115, Jalvayu Towers, Gurgaon, Haryana. India.\nTelephone: +91-0124-4142594.\nQualification: Post Doctoral from Indian Institute of Science,Bangalore.\nVisiting Scientist: IBM India Research Laboratory, Bangalore.</p>\n", "prob": 0.046820592135190964, "mask": 0.0}, "views": {"value": 0.79, "prob": 0.07629116624593735, "mask": 0.0}, "reputation": {"value": 0.0059, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.061618492007255554, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07103127986192703, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04346153140068054, "mask": 0.0}, "website": {"value": 0, "prob": 0.5254546999931335, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06712078303098679, "mask": 0.0}, "badges": {"value": null, "prob": 0.0942036360502243, "mask": 0.0}, "terminate": {"prob": 0.01399775967001915, "mask": 0, "value": null}}, "cls_probs": [0.46497225761413574, 0.40960511565208435, 0.12542268633842468], "s_value": 0.5320577621459961, "orig_id": 805, "true_y": 2, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Name: SUBHABRATA BANERJEE.\nDate of Birth: 16.08.1971\nAddress: B2-115, Jalvayu Towers, Gurgaon, Haryana. India.\nTelephone: +91-0124-4142594.\nQualification: Post Doctoral from Indian Institute of Science,Bangalore.\nVisiting Scientist: IBM India Research Laboratory, Bangalore.</p>\n", "prob": 0.07049118727445602, "mask": 0.0}, "views": {"value": 0.79, "prob": 0.10197526216506958, "mask": 0.0}, "reputation": {"value": 0.0059, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.07686541229486465, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08380480110645294, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06736379861831665, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06816592067480087, "mask": 0.0}, "badges": {"value": null, "prob": 0.08465103060007095, "mask": 0.0}, "terminate": {"prob": 0.4466826021671295, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5432767271995544, 0.3726981282234192, 0.08402512967586517], "s_value": 0.5501742362976074, "orig_id": 805, "true_y": 2, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06762965768575668, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 0, "prob": 0.44816169142723083, "mask": 0.0}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.06007486581802368, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09866976737976074, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.07733558118343353, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05744475498795509, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.4744214415550232, "mask": 0.0}, "posts": {"value": null, "prob": 0.08433166146278381, "mask": 0.0}, "badges": {"value": null, "prob": 0.12471843510866165, "mask": 0.0}, "terminate": {"prob": 0.02300349995493889, "mask": 0, "value": null}}, "cls_probs": [0.4679485857486725, 0.4055231511592865, 0.12652833759784698], "s_value": 0.5286751985549927, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.0661495253443718, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.11275096982717514, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.08544813841581345, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.4592699706554413, "mask": 0.0}, "posts": {"value": null, "prob": 0.0942489504814148, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.1606871485710144, "mask": 0.0}, "terminate": {"prob": 0.021445227786898613, "mask": 0, "value": null}}, "cls_probs": [0.4735090732574463, 0.40648937225341797, 0.12000154703855515], "s_value": 0.531981348991394, "orig_id": 1231, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.0678311288356781, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.10489489138126373, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.08599065989255905, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.579780101776123, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.1423763632774353, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01912682317197323, "mask": 0, "value": null}}, "cls_probs": [0.470037043094635, 0.39618512988090515, 0.13377782702445984], "s_value": 0.522106409072876, "orig_id": 1231, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.0840827152132988, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.15593542158603668, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.11037259548902512, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.2883433997631073, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3384650945663452, "mask": 0.0}, "terminate": {"prob": 0.022800784558057785, "mask": 0, "value": null}}, "cls_probs": [0.4741438627243042, 0.39607474207878113, 0.12978143990039825], "s_value": 0.5200822949409485, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.11099407076835632, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.19395489990711212, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1304037719964981, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3546861708164215, "mask": 0.0}, "terminate": {"prob": 0.20996107161045074, "mask": 0, "value": null}}, "cls_probs": [0.5498248338699341, 0.3559785485267639, 0.09419658035039902], "s_value": 0.5336124897003174, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.13811494410037994, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.24815882742404938, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.4690781533718109, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.14464804530143738, "mask": 0, "value": null}}, "cls_probs": [0.5206277370452881, 0.37841635942459106, 0.10095591843128204], "s_value": 0.5150187611579895, "orig_id": 1231, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.21248981356620789, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.3101710379123688, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.4773391783237457, "mask": 0, "value": null}}, "cls_probs": [0.5054093599319458, 0.37446823716163635, 0.12012243270874023], "s_value": 0.49845466017723083, "orig_id": 1231, "true_y": 1, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>I live in Brazil where I work for a software house. </p>\n\n<p>I am a delphi and C# programmer who loves working with new technologies.</p>\n", "prob": 0.31476181745529175, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.6852381825447083, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5173850059509277, 0.3689011037349701, 0.11371397227048874], "s_value": 0.4944196939468384, "orig_id": 1231, "true_y": 1, "last_cost": 0.0, "total_cost": 5.100000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7765, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.042067524045705795, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 1, "prob": 0.621188759803772, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.0769987404346466, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.148077592253685, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.10395338386297226, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12679272890090942, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16148799657821655, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.07459203153848648, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3030097186565399, "mask": 0.0}, "terminate": {"prob": 0.005087730009108782, "mask": 0, "value": null}}, "cls_probs": [0.4247566759586334, 0.41208311915397644, 0.16316016018390656], "s_value": 0.5213593244552612, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.0949157252907753, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17909090220928192, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.12706594169139862, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1541946977376938, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09352602064609528, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3434055745601654, "mask": 0.0}, "terminate": {"prob": 0.007801218889653683, "mask": 0, "value": null}}, "cls_probs": [0.43020501732826233, 0.4035753607749939, 0.16621962189674377], "s_value": 0.5186383128166199, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.1010996475815773, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.19449159502983093, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.13677076995372772, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.16459962725639343, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.39533278346061707, "mask": 0.0}, "terminate": {"prob": 0.007705623283982277, "mask": 0, "value": null}}, "cls_probs": [0.42084801197052, 0.41025328636169434, 0.1688985973596573], "s_value": 0.5141066908836365, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.12215058505535126, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.23539409041404724, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.16291317343711853, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.47193601727485657, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.0076061212457716465, "mask": 0, "value": null}}, "cls_probs": [0.4003198742866516, 0.43301138281822205, 0.1666688621044159], "s_value": 0.5056156516075134, "orig_id": 7765, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.24230554699897766, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.42992645502090454, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.30726462602615356, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.020503362640738487, "mask": 0, "value": null}}, "cls_probs": [0.4076128602027893, 0.4176191985607147, 0.17476795613765717], "s_value": 0.49769216775894165, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.4219159781932831, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.538781464099884, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03930257260799408, "mask": 0, "value": null}}, "cls_probs": [0.4133886396884918, 0.41687947511672974, 0.16973187029361725], "s_value": 0.49087047576904297, "orig_id": 7765, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.9132805466651917, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08671945333480835, "mask": 0, "value": null}}, "cls_probs": [0.4174050986766815, 0.427379310131073, 0.15521559119224548], "s_value": 0.4848311245441437, "orig_id": 7765, "true_y": 1, "last_cost": 1.0, "total_cost": 5.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4242100715637207, 0.4268720746040344, 0.14891783893108368], "s_value": 0.47722727060317993, "orig_id": 7765, "true_y": 1, "last_cost": 0.0, "total_cost": 6.0}], [{"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7049, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 7049, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.0769987404346466, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.148077592253685, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10395338386297226, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.12679272890090942, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16148799657821655, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07459203153848648, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3030097186565399, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005087730009108782, "mask": 0, "value": null}}, "cls_probs": [0.4247566759586334, 0.41208311915397644, 0.16316016018390656], "s_value": 0.5213593244552612, "orig_id": 7049, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.070609450340271, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13829325139522552, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09733446687459946, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11277060955762863, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.14828462898731232, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06973160058259964, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3576032519340515, "mask": 0.0}, "terminate": {"prob": 0.005372782703489065, "mask": 0, "value": null}}, "cls_probs": [0.4309535324573517, 0.40981411933898926, 0.15923236310482025], "s_value": 0.5181341767311096, "orig_id": 7049, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.10691805928945541, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1538313329219818, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.12666389346122742, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1404683142900467, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09992004185914993, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.2668587863445282, "mask": 0.0}, "terminate": {"prob": 0.10533955693244934, "mask": 0, "value": null}}, "cls_probs": [0.4856772720813751, 0.34477269649505615, 0.1695500761270523], "s_value": 0.5122811794281006, "orig_id": 7049, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I'm a Programmer and Web Developer from Tabriz/Iran. I think stackoverflow.com is the best website to find answers to your questions. I have been using this website for a long time but at last I registered  and became a member.</p>\n", "prob": 0.1143113374710083, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16938449442386627, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.13775934278964996, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.15649209916591644, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.33000195026397705, "mask": 0.0}, "terminate": {"prob": 0.09205067902803421, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.47635143995285034, 0.3520662784576416, 0.17158226668834686], "s_value": 0.5077651739120483, "orig_id": 7049, "true_y": 1, "last_cost": 0.0, "total_cost": 3.5}], [{"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1830, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 1830, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.053191423416137695, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.08493419736623764, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06222096458077431, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.08807074278593063, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046432364732027054, "mask": 0.0}, "website": {"value": 1, "prob": 0.5474385023117065, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.1096181869506836, "mask": 0.0}, "terminate": {"prob": 0.008093626238405704, "mask": 0, "value": null}}, "cls_probs": [0.433281272649765, 0.41703012585639954, 0.14968863129615784], "s_value": 0.5145041942596436, "orig_id": 1830, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.08963814377784729, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16942733526229858, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.11810021847486496, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18814072012901306, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08511225879192352, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.34444013237953186, "mask": 0.0}, "terminate": {"prob": 0.005141121800988913, "mask": 0, "value": null}}, "cls_probs": [0.4052918553352356, 0.4295039772987366, 0.16520413756370544], "s_value": 0.5113533735275269, "orig_id": 1830, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.10551399737596512, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.14004555344581604, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.2187279462814331, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10057689994573593, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.4284772574901581, "mask": 0.0}, "terminate": {"prob": 0.00665827514603734, "mask": 0, "value": null}}, "cls_probs": [0.4108120799064636, 0.429080605506897, 0.16010737419128418], "s_value": 0.5036683082580566, "orig_id": 1830, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.12239719927310944, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.24883125722408295, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.11744960397481918, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5025749206542969, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008746957406401634, "mask": 0, "value": null}}, "cls_probs": [0.4147886037826538, 0.43090036511421204, 0.15431098639965057], "s_value": 0.4965662360191345, "orig_id": 1830, "true_y": 0, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.10630888491868973, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.21842826902866364, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.10460086166858673, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Analytical", "prob": 0.5, "mask": 0.0}}], "prob": 0.5622482895851135, "mask": 0.0}, "terminate": {"prob": 0.008413576520979404, "mask": 0, "value": null}}, "cls_probs": [0.4256505072116852, 0.4236808717250824, 0.1506686508655548], "s_value": 0.49629685282707214, "orig_id": 1830, "true_y": 0, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.1339598447084427, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13399595022201538, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Analytical", "prob": 0.5, "mask": 0.0, "selected": true}}], "prob": 0.7177863121032715, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.014257783070206642, "mask": 0, "value": null}}, "cls_probs": [0.4303044378757477, 0.4158705770969391, 0.15382492542266846], "s_value": 0.494013249874115, "orig_id": 1830, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>I've been an avid Linux user since the summer of 1998 and feel it's high time to give back to the ever growing community.\nI've written basic software and scripts to meet my needs and I've occasionally written bits for the local schools (and small companies).</p>\n", "prob": 0.0180499330163002, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.013004555366933346, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Analytical", "prob": 0.0, "mask": 1.0}}], "prob": 0.007703383453190327, "mask": 0.5}, "terminate": {"prob": 0.9612420797348022, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.6097124218940735, 0.2765679359436035, 0.1137196496129036], "s_value": 0.5488543510437012, "orig_id": 1830, "true_y": 0, "last_cost": 0.0, "total_cost": 4.600000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7013, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 7013, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.07047983258962631, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.10195153951644897, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.07685013115406036, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08378825336694717, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06735152751207352, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06815963238477707, "mask": 0.0}, "badges": {"value": null, "prob": 0.08463393151760101, "mask": 0.0}, "terminate": {"prob": 0.44678518176078796, "mask": 0, "value": null}}, "cls_probs": [0.5433760285377502, 0.3726280927658081, 0.08399589359760284], "s_value": 0.5501903891563416, "orig_id": 7013, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.07177818566560745, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.052518200129270554, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.057010848075151443, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.048708271235227585, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.043923746794462204, "mask": 0.0}, "badges": {"value": null, "prob": 0.05244073644280434, "mask": 0.0}, "terminate": {"prob": 0.6736201047897339, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5531851053237915, 0.36968711018562317, 0.07712778449058533], "s_value": 0.5557306408882141, "orig_id": 7013, "true_y": 0, "last_cost": 0.0, "total_cost": 2.0}], [{"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0133, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.0451396144926548, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0725313052535057, "mask": 0.0}, "reputation": {"value": 0.0133, "prob": 0.05382183939218521, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05918215960264206, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.07011939585208893, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.531177282333374, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06768590956926346, "mask": 0.0}, "badges": {"value": null, "prob": 0.08940514922142029, "mask": 0.0}, "terminate": {"prob": 0.010937291197478771, "mask": 0, "value": null}}, "cls_probs": [0.45435237884521484, 0.41012728214263916, 0.1355203092098236], "s_value": 0.5306205749511719, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0133, "prob": 0.09319149702787399, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11058338731527328, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.14358055591583252, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.07691377401351929, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.14987774193286896, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0133, "prob": 0.10312572121620178, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.16330140829086304, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19516275823116302, "mask": 0.0}, "badges": {"value": null, "prob": 0.30702340602874756, "mask": 0.0}, "terminate": {"prob": 0.004595229867845774, "mask": 0, "value": null}}, "cls_probs": [0.39260172843933105, 0.44206786155700684, 0.16533048450946808], "s_value": 0.5083354711532593, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.09114862233400345, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.12185351550579071, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.188346728682518, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.22313299775123596, "mask": 0.0}, "badges": {"value": null, "prob": 0.36928725242614746, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.0062309084460139275, "mask": 0, "value": null}}, "cls_probs": [0.3981033265590668, 0.4390524923801422, 0.1628441959619522], "s_value": 0.50217205286026, "orig_id": 1433, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.08212067931890488, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.1126260906457901, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.168543741106987, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18716879189014435, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.44280338287353516, "mask": 0.0}, "terminate": {"prob": 0.006737379357218742, "mask": 0, "value": null}}, "cls_probs": [0.40658000111579895, 0.4303441345691681, 0.16307586431503296], "s_value": 0.5024265050888062, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.09816651791334152, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.13514156639575958, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.20748665928840637, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.5486093163490295, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.010596000589430332, "mask": 0, "value": null}}, "cls_probs": [0.4110206365585327, 0.4235069453716278, 0.1654723435640335], "s_value": 0.5012962818145752, "orig_id": 1433, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.10170228034257889, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.1392003446817398, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21759729087352753, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.530062735080719, "mask": 0.1666666716337204}, "terminate": {"prob": 0.011437383480370045, "mask": 0, "value": null}}, "cls_probs": [0.41189658641815186, 0.423076331615448, 0.16502702236175537], "s_value": 0.49916601181030273, "orig_id": 1433, "true_y": 1, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.11884753406047821, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.24092893302440643, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.624657154083252, "mask": 0.1666666716337204, "selected": true}, "terminate": {"prob": 0.015566427260637283, "mask": 0, "value": null}}, "cls_probs": [0.4148712456226349, 0.42605695128440857, 0.15907175838947296], "s_value": 0.49447953701019287, "orig_id": 1433, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>SSL isn't good enough.  Your website can be hacked. <br>\nHelp solve the problem by advocating these RFCs:<br>\n<br>\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\n\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\n\n<hr>\n\n<p><strong>About me</strong><br>\nI have no relation to the above sites; I am just an advocate<br>\n<br>\nWhy \"makerofthings7\"?  It's a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I'm not married, no kids yet)\n<br>\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\n", "prob": 0.12427142262458801, "mask": 0.0}, "views": {"value": 0.05, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0133, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.25146955251693726, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.25, "mask": 0.0}}], "prob": 0.6060765385627747, "mask": 0.3333333432674408}, "terminate": {"prob": 0.018182434141635895, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.41538205742836, 0.4249252676963806, 0.1596926897764206], "s_value": 0.489912748336792, "orig_id": 1433, "true_y": 1, "last_cost": 0.0, "total_cost": 4.200000002980232}], [{"sample": {"about_me": {"value": "<p>Ascetic devoted to mathematics</p>\n\n<p><a href=\"http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\">MathJax basic tutorial and quick reference</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.22, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0119, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5260, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Ascetic devoted to mathematics</p>\n\n<p><a href=\"http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\">MathJax basic tutorial and quick reference</a></p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.22, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0119, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 5260, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Ascetic devoted to mathematics</p>\n\n<p><a href=\"http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\">MathJax basic tutorial and quick reference</a></p>\n", "prob": 0.0706249251961708, "mask": 0.0}, "views": {"value": 0.22, "prob": 0.11103102564811707, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0119, "prob": 0.08794380724430084, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.08237209916114807, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.0904250368475914, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07004864513874054, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Looking for a mathematical book on probability and statistics", "prob": 0.1253253072500229, "mask": 0.0}, "body": {"value": "<p><strong>The Short question:</strong> Where can I find a book for the theory of probability and statistics that teaches from scratch in a <strong>rigorous</strong> (very important condition) way? The book must not be elementary, but it has to start from scratch. (For example, I think the Lang/Hungerford algebra texts begin by defining what a group is: in that sense they start from scratch.)</p>\n\n<p><strong>The long question:</strong> I only took an engineering course in probability and statistics. In my opinion, it is very lousy/non-rigorous. You may assume I have no knowledge of probability and statistics. I have to take an independent study statistics course this year. I am allowed to choose a book for the course. It has to be a statistics course. My instructor assumes I know probability because I took the course mentioned above. (I admit I  have a poor understanding of probability and this irritates me a lot.) I'd like to have a book that:</p>\n\n<p>1) Is mathematically oriented and rigorous</p>\n\n<p>2) Has a significant statistics part</p>\n\n<p>3) Teaches the amount of probability needed to do statistics.</p>\n", "prob": 0.12605637311935425, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.12729790806770325, "mask": 0.0}, "views": {"value": 0.0354, "prob": 0.12138865143060684, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.12769676744937897, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12473057955503464, "mask": 0.0}, "tags": {"value": null, "prob": 0.11905675381422043, "mask": 0.0}, "comments": {"value": null, "prob": 0.12844763696193695, "mask": 0.0}}], "prob": 0.0687640979886055, "mask": 0.0}, "badges": {"value": null, "prob": 0.1060422733426094, "mask": 0.0}, "terminate": {"prob": 0.3127480447292328, "mask": 0, "value": null}}, "cls_probs": [0.5507749319076538, 0.37016767263412476, 0.07905746251344681], "s_value": 0.5589570999145508, "orig_id": 5260, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Ascetic devoted to mathematics</p>\n\n<p><a href=\"http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\">MathJax basic tutorial and quick reference</a></p>\n", "prob": 0.07985101640224457, "mask": 0.0}, "views": {"value": 0.22, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0119, "prob": 0.10144451260566711, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0930342972278595, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.10126994550228119, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07987809926271439, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Looking for a mathematical book on probability and statistics", "prob": 0.12500107288360596, "mask": 0.0}, "body": {"value": "<p><strong>The Short question:</strong> Where can I find a book for the theory of probability and statistics that teaches from scratch in a <strong>rigorous</strong> (very important condition) way? The book must not be elementary, but it has to start from scratch. (For example, I think the Lang/Hungerford algebra texts begin by defining what a group is: in that sense they start from scratch.)</p>\n\n<p><strong>The long question:</strong> I only took an engineering course in probability and statistics. In my opinion, it is very lousy/non-rigorous. You may assume I have no knowledge of probability and statistics. I have to take an independent study statistics course this year. I am allowed to choose a book for the course. It has to be a statistics course. My instructor assumes I know probability because I took the course mentioned above. (I admit I  have a poor understanding of probability and this irritates me a lot.) I'd like to have a book that:</p>\n\n<p>1) Is mathematically oriented and rigorous</p>\n\n<p>2) Has a significant statistics part</p>\n\n<p>3) Teaches the amount of probability needed to do statistics.</p>\n", "prob": 0.12579584121704102, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.12768955528736115, "mask": 0.0}, "views": {"value": 0.0354, "prob": 0.1210109293460846, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.1284152865409851, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12485421448945999, "mask": 0.0}, "tags": {"value": null, "prob": 0.11948968470096588, "mask": 0.0}, "comments": {"value": null, "prob": 0.1277434229850769, "mask": 0.0}}], "prob": 0.07935350388288498, "mask": 0.0}, "badges": {"value": null, "prob": 0.1349281221628189, "mask": 0.0}, "terminate": {"prob": 0.330240398645401, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.561369001865387, 0.3648257255554199, 0.07380525767803192], "s_value": 0.557343065738678, "orig_id": 5260, "true_y": 0, "last_cost": 0.0, "total_cost": 2.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7102, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 7102, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0056, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0056, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.0771675556898117, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.14489252865314484, "mask": 0.0}, "reputation": {"value": 0.0056, "prob": 0.10301674902439117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12164411693811417, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.07795343548059464, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19800105690956116, "mask": 0.0}, "badges": {"value": null, "prob": 0.2697715163230896, "mask": 0.0}, "terminate": {"prob": 0.007553049363195896, "mask": 0, "value": null}}, "cls_probs": [0.4237002730369568, 0.4112336337566376, 0.16506606340408325], "s_value": 0.5202891826629639, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.08143333345651627, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.1558399200439453, "mask": 0.0}, "reputation": {"value": 0.0056, "prob": 0.10981719940900803, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12809358537197113, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2070697695016861, "mask": 0.0}, "badges": {"value": null, "prob": 0.3102585971355438, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007487614173442125, "mask": 0, "value": null}}, "cls_probs": [0.4140949249267578, 0.41732847690582275, 0.16857662796974182], "s_value": 0.5163363218307495, "orig_id": 2012, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.07274115830659866, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.14305557310581207, "mask": 0.0}, "reputation": {"value": 0.0056, "prob": 0.10170059651136398, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.10825134068727493, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1618088185787201, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.4040200710296631, "mask": 0.0}, "terminate": {"prob": 0.008422376587986946, "mask": 0, "value": null}}, "cls_probs": [0.42349231243133545, 0.4072098433971405, 0.16929779946804047], "s_value": 0.5162491798400879, "orig_id": 2012, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.07301396876573563, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.14371448755264282, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0056, "prob": 0.10286551713943481, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.10518522560596466, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.12125958502292633, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.12160670012235641, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.12902046740055084, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.11983521282672882, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.13227495551109314, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12385740131139755, "mask": 0.0}, "tags": {"value": null, "prob": 0.12930496037006378, "mask": 0.0}, "comments": {"value": null, "prob": 0.12284072488546371, "mask": 0.0}}], "prob": 0.14388863742351532, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.420650839805603, "mask": 0.0}, "terminate": {"prob": 0.01068141870200634, "mask": 0, "value": null}}, "cls_probs": [0.42966511845588684, 0.41602253913879395, 0.15431229770183563], "s_value": 0.5205233693122864, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.08265227824449539, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.11771363765001297, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.11739827692508698, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.12120205909013748, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.12170655280351639, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.12916122376918793, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.11967863887548447, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.1326560378074646, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12400932610034943, "mask": 0.0}, "tags": {"value": null, "prob": 0.12903210520744324, "mask": 0.0}, "comments": {"value": null, "prob": 0.12255401909351349, "mask": 0.0}}], "prob": 0.15532605350017548, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.5127502679824829, "mask": 0.0}, "terminate": {"prob": 0.014159507118165493, "mask": 0, "value": null}}, "cls_probs": [0.4523371160030365, 0.4051501750946045, 0.1425127536058426], "s_value": 0.5198682546615601, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.09299332648515701, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.12932088971138, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.12098715454339981, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.121646948158741, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.12935523688793182, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.11937108635902405, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.1331522911787033, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12417574226856232, "mask": 0.0}, "tags": {"value": null, "prob": 0.12923046946525574, "mask": 0.0, "selected": true}, "comments": {"value": null, "prob": 0.12208110839128494, "mask": 0.0}}], "prob": 0.16404061019420624, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.594750702381134, "mask": 0.0}, "terminate": {"prob": 0.018894420936703682, "mask": 0, "value": null}}, "cls_probs": [0.461041659116745, 0.40623313188552856, 0.13272523880004883], "s_value": 0.5194042921066284, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.09894933551549911, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.14683467149734497, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.13758088648319244, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.13771888613700867, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.14869804680347443, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1414099484682083, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.15842141211032867, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14843133091926575, "mask": 0.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.12773959338665009, "mask": 0.0}}], "prob": 0.2793547213077545, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.4659736752510071, "mask": 0.0}, "terminate": {"prob": 0.008887603878974915, "mask": 0, "value": null}}, "cls_probs": [0.3938313126564026, 0.4209400713443756, 0.18522858619689941], "s_value": 0.5264091491699219, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": null, "prob": 0.12107299268245697, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.13752788305282593, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.13764433562755585, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.14884059131145477, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.14113810658454895, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.1583155244588852, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14803241193294525, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.12850111722946167, "mask": 0.0}}], "prob": 0.3312091529369354, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.5385752320289612, "mask": 0.0}, "terminate": {"prob": 0.009142580442130566, "mask": 0, "value": null}}, "cls_probs": [0.3684557378292084, 0.4464670717716217, 0.1850772500038147], "s_value": 0.5158421993255615, "orig_id": 2012, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": null, "prob": 0.12700475752353668, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.1614721417427063, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.16159464418888092, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.17487724125385284, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16625092923641205, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.18682041764259338, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.1489846557378769, "mask": 0.0}}], "prob": 0.34753498435020447, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.5137021541595459, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.011758108623325825, "mask": 0, "value": null}}, "cls_probs": [0.37218788266181946, 0.4317094385623932, 0.19610276818275452], "s_value": 0.5098009705543518, "orig_id": 2012, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.3240649104118347, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.15888747572898865, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.15936298668384552, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.17631028592586517, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17983824014663696, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.20089200139045715, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.12470906227827072, "mask": 0.0}}], "prob": 0.6323488354682922, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.043586235493421555, "mask": 0, "value": null}}, "cls_probs": [0.13635247945785522, 0.29361239075660706, 0.5700351595878601], "s_value": 0.4406496286392212, "orig_id": 2012, "true_y": 1, "last_cost": 1.0, "total_cost": 5.700000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.1591823399066925, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.1596500277519226, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.17603379487991333, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.18010149896144867, "mask": 0.0, "selected": true}, "answers": {"value": 0.0, "prob": 0.20068782567977905, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.12434446811676025, "mask": 0.0}}], "prob": 0.9295759797096252, "mask": 0.25, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.07042406499385834, "mask": 0, "value": null}}, "cls_probs": [0.14066657423973083, 0.3028527796268463, 0.5564805865287781], "s_value": 0.44020548462867737, "orig_id": 2012, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.700000002980232}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.194837287068367, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.19533765316009521, "mask": 0.0, "selected": true}, "score": {"value": 0.04, "prob": 0.21452154219150543, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.24600741267204285, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.14929604530334473, "mask": 0.0}}], "prob": 0.9299993515014648, "mask": 0.375, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.07000064849853516, "mask": 0, "value": null}}, "cls_probs": [0.14255182445049286, 0.28975293040275574, 0.5676952600479126], "s_value": 0.43307459354400635, "orig_id": 2012, "true_y": 1, "last_cost": 0.5, "total_cost": 6.800000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.2421453446149826, "mask": 0.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.2671873867511749, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.30839526653289795, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.18227198719978333, "mask": 0.0}}], "prob": 0.9280223846435547, "mask": 0.5, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.07197759300470352, "mask": 0, "value": null}}, "cls_probs": [0.13243244588375092, 0.2740624248981476, 0.5935050845146179], "s_value": 0.4283078908920288, "orig_id": 2012, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 7.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.3302571475505829, "mask": 0.0, "selected": true}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.4295864999294281, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.24015632271766663, "mask": 0.0}}], "prob": 0.936378002166748, "mask": 0.625, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06362202763557434, "mask": 0, "value": null}}, "cls_probs": [0.11611457169055939, 0.2590455710887909, 0.6248399019241333], "s_value": 0.4277169108390808, "orig_id": 2012, "true_y": 1, "last_cost": 0.20000000298023224, "total_cost": 7.4000000059604645}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0056, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>This is a classic Black Swan problem.  HMM1 will assign zero likelihood to symbols D, E, F and HMM2 will assign zero likelihood to symbols A, B, C.  Essentially from HMM1's perspective, D, E, F are impossible, while from HMM2s perspective D, E, F are.  They will never predict them.   (Note that there is nothing about HMMs in this answer -- you could replace \"HMM\" with \"classifier\" or \"model\" and the previous statement would still hold.)</p>\n\n<p>If you knew something about the relationship between the symbols A, B, C and D, E, F you could  get creative with mapping them between each other.</p>\n\n<p>In short, the loglikelihood of that sequence, i.e. a sequence A, B, C using a model trained on D, E, F is always -inf (= log 0).</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.648469090461731, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.35153084993362427, "mask": 0.0}}], "prob": 0.9314718246459961, "mask": 0.75}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.0685281902551651, "mask": 1.0, "value": "class=2", "selected": true}}, "cls_probs": [0.10731811076402664, 0.24858033657073975, 0.6441015005111694], "s_value": 0.4228401780128479, "orig_id": 2012, "true_y": 1, "last_cost": 0.0, "total_cost": 7.600000008940697}], [{"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 5914, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.0769987404346466, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.148077592253685, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10395338386297226, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.12679272890090942, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.16148799657821655, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07459203153848648, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3030097186565399, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005087730009108782, "mask": 0, "value": null}}, "cls_probs": [0.4247566759586334, 0.41208311915397644, 0.16316016018390656], "s_value": 0.5213593244552612, "orig_id": 5914, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.070609450340271, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13829325139522552, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09733446687459946, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11277060955762863, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14828462898731232, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06973160058259964, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3576032519340515, "mask": 0.0}, "terminate": {"prob": 0.005372782703489065, "mask": 0, "value": null}}, "cls_probs": [0.4309535324573517, 0.40981411933898926, 0.15923236310482025], "s_value": 0.5181341767311096, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.073963962495327, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.14576807618141174, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10217815637588501, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11681711673736572, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1560613214969635, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3996872901916504, "mask": 0.0}, "terminate": {"prob": 0.0055241333320736885, "mask": 0, "value": null}}, "cls_probs": [0.4218246638774872, 0.4140007197856903, 0.16417455673217773], "s_value": 0.512089192867279, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.08591890335083008, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16907913982868195, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.12011539191007614, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.13240058720111847, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.48428210616111755, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008203850127756596, "mask": 0, "value": null}}, "cls_probs": [0.4327065348625183, 0.39778468012809753, 0.16950881481170654], "s_value": 0.5126139521598816, "orig_id": 5914, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.17297478020191193, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.31309449672698975, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.227657288312912, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.2628815472126007, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.023391876369714737, "mask": 0, "value": null}}, "cls_probs": [0.4356449246406555, 0.38984525203704834, 0.17450979351997375], "s_value": 0.4998803734779358, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.17517691850662231, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.2165362536907196, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.18940211832523346, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.41888463497161865, "mask": 0, "value": null}}, "cls_probs": [0.4850866198539734, 0.3262963891029358, 0.18861699104309082], "s_value": 0.5034866333007812, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.22343118488788605, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.24629846215248108, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.5302703380584717, "mask": 0, "value": null}}, "cls_probs": [0.49297648668289185, 0.32293981313705444, 0.1840837597846985], "s_value": 0.4949330985546112, "orig_id": 5914, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.3600725829601288, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.6399273872375488, "mask": 0, "value": null}}, "cls_probs": [0.4969930052757263, 0.33425530791282654, 0.16875167191028595], "s_value": 0.49048227071762085, "orig_id": 5914, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Hello, I'm an Engineering graduate and I like exploring facts and stories. - MD</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4971240162849426, 0.3386741280555725, 0.16420188546180725], "s_value": 0.48891276121139526, "orig_id": 5914, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.06915700435638428, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13370193541049957, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.09319149702787399, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11058338731527328, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14358055591583252, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17294655740261078, "mask": 0.0}, "badges": {"value": null, "prob": 0.27175936102867126, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005079738795757294, "mask": 0, "value": null}}, "cls_probs": [0.40628311038017273, 0.4293951094150543, 0.16432179510593414], "s_value": 0.516252338886261, "orig_id": 5288, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.06393192708492279, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12605567276477814, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.08850457519292831, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09802098572254181, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13112056255340576, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1466534584760666, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.3399604856967926, "mask": 0.0}, "terminate": {"prob": 0.005752329248934984, "mask": 0, "value": null}}, "cls_probs": [0.415014386177063, 0.4198645353317261, 0.16512107849121094], "s_value": 0.5162007808685303, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.07207787781953812, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.10018207132816315, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.10867641866207123, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1452181339263916, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1598517745733261, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.4066389203071594, "mask": 0.0}, "terminate": {"prob": 0.0073548960499465466, "mask": 0, "value": null}}, "cls_probs": [0.42760246992111206, 0.4111357033252716, 0.16126178205013275], "s_value": 0.509727418422699, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.08002081513404846, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1181529238820076, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.15791943669319153, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16984225809574127, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.46474653482437134, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.009318092837929726, "mask": 0, "value": null}}, "cls_probs": [0.43295150995254517, 0.4134349524974823, 0.15361350774765015], "s_value": 0.5071498155593872, "orig_id": 5288, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.1463891565799713, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.21795836091041565, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.27775058150291443, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.33628636598587036, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.021615536883473396, "mask": 0, "value": null}}, "cls_probs": [0.43198615312576294, 0.4147212505340576, 0.15329255163669586], "s_value": 0.49076271057128906, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.20652811229228973, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.29675939679145813, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.4564018249511719, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04031066596508026, "mask": 0, "value": null}}, "cls_probs": [0.44035252928733826, 0.40089309215545654, 0.15875433385372162], "s_value": 0.48950648307800293, "orig_id": 5288, "true_y": 0, "last_cost": 1.0, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.37611496448516846, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.5607452988624573, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06313978135585785, "mask": 0, "value": null}}, "cls_probs": [0.4301862120628357, 0.411272257566452, 0.15854154527187347], "s_value": 0.4857732653617859, "orig_id": 5288, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.9148948788642883, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08510507643222809, "mask": 0, "value": null}}, "cls_probs": [0.40668803453445435, 0.4401746094226837, 0.1531374156475067], "s_value": 0.4734210968017578, "orig_id": 5288, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>New PhD student in Computing at the University of Utah; interested in information visualization, particularly in the infovis process and developing tools to facilitate that process. I also have some bioinformatics experience (working with next-generation sequencing data) and interests in visualization of biological data (specifically genetics and neuroscience).</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4153563976287842, 0.43109336495399475, 0.15355020761489868], "s_value": 0.4704691171646118, "orig_id": 5288, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2097, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 2097, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>I'm a socialist (of the council-communist current promoted by the likes of Anton Luxemburg, Pannekoek and other Dutch German leftists, <b>NOT</b> state-socialism) with an avid interest in history and economics.  I am currently studying computer science at the University of Central Florida and hope to graduate with my Bachelors degree within the next year.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5022, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a socialist (of the council-communist current promoted by the likes of Anton Luxemburg, Pannekoek and other Dutch German leftists, <b>NOT</b> state-socialism) with an avid interest in history and economics.  I am currently studying computer science at the University of Central Florida and hope to graduate with my Bachelors degree within the next year.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.002, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 5022, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "Research scientist finds himself dragged into software engineering-- at least it's interesting.", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0161, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1201, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "Research scientist finds himself dragged into software engineering-- at least it's interesting.", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0161, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 1201, "true_y": 2, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "Research scientist finds himself dragged into software engineering-- at least it's interesting.", "prob": 0.06386525183916092, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.09098470956087112, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0161, "prob": 0.07489465922117233, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06941630691289902, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.06202324852347374, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06582651287317276, "mask": 0.0}, "badges": {"value": null, "prob": 0.0724131315946579, "mask": 0.0}, "terminate": {"prob": 0.5005761384963989, "mask": 0, "value": null}}, "cls_probs": [0.5411810278892517, 0.36297500133514404, 0.09584403783082962], "s_value": 0.550512969493866, "orig_id": 1201, "true_y": 2, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "Research scientist finds himself dragged into software engineering-- at least it's interesting.", "prob": 0.0669146254658699, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0161, "prob": 0.08114033937454224, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07240070402622223, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.06571811437606812, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06683533638715744, "mask": 0.0}, "badges": {"value": null, "prob": 0.08890900760889053, "mask": 0.0}, "terminate": {"prob": 0.558081865310669, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5619591474533081, 0.3534775674343109, 0.08456332981586456], "s_value": 0.5521799325942993, "orig_id": 1201, "true_y": 2, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>Garbage collector</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6288, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Garbage collector</p>\n", "prob": 0.032880865037441254, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.041435662657022476, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.033930495381355286, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.03720048442482948, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.04095682501792908, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.02832750976085663, "mask": 0.0}, "website": {"value": 0, "prob": 0.7165712714195251, "mask": 0.0}, "posts": {"value": null, "prob": 0.036715902388095856, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03198100998997688, "mask": 0, "value": null}}, "cls_probs": [0.4731794595718384, 0.3826155662536621, 0.14420495927333832], "s_value": 0.5287606716156006, "orig_id": 6288, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Garbage collector</p>\n", "prob": 0.04514259845018387, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.058759644627571106, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04787617176771164, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05159898102283478, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.03990783914923668, "mask": 0.0}, "website": {"value": 0, "prob": 0.6593658924102783, "mask": 0.0}, "posts": {"value": null, "prob": 0.052463069558143616, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.0448857806622982, "mask": 0, "value": null}}, "cls_probs": [0.4743916690349579, 0.37920668721199036, 0.14640170335769653], "s_value": 0.5254586935043335, "orig_id": 6288, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Garbage collector</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.05407317355275154, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04463017359375954, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.04718168079853058, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.03847699612379074, "mask": 0.0}, "website": {"value": 0, "prob": 0.6792992949485779, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.045984841883182526, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.0903538390994072, "mask": 0, "value": null}}, "cls_probs": [0.4801112115383148, 0.380850613117218, 0.13903817534446716], "s_value": 0.5283442139625549, "orig_id": 6288, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Garbage collector</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.015656977891921997, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.014340846799314022, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.012985272333025932, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.013371123000979424, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.010241569951176643, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.9334042072296143, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5636498332023621, 0.3400322496891022, 0.09631795436143875], "s_value": 0.5487822890281677, "orig_id": 6288, "true_y": 1, "last_cost": 0.0, "total_cost": 3.0}], [{"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0128, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 7515, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0128, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.08534760773181915, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 1, "prob": 0.3274863362312317, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.15204817056655884, "mask": 0.0}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 7515, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.06145130470395088, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12017360329627991, "mask": 0.0}, "reputation": {"value": 0.0128, "prob": 0.08465293794870377, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 7515, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.06393192708492279, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12605567276477814, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0128, "prob": 0.08850457519292831, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09802098572254181, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.13112056255340576, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1466534584760666, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.3399604856967926, "mask": 0.0}, "terminate": {"prob": 0.005752329248934984, "mask": 0, "value": null}}, "cls_probs": [0.415014386177063, 0.4198645353317261, 0.16512107849121094], "s_value": 0.5162007808685303, "orig_id": 7515, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.07239611446857452, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0128, "prob": 0.10048998147249222, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1088821217417717, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.14528338611125946, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1597118228673935, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.4057518541812897, "mask": 0.0}, "terminate": {"prob": 0.007484792731702328, "mask": 0, "value": null}}, "cls_probs": [0.42639249563217163, 0.41192325949668884, 0.16168427467346191], "s_value": 0.5099958777427673, "orig_id": 7515, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.08329061418771744, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0128, "prob": 0.11680615693330765, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.12124529480934143, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17523840069770813, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.492350697517395, "mask": 0.0}, "terminate": {"prob": 0.01106886938214302, "mask": 0, "value": null}}, "cls_probs": [0.43567797541618347, 0.40191513299942017, 0.1624068319797516], "s_value": 0.5114887356758118, "orig_id": 7515, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.1201753243803978, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0128, "prob": 0.1534537672996521, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1427043229341507, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.4262123703956604, "mask": 0.0}, "terminate": {"prob": 0.1574542075395584, "mask": 0, "value": null}}, "cls_probs": [0.4999885559082031, 0.34940728545188904, 0.15060411393642426], "s_value": 0.5140201449394226, "orig_id": 7515, "true_y": 0, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.10930388420820236, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0128, "prob": 0.14740629494190216, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Designing Computer Science experiments (Hypotheses and errors?)", "prob": 0.060683052986860275, "mask": 0.0}, "body": {"value": "<p>For my master thesis I want to set-up several experiments. One thing my professor has been complaining about is that most computer science experiments are lacking scientifically. To not fall into this trap I want to properly design my experiments. Unfortunately I could not find much literature on CS experiment design. (Except for D. Feitelson's \"Experimental Computer Science: The Need for a Cultural Change\" available <a href=\"http://www.cs.huji.ac.il/~feit/pub.html\" rel=\"nofollow\">here</a> which is more of an appeal than a guide.)</p>\n\n<p>I remember that in statistics people often use a H_0 and H_1 hypothesis together with the Type I and Type II error. I wonder if this is applicable in the field of computer science.</p>\n\n<p>A quick example that I could think of:</p>\n\n<blockquote>\n  <p>H_0: Both methods create the same output in this scenario </p>\n  \n  <p>H_1: One method creates a worse output in this scenario (worse: according to\n  metric X)</p>\n</blockquote>\n\n<p>But then how would I calculate the Type I and Type II errors? In a speed-benchmark I could run the experiments multiple times and then possibly calculate how likely it is that the benchmark result is still wrong. However often I just want to compare the output of two deterministic methods. Measuring the quality according to a certain metric. This result doesn't change when running the experiment multiple times. What do I do then? Is the null hypothesis approach not a good fit for these kind of experiments?</p>\n", "prob": 0.06141861900687218, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06441430747509003, "mask": 0.0}, "views": {"value": 0.0107, "prob": 0.05977289378643036, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0659390464425087, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06225285306572914, "mask": 0.0}, "tags": {"value": null, "prob": 0.06398525834083557, "mask": 0.0}, "comments": {"value": null, "prob": 0.06153392046689987, "mask": 0.0}}, {"title": {"value": "Literature on measuring speed and memory usage of software", "prob": 0.060683052986860275, "mask": 0.0}, "body": {"value": "<p>In one of my experiments for measuring the performance of a specific type of software I measure the execution time and memory usage. Now I know that measuring these things can be extremely problematic since the operating system interferes with how the program is run and other programs/processes can be run during the execution of the benchmark.</p>\n\n<p>I remember the following best practices:</p>\n\n<ul>\n<li>Run multiple times</li>\n<li>Take the smallest time it took to perform the benchmark, not the average.</li>\n<li>Take the memory usage with a HUGE grain of salt and only talk about it when the memory used is large (if its in kilobytes or a few megabytes you can't say much about it) and when the difference is significant (at least a couple of tens of megabytes).</li>\n</ul>\n\n<p>While its fun that I can remember these best practices and that I can argue for the correctness of each of them I'd rather tie these best practices to some literature. However I have not yet found any that speaks specifically about speed/memory benchmarks. Does anybody know of papers or other material I can cite to substantiates these best practices?</p>\n\n<p><em>(I'm not sure if this question belongs on cross validated. But since I asked a related question <a href=\"http://stats.stackexchange.com/questions/101526/designing-computer-science-experiments-hypotheses-and-errors\">here</a> I thought it would be best to start here. If this question fits better on academia, cs theory or stack overflow, please help me move it to the correct sub-site)</em> </p>\n", "prob": 0.06141861900687218, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06441430747509003, "mask": 0.0}, "views": {"value": 0.0021, "prob": 0.05977289378643036, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0659390464425087, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06225285306572914, "mask": 0.0}, "tags": {"value": null, "prob": 0.06398525834083557, "mask": 0.0}, "comments": {"value": null, "prob": 0.06153392046689987, "mask": 0.0}}], "prob": 0.11464481055736542, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}], "prob": 0.46009936928749084, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.16854563355445862, "mask": 0, "value": null}}, "cls_probs": [0.5223072171211243, 0.358758807182312, 0.11893396079540253], "s_value": 0.5317848920822144, "orig_id": 7515, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>At the age of 14 I started programming in PowerPoint\u2019s VBScript environment, quickly after that I switched to Visual Basic 6 learning basic programming skills. A few years later I applied these skill to write my first \u2018database\u2019 application for the Salida youth crisis relief center. The application used VB6 and flat files on an FTP server to mimic a real database. Around the same time I wrote my first game, a two player top-down shooter. A year later I rewrote the database application in C#, now using a real database (MySQL) which allowed users to really work together. This application is still used daily by approximately 20 people at Stichting Cardan.</p>\n\n<p>Since 2008 I started focusing on games. In 2008 I discovered Microsoft\u2019s XNA framework (a managed DirectX wrapper for PC, Xbox and Windows Phone). Since then I\u2019ve been an active member of the XNA community. While I was learning the framework and general game development techniques I started keeping <a href=\"http://roy-t.nl\" rel=\"nofollow\">a blog</a>. At this blog I periodically post tutorials and code snippets. I also wrote a few tutorials for popular XNA sites like www.ziggyware.com (now defunct) and <a href=\"http://madgamedev.com\" rel=\"nofollow\">www.sgtconker.com</a> (renamed to MadGameDev). With these tutorials I won a couple of prizes.</p>\n\n<p>In 2010 I met a few game designers and artists working on the game <a href=\"http://www.hollandiagame.com\" rel=\"nofollow\">Hollandia</a>. They had just won a Dutch Game Award but their programmer was unable to continue working on the game engine, which caused the project to stall. I rewrote most of their engine and coupled it to an existing physics framework. Around the same time I worked with a 15 man strong team on an extensive web shop project for Q-Free. We developed a process that automatically pulled new releases from source control and integrated them into our web shop.</p>\n\n<p>While finishing up my Bsc. in Computing Science I worked at Science LinX (2009-2012) where I grew from a system administrator to serious game developer with a diverse set of responsibilities, from building exhibits to managing projects.</p>\n\n<p>During my master at Utrecht University I interned at Abbey Games. I'm currently finishing up my thesis (on navigation mesh) and getting ready to join the real world.</p>\n", "prob": 0.1150217354297638, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0128, "prob": 0.15220892429351807, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Designing Computer Science experiments (Hypotheses and errors?)", "prob": 0.06085582822561264, "mask": 0.0}, "body": {"value": "<p>For my master thesis I want to set-up several experiments. One thing my professor has been complaining about is that most computer science experiments are lacking scientifically. To not fall into this trap I want to properly design my experiments. Unfortunately I could not find much literature on CS experiment design. (Except for D. Feitelson's \"Experimental Computer Science: The Need for a Cultural Change\" available <a href=\"http://www.cs.huji.ac.il/~feit/pub.html\" rel=\"nofollow\">here</a> which is more of an appeal than a guide.)</p>\n\n<p>I remember that in statistics people often use a H_0 and H_1 hypothesis together with the Type I and Type II error. I wonder if this is applicable in the field of computer science.</p>\n\n<p>A quick example that I could think of:</p>\n\n<blockquote>\n  <p>H_0: Both methods create the same output in this scenario </p>\n  \n  <p>H_1: One method creates a worse output in this scenario (worse: according to\n  metric X)</p>\n</blockquote>\n\n<p>But then how would I calculate the Type I and Type II errors? In a speed-benchmark I could run the experiments multiple times and then possibly calculate how likely it is that the benchmark result is still wrong. However often I just want to compare the output of two deterministic methods. Measuring the quality according to a certain metric. This result doesn't change when running the experiment multiple times. What do I do then? Is the null hypothesis approach not a good fit for these kind of experiments?</p>\n", "prob": 0.06155131757259369, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06430891156196594, "mask": 0.0}, "views": {"value": 0.0107, "prob": 0.059923943132162094, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.06574014574289322, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06232659891247749, "mask": 0.0}, "tags": {"value": null, "prob": 0.06368398666381836, "mask": 0.0}, "comments": {"value": null, "prob": 0.061609234660863876, "mask": 0.0}}, {"title": {"value": "Literature on measuring speed and memory usage of software", "prob": 0.06085582822561264, "mask": 0.0}, "body": {"value": "<p>In one of my experiments for measuring the performance of a specific type of software I measure the execution time and memory usage. Now I know that measuring these things can be extremely problematic since the operating system interferes with how the program is run and other programs/processes can be run during the execution of the benchmark.</p>\n\n<p>I remember the following best practices:</p>\n\n<ul>\n<li>Run multiple times</li>\n<li>Take the smallest time it took to perform the benchmark, not the average.</li>\n<li>Take the memory usage with a HUGE grain of salt and only talk about it when the memory used is large (if its in kilobytes or a few megabytes you can't say much about it) and when the difference is significant (at least a couple of tens of megabytes).</li>\n</ul>\n\n<p>While its fun that I can remember these best practices and that I can argue for the correctness of each of them I'd rather tie these best practices to some literature. However I have not yet found any that speaks specifically about speed/memory benchmarks. Does anybody know of papers or other material I can cite to substantiates these best practices?</p>\n\n<p><em>(I'm not sure if this question belongs on cross validated. But since I asked a related question <a href=\"http://stats.stackexchange.com/questions/101526/designing-computer-science-experiments-hypotheses-and-errors\">here</a> I thought it would be best to start here. If this question fits better on academia, cs theory or stack overflow, please help me move it to the correct sub-site)</em> </p>\n", "prob": 0.06155131757259369, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.06430891156196594, "mask": 0.0}, "views": {"value": 0.0021, "prob": 0.059923943132162094, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.06574014574289322, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06232659891247749, "mask": 0.0}, "tags": {"value": null, "prob": 0.06368398666381836, "mask": 0.0}, "comments": {"value": null, "prob": 0.061609234660863876, "mask": 0.0}}], "prob": 0.12121429294347763, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.43004852533340454, "mask": 0.25}, "terminate": {"prob": 0.18150655925273895, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5263610482215881, 0.3550952076911926, 0.11854366958141327], "s_value": 0.5309180021286011, "orig_id": 7515, "true_y": 0, "last_cost": 0.0, "total_cost": 4.600000001490116}], [{"sample": {"about_me": {"value": "<p>Student in computer science and a programmer. My primary focus is mobile development for iOS/Android. My interests is low-level optimization, functional programming, NoSQL databases, server-side development, category theory</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4470, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Student in computer science and a programmer. My primary focus is mobile development for iOS/Android. My interests is low-level optimization, functional programming, NoSQL databases, server-side development, category theory</p>\n", "prob": 0.042067524045705795, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.060199297964572906, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04677363112568855, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05397053435444832, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060479216277599335, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035835765302181244, "mask": 0.0}, "website": {"value": 0, "prob": 0.621188759803772, "mask": 0.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.06880562007427216, "mask": 0.0}, "terminate": {"prob": 0.01067968551069498, "mask": 0, "value": null}}, "cls_probs": [0.4583556056022644, 0.394868940114975, 0.14677540957927704], "s_value": 0.5266878604888916, "orig_id": 4470, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Student in computer science and a programmer. My primary focus is mobile development for iOS/Android. My interests is low-level optimization, functional programming, NoSQL databases, server-side development, category theory</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.06854021549224854, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05300510674715042, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.059797417372465134, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06651376932859421, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04132568836212158, "mask": 0.0}, "website": {"value": 0, "prob": 0.621497631072998, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0732167661190033, "mask": 0.0}, "terminate": {"prob": 0.016103442758321762, "mask": 0, "value": null}}, "cls_probs": [0.45898669958114624, 0.3966839909553528, 0.14432933926582336], "s_value": 0.5228167176246643, "orig_id": 4470, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Student in computer science and a programmer. My primary focus is mobile development for iOS/Android. My interests is low-level optimization, functional programming, NoSQL databases, server-side development, category theory</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.06475071609020233, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.05614883825182915, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05315512418746948, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.05556522309780121, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04829368740320206, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.04023367911577225, "mask": 0.0}, "terminate": {"prob": 0.6818526387214661, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535335123538971, 0.3580611050128937, 0.10660377144813538], "s_value": 0.539081335067749, "orig_id": 4470, "true_y": 0, "last_cost": 0.0, "total_cost": 2.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6682, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 6682, "true_y": 0, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.07328308373689651, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.1409580111503601, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.09756907820701599, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.15240715444087982, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07209322601556778, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17674970626831055, "mask": 0.0}, "badges": {"value": null, "prob": 0.28212639689445496, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.004813333507627249, "mask": 0, "value": null}}, "cls_probs": [0.3990987539291382, 0.44010719656944275, 0.16079400479793549], "s_value": 0.5127114057540894, "orig_id": 5557, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.1081824004650116, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.19145607948303223, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.13676294684410095, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.21037419140338898, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1033046767115593, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2402203530073166, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.00969931110739708, "mask": 0, "value": null}}, "cls_probs": [0.40764161944389343, 0.4234164357185364, 0.1689419448375702], "s_value": 0.5060752034187317, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.13484138250350952, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.17147308588027954, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.2567116618156433, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.12922538816928864, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.29441070556640625, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01333775743842125, "mask": 0, "value": null}}, "cls_probs": [0.4142918884754181, 0.41951191425323486, 0.16619618237018585], "s_value": 0.49985864758491516, "orig_id": 5557, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.19328394532203674, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.2419895976781845, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.37011387944221497, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1776391714811325, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.016973448917269707, "mask": 0, "value": null}}, "cls_probs": [0.41835615038871765, 0.4110824465751648, 0.17056143283843994], "s_value": 0.4985339045524597, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.2508326768875122, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.4886738061904907, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.2379918396472931, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.022501574829220772, "mask": 0, "value": null}}, "cls_probs": [0.42346063256263733, 0.41851726174354553, 0.15802210569381714], "s_value": 0.4911932945251465, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.48107844591140747, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.46920520067214966, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04971635341644287, "mask": 0, "value": null}}, "cls_probs": [0.4275738000869751, 0.41966694593429565, 0.15275919437408447], "s_value": 0.489362895488739, "orig_id": 5557, "true_y": 1, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.9132805466651917, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08671945333480835, "mask": 0, "value": null}}, "cls_probs": [0.4174050986766815, 0.427379310131073, 0.15521559119224548], "s_value": 0.4848311245441437, "orig_id": 5557, "true_y": 1, "last_cost": 1.0, "total_cost": 5.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4242100715637207, 0.4268720746040344, 0.14891783893108368], "s_value": 0.47722727060317993, "orig_id": 5557, "true_y": 1, "last_cost": 0.0, "total_cost": 6.0}], [{"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0654018297791481, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 1045, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.13743709027767181, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0983174666762352, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11399798840284348, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.14147010445594788, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07554514706134796, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17744800448417664, "mask": 0.0}, "badges": {"value": null, "prob": 0.2478794902563095, "mask": 0.0}, "terminate": {"prob": 0.007904719561338425, "mask": 0, "value": null}}, "cls_probs": [0.41559192538261414, 0.42174628376960754, 0.16266180574893951], "s_value": 0.5135846734046936, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1480793058872223, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10466322302818298, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12128537893295288, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.1536129117012024, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1892327219247818, "mask": 0.0}, "badges": {"value": null, "prob": 0.2755070626735687, "mask": 0.0}, "terminate": {"prob": 0.007619363255798817, "mask": 0, "value": null}}, "cls_probs": [0.4030021131038666, 0.4328747093677521, 0.16412322223186493], "s_value": 0.509828507900238, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.16913795471191406, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.11765314638614655, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.17766983807086945, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21089094877243042, "mask": 0.0}, "badges": {"value": null, "prob": 0.3173695206642151, "mask": 0.0}, "terminate": {"prob": 0.007278564851731062, "mask": 0, "value": null}}, "cls_probs": [0.38588428497314453, 0.45174440741539, 0.16237138211727142], "s_value": 0.501296877861023, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1423368901014328, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.21003898978233337, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2474149465560913, "mask": 0.0}, "badges": {"value": null, "prob": 0.39032605290412903, "mask": 0.0}, "terminate": {"prob": 0.009883119724690914, "mask": 0, "value": null}}, "cls_probs": [0.3900717496871948, 0.44841116666793823, 0.16151711344718933], "s_value": 0.49461644887924194, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.2423916906118393, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2795461416244507, "mask": 0.0}, "badges": {"value": null, "prob": 0.46463721990585327, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013424902223050594, "mask": 0, "value": null}}, "cls_probs": [0.3944733440876007, 0.4493197798728943, 0.1562069207429886], "s_value": 0.48833099007606506, "orig_id": 1045, "true_y": 1, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.2165333777666092, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.23803509771823883, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}], "prob": 0.5317919254302979, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013639622367918491, "mask": 0, "value": null}}, "cls_probs": [0.399413138628006, 0.44731980562210083, 0.1532670259475708], "s_value": 0.49137014150619507, "orig_id": 1045, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.2394130527973175, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2646394371986389, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.47988831996917725, "mask": 0.5}, "terminate": {"prob": 0.016059182584285736, "mask": 0, "value": null}}, "cls_probs": [0.38836947083473206, 0.47623613476753235, 0.1353943943977356], "s_value": 0.48684781789779663, "orig_id": 1045, "true_y": 1, "last_cost": 1.0, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.324642151594162, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.6575008034706116, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.017856968566775322, "mask": 0, "value": null}}, "cls_probs": [0.3923843204975128, 0.4701710343360901, 0.13744470477104187], "s_value": 0.4829736649990082, "orig_id": 1045, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.9347904920578003, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.0652095228433609, "mask": 0, "value": null}}, "cls_probs": [0.3894464075565338, 0.46943971514701843, 0.1411137729883194], "s_value": 0.4741997718811035, "orig_id": 1045, "true_y": 1, "last_cost": 0.5, "total_cost": 5.700000002980232}, {"sample": {"about_me": {"value": "<p>A data and visualisation junkie, occasional electronica artist and amateur wildlife photographer.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3861657679080963, 0.47300219535827637, 0.14083200693130493], "s_value": 0.4729604721069336, "orig_id": 1045, "true_y": 1, "last_cost": 0.0, "total_cost": 6.200000002980232}], [{"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6853, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 6853, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.06145130470395088, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12017360329627991, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.08465293794870377, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 6853, "true_y": 1, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.06275638192892075, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12122414261102676, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.08636905997991562, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.09362088143825531, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.12089504301548004, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0658499151468277, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.12207033485174179, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.12196899950504303, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.12898430228233337, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.12025228142738342, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.13163673877716064, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.123863585293293, "mask": 0.0}, "tags": {"value": null, "prob": 0.127655029296875, "mask": 0.0}, "comments": {"value": null, "prob": 0.12356878072023392, "mask": 0.0}}], "prob": 0.1298789381980896, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0, "selected": true}}], "prob": 0.3119758665561676, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.00742984376847744, "mask": 0, "value": null}}, "cls_probs": [0.4223833382129669, 0.4217914640903473, 0.1558251678943634], "s_value": 0.5226652026176453, "orig_id": 6853, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.08360454440116882, "mask": 0.0, "selected": true}, "views": {"value": 0.03, "prob": 0.13297542929649353, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10263040661811829, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1045435220003128, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.11192875355482101, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08193384110927582, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.12370935827493668, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.12456921488046646, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.12820962071418762, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.12064727395772934, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.12990331649780273, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.12489477545022964, "mask": 0.0}, "tags": {"value": null, "prob": 0.12247958034276962, "mask": 0.0}, "comments": {"value": null, "prob": 0.1255868375301361, "mask": 0.0}}], "prob": 0.07855908572673798, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}], "prob": 0.18614065647125244, "mask": 0.5}, "terminate": {"prob": 0.11768373847007751, "mask": 0, "value": null}}, "cls_probs": [0.5103973150253296, 0.3794459402561188, 0.11015672236680984], "s_value": 0.5366217494010925, "orig_id": 6853, "true_y": 1, "last_cost": 1.0, "total_cost": 2.600000001490116}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.13298530876636505, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.10528071224689484, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.10268323123455048, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.10631446540355682, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08614759892225266, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.12364116311073303, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.12478891760110855, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.12800469994544983, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.12049857527017593, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.12982968986034393, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.1252209097146988, "mask": 0.0}, "tags": {"value": null, "prob": 0.1226118952035904, "mask": 0.0}, "comments": {"value": null, "prob": 0.12540416419506073, "mask": 0.0, "selected": true}}], "prob": 0.07359451800584793, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}], "prob": 0.16315554082393646, "mask": 0.5}, "terminate": {"prob": 0.2298385500907898, "mask": 0, "value": null}}, "cls_probs": [0.5249938368797302, 0.3700394928455353, 0.1049666777253151], "s_value": 0.531969428062439, "orig_id": 6853, "true_y": 1, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.13999222218990326, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11122559010982513, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11141648888587952, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1137663722038269, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09382103383541107, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.13936181366443634, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.14030569791793823, "mask": 0.0, "selected": true}, "score": {"value": 0.0, "prob": 0.1451936960220337, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.13737288117408752, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.14903683960437775, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.1433328539133072, "mask": 0.0}, "tags": {"value": null, "prob": 0.14539627730846405, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.09560689330101013, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}], "prob": 0.19482742249965668, "mask": 0.5}, "terminate": {"prob": 0.13934394717216492, "mask": 0, "value": null}}, "cls_probs": [0.5044671297073364, 0.37193503975868225, 0.1235978752374649], "s_value": 0.5294535160064697, "orig_id": 6853, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.1335151195526123, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11004474014043808, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.109918974339962, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.10609535127878189, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09770479053258896, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.16055208444595337, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.16748417913913727, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.15955372154712677, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.17286458611488342, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.1668354570865631, "mask": 0.0}, "tags": {"value": null, "prob": 0.17270998656749725, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.10209934413433075, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}], "prob": 0.17607128620147705, "mask": 0.5}, "terminate": {"prob": 0.16455042362213135, "mask": 0, "value": null}}, "cls_probs": [0.5053180456161499, 0.35293322801589966, 0.14174874126911163], "s_value": 0.5227056741714478, "orig_id": 6853, "true_y": 1, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.05641581863164902, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.052199747413396835, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.039077602326869965, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04847598075866699, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.1631559431552887, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.16702361404895782, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.1617872714996338, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.17067769169807434, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.16867731511592865, "mask": 0.0}, "tags": {"value": null, "prob": 0.1686781495809555, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.029734641313552856, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}, {"badge": {"value": "Tumbleweed", "prob": 0.0, "mask": 1.0}}], "prob": 0.049892544746398926, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.7242037057876587, "mask": 0, "value": null}}, "cls_probs": [0.5715564489364624, 0.30491191148757935, 0.12353168427944183], "s_value": 0.5382884740829468, "orig_id": 6853, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>I. am.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.04254821315407753, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.040629979223012924, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.030419640243053436, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.03863172605633736, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "Which approaches are known to extract product / service data from previously unknown webpages via machine-learning?", "prob": 0.16422240436077118, "mask": 0.0}, "body": {"value": "<p>To avoid reinventing the wheel, which approaches are known to extract product / service data from previously unknown webpages via machine-learning?</p>\n\n<p>Which keywords in a search engine might give me better results about this topic?</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.0, "prob": 0.16615986824035645, "mask": 0.0}, "views": {"value": 0.0016, "prob": 0.16345135867595673, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.16901841759681702, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.1690208613872528, "mask": 0.0}, "tags": {"value": null, "prob": 0.16812707483768463, "mask": 0.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.02334488369524479, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.824425458908081, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5587829947471619, 0.3039325177669525, 0.137284517288208], "s_value": 0.5272991061210632, "orig_id": 6853, "true_y": 1, "last_cost": 0.0, "total_cost": 5.200000002980232}], [{"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.06316898763179779, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0118, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.04317622631788254, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.05264774337410927, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.056019239127635956, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.06631609052419662, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.03961151838302612, "mask": 0.0}, "website": {"value": 1, "prob": 0.5790371298789978, "mask": 0.0}, "posts": {"value": null, "prob": 0.0617695227265358, "mask": 0.0}, "badges": {"value": null, "prob": 0.08736518770456314, "mask": 0.0}, "terminate": {"prob": 0.014057343825697899, "mask": 0, "value": null}}, "cls_probs": [0.4677671790122986, 0.4091457724571228, 0.1230870932340622], "s_value": 0.5300579071044922, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.05674067512154579, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.06854918599128723, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0922752395272255, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05205552279949188, "mask": 0.0}, "website": {"value": 1, "prob": 0.5075803995132446, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08680707216262817, "mask": 0.0}, "badges": {"value": null, "prob": 0.12497220933437347, "mask": 0.0}, "terminate": {"prob": 0.011019686236977577, "mask": 0, "value": null}}, "cls_probs": [0.44383910298347473, 0.41963809728622437, 0.1365227997303009], "s_value": 0.5165539979934692, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.0858902782201767, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.11396752297878265, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.17330175638198853, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0843469500541687, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19990424811840057, "mask": 0.0}, "badges": {"value": null, "prob": 0.3361351490020752, "mask": 0.0}, "terminate": {"prob": 0.006454132497310638, "mask": 0, "value": null}}, "cls_probs": [0.4056474268436432, 0.43502020835876465, 0.1593322902917862], "s_value": 0.50592041015625, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.10695391148328781, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.1406441330909729, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10649671405553818, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.24877764284610748, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.387105256319046, "mask": 0.0}, "terminate": {"prob": 0.010022305883467197, "mask": 0, "value": null}}, "cls_probs": [0.4091436266899109, 0.42998507618904114, 0.16087135672569275], "s_value": 0.504237711429596, "orig_id": 4352, "true_y": 1, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.10804852098226547, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.1428995430469513, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.11001424491405487, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.12245429307222366, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.12242110073566437, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12833216786384583, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.12072879821062088, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.13093210756778717, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.1241922676563263, "mask": 0.0}, "tags": {"value": null, "prob": 0.12729480862617493, "mask": 0.0}, "comments": {"value": null, "prob": 0.12364447861909866, "mask": 0.0}}], "prob": 0.22400601208209991, "mask": 0.0}, "badges": {"value": null, "prob": 0.4026758670806885, "mask": 0.0}, "terminate": {"prob": 0.012355852872133255, "mask": 0, "value": null}}, "cls_probs": [0.41172856092453003, 0.4372275769710541, 0.15104390680789948], "s_value": 0.5057014226913452, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.12654732167720795, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13077126443386078, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.12224669754505157, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.12238257378339767, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12848223745822906, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.12044785171747208, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.1313539296388626, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12434497475624084, "mask": 0.0}, "tags": {"value": null, "prob": 0.1275373250246048, "mask": 0.0}, "comments": {"value": null, "prob": 0.12320448458194733, "mask": 0.0, "selected": true}}], "prob": 0.24483537673950195, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.48049622774124146, "mask": 0.0}, "terminate": {"prob": 0.01734982244670391, "mask": 0, "value": null}}, "cls_probs": [0.4194040596485138, 0.4372069239616394, 0.1433890163898468], "s_value": 0.5020648837089539, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.12923842668533325, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16074667870998383, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.11984878778457642, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.1210048645734787, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12515370547771454, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.12056903541088104, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.13521815836429596, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12967999279499054, "mask": 0.0}, "tags": {"value": null, "prob": 0.14000582695007324, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.2369673103094101, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.2630326747894287, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.2369673103094101, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.2630326747894287, "mask": 0.0}}], "prob": 0.10851970314979553, "mask": 0.0}}], "prob": 0.2516704797744751, "mask": 0.0}, "badges": {"value": null, "prob": 0.4386177659034729, "mask": 0.0}, "terminate": {"prob": 0.019726667553186417, "mask": 0, "value": null}}, "cls_probs": [0.4420294165611267, 0.4267400801181793, 0.13123048841953278], "s_value": 0.5078275203704834, "orig_id": 4352, "true_y": 1, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.14439788460731506, "mask": 0.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.11960800737142563, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.12084042280912399, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12565751373767853, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.12006615102291107, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.13589562475681305, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12948624789714813, "mask": 0.0}, "tags": {"value": null, "prob": 0.13960547745227814, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.23744212090969086, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.26255789399147034, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.23744212090969086, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.26255789399147034, "mask": 0.0}}], "prob": 0.10884059965610504, "mask": 0.0}}], "prob": 0.2863386869430542, "mask": 0.0}, "badges": {"value": null, "prob": 0.5488038063049316, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02045956626534462, "mask": 0, "value": null}}, "cls_probs": [0.4493345022201538, 0.43266040086746216, 0.11800505965948105], "s_value": 0.5133525729179382, "orig_id": 4352, "true_y": 1, "last_cost": 1.0, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.1366354078054428, "mask": 0.0, "selected": true}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.11924117058515549, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.12067572772502899, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12624289095401764, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.11936812102794647, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.1367284655570984, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12933775782585144, "mask": 0.0}, "tags": {"value": null, "prob": 0.1391899734735489, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.23813112080097198, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.2618688642978668, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.23813112080097198, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.2618688642978668, "mask": 0.0}}], "prob": 0.10921593755483627, "mask": 0.0}}], "prob": 0.24975113570690155, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.5929797291755676, "mask": 0.0}, "terminate": {"prob": 0.020633762702345848, "mask": 0, "value": null}}, "cls_probs": [0.4367673397064209, 0.45088616013526917, 0.11234645545482635], "s_value": 0.5110219717025757, "orig_id": 4352, "true_y": 1, "last_cost": 1.0, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.11894240230321884, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.12046484649181366, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12597525119781494, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.11949364095926285, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.13680891692638397, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.12969531118869781, "mask": 0.0}, "tags": {"value": null, "prob": 0.14049983024597168, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.23726274073123932, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.2627372145652771, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.23726274073123932, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.2627372145652771, "mask": 0.0}}], "prob": 0.10811980068683624, "mask": 0.0}}], "prob": 0.2972980737686157, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0, "selected": true}}], "prob": 0.6730456352233887, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.029656264930963516, "mask": 0, "value": null}}, "cls_probs": [0.4314478635787964, 0.4540259540081024, 0.1145261749625206], "s_value": 0.5094172954559326, "orig_id": 4352, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.5}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.11832599341869354, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.1200283020734787, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12598450481891632, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.11913344264030457, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.1371893733739853, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.1295616179704666, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.1419452279806137, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.2373080849647522, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.2626919150352478, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.2373080849647522, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.2626919150352478, "mask": 0.0}}], "prob": 0.10783155262470245, "mask": 0.0}}], "prob": 0.29739513993263245, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.6730859279632568, "mask": 0.25}, "terminate": {"prob": 0.029518935829401016, "mask": 0, "value": null}}, "cls_probs": [0.4149700403213501, 0.4741595685482025, 0.11087050288915634], "s_value": 0.510992705821991, "orig_id": 4352, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.600000001490116}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.13642320036888123, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.13824452459812164, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.14414860308170319, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.1376255601644516, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.15716151893138885, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.16352912783622742, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.23621970415115356, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.2637803256511688, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.23621970415115356, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.2637803256511688, "mask": 0.0}}], "prob": 0.12286743521690369, "mask": 0.0}}], "prob": 0.3124777376651764, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.6588053703308105, "mask": 0.25, "selected": true}, "terminate": {"prob": 0.028716938570141792, "mask": 0, "value": null}}, "cls_probs": [0.4223673641681671, 0.462651789188385, 0.11498083174228668], "s_value": 0.5172584056854248, "orig_id": 4352, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.700000002980232}, {"sample": {"about_me": {"value": "<p>I'm a Software Engineer based in London, contracting at various places. My interests and competencies include: Functional programming, Information Retrieval, Machine Learning, NLP, and Agent-based Simulation. Technologies I like, or know a bit about include : Haskell, Scala, Java, Ruby, R, Postgresql, Redis, Akka, Erlang, Hadoop, Mahout.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.04, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does the confidence interval for the parameter p of a bernoulli trial vary with the value of p itself?", "prob": 0.1363871544599533, "mask": 0.0}, "body": {"value": "<p>This is a very basic question (I'm currently studying undergrad level statistics), but I was hoping for some clarification regarding an assertion I read in <a href=\"http://www.guardian.co.uk/lifeandstyle/2013/mar/10/alice-roberts-on-science-childbirth-risks\" rel=\"nofollow\">a newspaper article</a> earlier today. The author asserts that</p>\n\n<blockquote>\n  <p>evidence about the risks (of childbirth) has been hard to come by and difficult to interpret. This is partly because the overall risks of maternal and neonatal death are now very small (about five per 100,000 women die in childbirth and four per 1,000 babies), so large numbers of mums are needed to assess relative risks.</p>\n</blockquote>\n\n<p>Now, intuitively, this seems uncontroversial - if an event occurs rarely, then you'd expect more trials to be needed to achieve a good estimate of the likelihood of the event, when compared to one that occurs more frequently.</p>\n\n<p>However, I'm having trouble seeing how the mathematics bares this out. If we take the example in the article, we can model the probability of the death of a mother during childbirth as a Bernoulli trial with an estimate of the parameter $p$ given by $\\\\hat{p}=\\\\frac{5}{1000}$.</p>\n\n<p>From this, we can construct a 95% confidence interval for the true value of p:</p>\n\n<p>$$ p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac{p(1-p)}{\\\\sqrt{n}} $$</p>\n\n<p>However, if we take the limit as $p \\\\to 0$ we get</p>\n\n<p>$$ \\\\begin{aligned}\n\\\\lim_{p \\\\to 0} p^{\\\\pm} = \\\\hat{p} \\\\pm 1.96 \\\\frac {0(1-0)}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 1.96 \\\\frac {0}{\\\\sqrt{n}}\\\\\\\\\n = \\\\hat{p} \\\\pm 0\n\\\\end{aligned} $$</p>\n\n<p>Therefore, it appears that our estimate of $p$ in fact becomes <em>more</em> accurate as $p$ gets smaller, regardless of our value of $n$. This seems pretty counterintuituve to me - am I going wrong somewhere, and if so, where?</p>\n\n<p>Many thanks,</p>\n\n<p>Tim</p>\n", "prob": 0.1382243037223816, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.14406095445156097, "mask": 0.0}, "views": {"value": 0.0239, "prob": 0.13767428696155548, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.1571289747953415, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.16382746398448944, "mask": 0.0}, "comments": {"value": [{"score": {"value": 0.2, "prob": 0.23604775965213776, "mask": 0.0}, "text": {"value": "One issue - not the only one - is you don't have the true value of p to use for your estimate of variance in constructing the confidence interval (as per your formula), only an estimate.  When p gets very close to 0 or 1 this starts to be a real issue.", "prob": 0.26395222544670105, "mask": 0.0}}, {"score": {"value": 0.0, "prob": 0.23604775965213776, "mask": 0.0}, "text": {"value": "Right, this is fundamental to where I went wrong, thank you! I also completely overlooked the fact that the formula above provides only a normal approximation for the CI, and not an exact CI. Thank you!", "prob": 0.26395222544670105, "mask": 0.0}}], "prob": 0.12269680202007294, "mask": 0.0}}], "prob": 0.3203432261943817, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.6492608189582825, "mask": 0.5}, "terminate": {"prob": 0.03039596602320671, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4257330298423767, 0.4599578380584717, 0.1143091470003128], "s_value": 0.5166809558868408, "orig_id": 4352, "true_y": 1, "last_cost": 0.0, "total_cost": 6.800000004470348}], [{"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.07510669529438019, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.10096142441034317, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1179569810628891, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.1502375602722168, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0744071826338768, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18036718666553497, "mask": 0.0}, "badges": {"value": null, "prob": 0.2942897379398346, "mask": 0.0}, "terminate": {"prob": 0.0066732075065374374, "mask": 0, "value": null}}, "cls_probs": [0.42407476902008057, 0.41744476556777954, 0.1584804654121399], "s_value": 0.5154073238372803, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.08547482639551163, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.11359433829784393, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1732178032398224, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.083945132791996, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19996650516986847, "mask": 0.0}, "badges": {"value": null, "prob": 0.33745989203453064, "mask": 0.0}, "terminate": {"prob": 0.006341572385281324, "mask": 0, "value": null}}, "cls_probs": [0.4062126874923706, 0.43472155928611755, 0.15906572341918945], "s_value": 0.5056849718093872, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.09046883136034012, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.1212293952703476, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18799151480197906, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.2229996919631958, "mask": 0.0}, "badges": {"value": null, "prob": 0.371232807636261, "mask": 0.0}, "terminate": {"prob": 0.006077748257666826, "mask": 0, "value": null}}, "cls_probs": [0.3996538519859314, 0.4378759264945984, 0.16247020661830902], "s_value": 0.5016763806343079, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.10395975410938263, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.21103867888450623, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.24498233199119568, "mask": 0.0}, "badges": {"value": null, "prob": 0.4320155680179596, "mask": 0.0}, "terminate": {"prob": 0.008003645576536655, "mask": 0, "value": null}}, "cls_probs": [0.4041654169559479, 0.4385525584220886, 0.15728197991847992], "s_value": 0.4958816170692444, "orig_id": 3542, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.13754786550998688, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.30453935265541077, "mask": 0.0}, "badges": {"value": null, "prob": 0.5444021821022034, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013510696589946747, "mask": 0, "value": null}}, "cls_probs": [0.4035683572292328, 0.44005075097084045, 0.15638086199760437], "s_value": 0.49476155638694763, "orig_id": 3542, "true_y": 0, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.11273663491010666, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.22783000767230988, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.6456023454666138, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.013831007294356823, "mask": 0, "value": null}}, "cls_probs": [0.417526513338089, 0.42529863119125366, 0.15717491507530212], "s_value": 0.49726152420043945, "orig_id": 3542, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.3013353645801544, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.650780975818634, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04788359999656677, "mask": 0, "value": null}}, "cls_probs": [0.4171193242073059, 0.42840710282325745, 0.15447357296943665], "s_value": 0.47902706265449524, "orig_id": 3542, "true_y": 0, "last_cost": 1.0, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.8827852606773376, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.11721472442150116, "mask": 0, "value": null}}, "cls_probs": [0.4246101379394531, 0.41949662566185, 0.1558932214975357], "s_value": 0.4773430824279785, "orig_id": 3542, "true_y": 0, "last_cost": 1.0, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>Designer, Developer, Mechanic.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.4162975251674652, 0.4291377663612366, 0.1545647531747818], "s_value": 0.47048118710517883, "orig_id": 3542, "true_y": 0, "last_cost": 0.0, "total_cost": 6.100000001490116}], [{"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4202, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.04287828505039215, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.052264608442783356, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05562107264995575, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.06584755331277847, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.03930831700563431, "mask": 0.0}, "website": {"value": 1, "prob": 0.5823312997817993, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.061085935682058334, "mask": 0.0}, "badges": {"value": null, "prob": 0.08656222373247147, "mask": 0.0}, "terminate": {"prob": 0.014100611209869385, "mask": 0, "value": null}}, "cls_probs": [0.46837085485458374, 0.4088307321071625, 0.12279842048883438], "s_value": 0.5302755832672119, "orig_id": 4202, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.0752011314034462, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.10104519128799438, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.11802148818969727, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.15026599168777466, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07448961585760117, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.18020692467689514, "mask": 0.0}, "badges": {"value": null, "prob": 0.29406601190567017, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.006703713443130255, "mask": 0, "value": null}}, "cls_probs": [0.42384371161460876, 0.4176537096500397, 0.15850257873535156], "s_value": 0.515460729598999, "orig_id": 4202, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.06883744150400162, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0953383520245552, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.10434933751821518, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.13797813653945923, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07002390176057816, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15122754871845245, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.36503490805625916, "mask": 0.0}, "terminate": {"prob": 0.00721035897731781, "mask": 0, "value": null}}, "cls_probs": [0.4317881762981415, 0.41195598244667053, 0.1562557965517044], "s_value": 0.5142475366592407, "orig_id": 4202, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.08033137023448944, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.11106254160404205, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.12563185393810272, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.1653202474117279, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.07926677912473679, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}], "prob": 0.4313759207725525, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.007011146284639835, "mask": 0, "value": null}}, "cls_probs": [0.4389917254447937, 0.4047461152076721, 0.1562621146440506], "s_value": 0.5100930333137512, "orig_id": 4202, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.08860734105110168, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.12008146941661835, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.13838519155979156, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.003, "prob": 0.17625917494297028, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0873534083366394, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.38037750124931335, "mask": 0.5}, "terminate": {"prob": 0.008935977704823017, "mask": 0, "value": null}}, "cls_probs": [0.44029852747917175, 0.4011085331439972, 0.15859299898147583], "s_value": 0.5067117214202881, "orig_id": 4202, "true_y": 0, "last_cost": 0.5, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>Full time Software Engineering student and Software Engineer.</p>\n", "prob": 0.12918531894683838, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.14783816039562225, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.15797613561153412, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.11788332462310791, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 1.0, "mask": 0.0}}], "prob": 0.2556939721107483, "mask": 0.5}, "terminate": {"prob": 0.19142314791679382, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4930911064147949, 0.33883070945739746, 0.16807815432548523], "s_value": 0.5017076730728149, "orig_id": 4202, "true_y": 0, "last_cost": 0.0, "total_cost": 3.600000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4553, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.032880865037441254, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.041435662657022476, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.033930495381355286, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.03720048442482948, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.04095682501792908, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.02832750976085663, "mask": 0.0}, "website": {"value": 0, "prob": 0.7165712714195251, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.036715902388095856, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03198100998997688, "mask": 0, "value": null}}, "cls_probs": [0.4731794595718384, 0.3826155662536621, 0.14420495927333832], "s_value": 0.5287606716156006, "orig_id": 4553, "true_y": 1, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.02578739821910858, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.02728184312582016, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.024909118190407753, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.02332187071442604, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.024190718308091164, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.022609204053878784, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.018328795209527016, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8335710167884827, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5494099259376526, 0.34747225046157837, 0.10311776399612427], "s_value": 0.5460960865020752, "orig_id": 4553, "true_y": 1, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>I am Learning Physics , though love to write short stories for kids.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8287, "true_y": 2, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am Learning Physics , though love to write short stories for kids.</p>\n", "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 0, "prob": 0.44816169142723083, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 8287, "true_y": 2, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am Learning Physics , though love to write short stories for kids.</p>\n", "prob": 0.06386525183916092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09098470956087112, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.07489465922117233, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06941630691289902, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.06202324852347374, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06582651287317276, "mask": 0.0}, "badges": {"value": null, "prob": 0.0724131315946579, "mask": 0.0}, "terminate": {"prob": 0.5005761384963989, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5411810278892517, 0.36297500133514404, 0.09584403783082962], "s_value": 0.550512969493866, "orig_id": 8287, "true_y": 2, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 4623, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.09110599011182785, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.16393209993839264, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11823386698961258, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.13953357934951782, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.17636369168758392, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08842766284942627, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.21292395889759064, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.00947917252779007, "mask": 0, "value": null}}, "cls_probs": [0.4253544807434082, 0.40841978788375854, 0.16622570157051086], "s_value": 0.5181680917739868, "orig_id": 4623, "true_y": 0, "last_cost": 1.0, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.11662387102842331, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.20535187423229218, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.14929400384426117, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.18153326213359833, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.22758403420448303, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10905490070581436, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.010558157227933407, "mask": 0, "value": null}}, "cls_probs": [0.4317892789840698, 0.3967249095439911, 0.17148582637310028], "s_value": 0.5166255235671997, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.1473585069179535, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.18970151245594025, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.22672253847122192, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.28301388025283813, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1384199559688568, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.014783652499318123, "mask": 0, "value": null}}, "cls_probs": [0.4359745383262634, 0.3964706361293793, 0.16755475103855133], "s_value": 0.5089271664619446, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": null, "prob": 0.17091910541057587, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.2188379168510437, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.262425035238266, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.33158063888549805, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01623733714222908, "mask": 0, "value": null}}, "cls_probs": [0.42581650614738464, 0.4045637249946594, 0.16961979866027832], "s_value": 0.5000385642051697, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.2154536247253418, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.33598223328590393, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.4280056655406952, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.020558485761284828, "mask": 0, "value": null}}, "cls_probs": [0.43205559253692627, 0.4112241864204407, 0.15672023594379425], "s_value": 0.4936330020427704, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 4.0}, {"sample": {"about_me": {"value": null, "prob": 0.37609899044036865, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.5806416869163513, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.04325927793979645, "mask": 0, "value": null}}, "cls_probs": [0.43933427333831787, 0.40522605180740356, 0.15543965995311737], "s_value": 0.49340489506721497, "orig_id": 4623, "true_y": 0, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": null, "prob": 0.9131743907928467, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.08682558685541153, "mask": 0, "value": null}}, "cls_probs": [0.4173296093940735, 0.427418977022171, 0.1552513837814331], "s_value": 0.48481789231300354, "orig_id": 4623, "true_y": 0, "last_cost": 1.0, "total_cost": 5.0}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.42412474751472473, 0.4269156754016876, 0.14895962178707123], "s_value": 0.4772082269191742, "orig_id": 4623, "true_y": 0, "last_cost": 0.0, "total_cost": 6.0}], [{"sample": {"about_me": {"value": "<p>I am undergrad final year student and very enthusiastic to learn new technologies and do something useful in life by helping someone or enriching my knowledge :)</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6495, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I am undergrad final year student and very enthusiastic to learn new technologies and do something useful in life by helping someone or enriching my knowledge :)</p>\n", "prob": 0.022596973925828934, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.021051395684480667, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.020138157531619072, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0185193233191967, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.018861593678593636, "mask": 0.0}, "website": {"value": 1, "prob": 0.45873865485191345, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.013075222261250019, "mask": 0.0}, "badges": {"value": null, "prob": 0.011445035226643085, "mask": 0.0}, "terminate": {"prob": 0.41557368636131287, "mask": 0, "value": null}}, "cls_probs": [0.535433292388916, 0.32578715682029724, 0.13877953588962555], "s_value": 0.5434094071388245, "orig_id": 6495, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I am undergrad final year student and very enthusiastic to learn new technologies and do something useful in life by helping someone or enriching my knowledge :)</p>\n", "prob": 0.10069730132818222, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13613314926624298, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.11296160519123077, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1270637810230255, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09370279312133789, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.12692825496196747, "mask": 0.0}, "badges": {"value": null, "prob": 0.14839661121368408, "mask": 0.0}, "terminate": {"prob": 0.15411652624607086, "mask": 0, "value": null}}, "cls_probs": [0.4678768515586853, 0.36892351508140564, 0.16319963335990906], "s_value": 0.5223966836929321, "orig_id": 6495, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I am undergrad final year student and very enthusiastic to learn new technologies and do something useful in life by helping someone or enriching my knowledge :)</p>\n", "prob": 0.10963071882724762, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.15020091831684113, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.12378083169460297, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1415734887123108, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1379205733537674, "mask": 0.0}, "badges": {"value": null, "prob": 0.17704501748085022, "mask": 0.0}, "terminate": {"prob": 0.15984852612018585, "mask": 0, "value": null}}, "cls_probs": [0.4617198407649994, 0.37243032455444336, 0.16584987938404083], "s_value": 0.5169459581375122, "orig_id": 6495, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>I am undergrad final year student and very enthusiastic to learn new technologies and do something useful in life by helping someone or enriching my knowledge :)</p>\n", "prob": 0.12530702352523804, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.14490360021591187, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.16150610148906708, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15366727113723755, "mask": 0.0}, "badges": {"value": null, "prob": 0.23294487595558167, "mask": 0.0}, "terminate": {"prob": 0.18167105317115784, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.4782484471797943, 0.3676723539829254, 0.15407918393611908], "s_value": 0.513482928276062, "orig_id": 6495, "true_y": 0, "last_cost": 0.0, "total_cost": 2.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8311, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 8311, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.02578739821910858, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02728184312582016, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.024909118190407753, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.02332187071442604, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.024190718308091164, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.022609204053878784, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.018328795209527016, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8335710167884827, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5494099259376526, 0.34747225046157837, 0.10311776399612427], "s_value": 0.5460960865020752, "orig_id": 8311, "true_y": 1, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>(your about me is currently blank)</p>\n\n<p><a href=\"http://stackoverflow.com/users/edit/584490\">Click here to edit</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6153, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>(your about me is currently blank)</p>\n\n<p><a href=\"http://stackoverflow.com/users/edit/584490\">Click here to edit</a></p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.06626950204372406, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 6153, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>(your about me is currently blank)</p>\n\n<p><a href=\"http://stackoverflow.com/users/edit/584490\">Click here to edit</a></p>\n", "prob": 0.06379367411136627, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.09080636501312256, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.07477690279483795, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06930391490459442, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.06193525716662407, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06568548083305359, "mask": 0.0}, "badges": {"value": null, "prob": 0.07221676409244537, "mask": 0.0}, "terminate": {"prob": 0.5014816522598267, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.541286051273346, 0.36283519864082336, 0.09587876498699188], "s_value": 0.5505232214927673, "orig_id": 6153, "true_y": 0, "last_cost": 0.0, "total_cost": 1.0}], [{"sample": {"about_me": {"value": "<p>Nothing to see here. Move along.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0021, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3689, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Nothing to see here. Move along.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.06, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0021, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.006, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 3689, "true_y": 1, "last_cost": 0.0, "total_cost": 0.5}], [{"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.03980446234345436, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8185, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.07373175770044327, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.05531802400946617, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.05981825292110443, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06878373771905899, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.043399378657341, "mask": 0.0}, "website": {"value": 1, "prob": 0.5353192090988159, "mask": 0.0}, "posts": {"value": null, "prob": 0.06455256789922714, "mask": 0.0}, "badges": {"value": null, "prob": 0.07940848916769028, "mask": 0.0}, "terminate": {"prob": 0.019668545573949814, "mask": 0, "value": null}}, "cls_probs": [0.45297154784202576, 0.4130527675151825, 0.13397572934627533], "s_value": 0.5273534059524536, "orig_id": 8185, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.06146078184247017, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0645415335893631, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07458800077438354, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04753806069493294, "mask": 0.0}, "website": {"value": 1, "prob": 0.5651422739028931, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.06904315948486328, "mask": 0.0}, "badges": {"value": null, "prob": 0.09448256343603134, "mask": 0.0}, "terminate": {"prob": 0.023203635588288307, "mask": 0, "value": null}}, "cls_probs": [0.4635659456253052, 0.41264691948890686, 0.12378709018230438], "s_value": 0.5241985321044922, "orig_id": 8185, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.11455979943275452, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.13060912489891052, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1612599939107895, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0878525897860527, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.19780036807060242, "mask": 0.0}, "badges": {"value": null, "prob": 0.297294557094574, "mask": 0.0}, "terminate": {"prob": 0.010623686015605927, "mask": 0, "value": null}}, "cls_probs": [0.4226987957954407, 0.41846585273742676, 0.1588352918624878], "s_value": 0.5061417818069458, "orig_id": 8185, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.13894379138946533, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.15752525627613068, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.10923866182565689, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.24530495703220367, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.3323565125465393, "mask": 0.0}, "terminate": {"prob": 0.016630802303552628, "mask": 0, "value": null}}, "cls_probs": [0.42845115065574646, 0.4101106822490692, 0.16143816709518433], "s_value": 0.5052333474159241, "orig_id": 8185, "true_y": 0, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.18122805655002594, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.21124622225761414, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1386365294456482, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.4513624310493469, "mask": 0.0}, "terminate": {"prob": 0.017526676878333092, "mask": 0, "value": null}}, "cls_probs": [0.4348703920841217, 0.40262699127197266, 0.16250267624855042], "s_value": 0.5030721426010132, "orig_id": 8185, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.25182563066482544, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16921894252300262, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.5545477867126465, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.02440761961042881, "mask": 0, "value": null}}, "cls_probs": [0.4381309151649475, 0.40647101402282715, 0.15539805591106415], "s_value": 0.4955095052719116, "orig_id": 8185, "true_y": 0, "last_cost": 1.0, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.19822654128074646, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1429896354675293, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}], "prob": 0.6343165040016174, "mask": 0.0}, "terminate": {"prob": 0.024467280134558678, "mask": 0, "value": null}}, "cls_probs": [0.4471518397331238, 0.39873161911964417, 0.15411651134490967], "s_value": 0.4954717457294464, "orig_id": 8185, "true_y": 0, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.17677998542785645, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.4905080497264862, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.3327118754386902, "mask": 0, "value": null}}, "cls_probs": [0.5069328546524048, 0.3498696982860565, 0.1431974470615387], "s_value": 0.49478769302368164, "orig_id": 8185, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>Currently i m final year student of B.E in Information Technology,ahmadabad ,Gujarat</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.23157241940498352, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.7684275507926941, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5013946294784546, 0.3352084457874298, 0.1633969247341156], "s_value": 0.48935556411743164, "orig_id": 8185, "true_y": 0, "last_cost": 0.0, "total_cost": 5.600000001490116}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.08, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0127, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3937, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.0451396144926548, "mask": 0.0}, "views": {"value": 0.08, "prob": 0.0725313052535057, "mask": 0.0}, "reputation": {"value": 0.0127, "prob": 0.05382183939218521, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.05918215960264206, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.07011939585208893, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.531177282333374, "mask": 0.0}, "posts": {"value": null, "prob": 0.06768590956926346, "mask": 0.0}, "badges": {"value": null, "prob": 0.08940514922142029, "mask": 0.0}, "terminate": {"prob": 0.010937291197478771, "mask": 0, "value": null}}, "cls_probs": [0.45435237884521484, 0.41012728214263916, 0.1355203092098236], "s_value": 0.5306205749511719, "orig_id": 3937, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.055070120841264725, "mask": 0.0}, "views": {"value": 0.08, "prob": 0.09186124056577682, "mask": 0.0}, "reputation": {"value": 0.0127, "prob": 0.06625081598758698, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.09258115291595459, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.4754077196121216, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08930721133947372, "mask": 0.0}, "badges": {"value": null, "prob": 0.12027840316295624, "mask": 0.0}, "terminate": {"prob": 0.009243430569767952, "mask": 0, "value": null}}, "cls_probs": [0.4318924844264984, 0.42378729581832886, 0.14432023465633392], "s_value": 0.5170695185661316, "orig_id": 3937, "true_y": 0, "last_cost": 0.5, "total_cost": 1.0}, {"sample": {"about_me": {"value": null, "prob": 0.10271274298429489, "mask": 0.0}, "views": {"value": 0.08, "prob": 0.1555345505475998, "mask": 0.0}, "reputation": {"value": 0.0127, "prob": 0.1217493936419487, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.1413109004497528, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1208910420536995, "mask": 0.0}, "badges": {"value": null, "prob": 0.1542891561985016, "mask": 0.0}, "terminate": {"prob": 0.20351208746433258, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.502734363079071, 0.3884009122848511, 0.10886475443840027], "s_value": 0.532902717590332, "orig_id": 3937, "true_y": 0, "last_cost": 0.0, "total_cost": 1.5}], [{"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4790, "true_y": 0, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08534761518239975, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 1, "prob": 0.3274863362312317, "mask": 0.0}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.15204817056655884, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 4790, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.05282078683376312, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.08604662120342255, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06437923014163971, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.06764469295740128, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08063102513551712, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04955529049038887, "mask": 0.0}, "website": {"value": 1, "prob": 0.38537564873695374, "mask": 0.0}, "posts": {"value": null, "prob": 0.07169362157583237, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0}}], "prob": 0.12192042171955109, "mask": 0.3333333432674408}, "terminate": {"prob": 0.01993275061249733, "mask": 0, "value": null}}, "cls_probs": [0.46622565388679504, 0.39057254791259766, 0.1432017982006073], "s_value": 0.5240697264671326, "orig_id": 4790, "true_y": 0, "last_cost": 0.5, "total_cost": 1.1000000014901161}, {"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.05798017606139183, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.10025986284017563, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.07506036013364792, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.09017065912485123, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05640796944499016, "mask": 0.0}, "website": {"value": 1, "prob": 0.3648991286754608, "mask": 0.0}, "posts": {"value": null, "prob": 0.07996458560228348, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0}}], "prob": 0.15258905291557312, "mask": 0.3333333432674408}, "terminate": {"prob": 0.022668125107884407, "mask": 0, "value": null}}, "cls_probs": [0.46910184621810913, 0.4009980261325836, 0.12990008294582367], "s_value": 0.5220236778259277, "orig_id": 4790, "true_y": 0, "last_cost": 0.5, "total_cost": 1.6000000014901161}, {"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.06531824916601181, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0837957039475441, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.10130230337381363, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0642508789896965, "mask": 0.0}, "website": {"value": 1, "prob": 0.3769941031932831, "mask": 0.0}, "posts": {"value": null, "prob": 0.08877157419919968, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0}}], "prob": 0.1912529319524765, "mask": 0.3333333432674408}, "terminate": {"prob": 0.028314264491200447, "mask": 0, "value": null}}, "cls_probs": [0.48527348041534424, 0.3935430943965912, 0.12118342518806458], "s_value": 0.521056592464447, "orig_id": 4790, "true_y": 0, "last_cost": 0.5, "total_cost": 2.100000001490116}, {"sample": {"about_me": {"value": "<p>I'm an Electrical Engineering and Computer Sciences major at UC Berkeley, planning on graduating in December 2015.</p>\n", "prob": 0.05445852130651474, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.05293472856283188, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.04851582273840904, "mask": 0.0}, "website": {"value": 1, "prob": 0.3306554853916168, "mask": 0.0}, "posts": {"value": null, "prob": 0.034213338047266006, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.5, "mask": 0.0}}], "prob": 0.07284259051084518, "mask": 0.3333333432674408}, "terminate": {"prob": 0.40637949109077454, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5607025623321533, 0.3192962408065796, 0.1200011596083641], "s_value": 0.5329301357269287, "orig_id": 4790, "true_y": 0, "last_cost": 0.0, "total_cost": 2.600000001490116}], [{"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0118, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 1717, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0118, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 1717, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.06710157543420792, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12682220339775085, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0118, "prob": 0.09017068147659302, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1035819873213768, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13106797635555267, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06801001727581024, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.12269587814807892, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.12239636480808258, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.1283058524131775, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.12088969349861145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13058021664619446, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12396494299173355, "mask": 0.0}, "tags": {"value": null, "prob": 0.12712417542934418, "mask": 0.0}, "comments": {"value": null, "prob": 0.12404289841651917, "mask": 0.0}}], "prob": 0.1497763991355896, "mask": 0.0}, "badges": {"value": null, "prob": 0.2569315433502197, "mask": 0.0}, "terminate": {"prob": 0.006537663750350475, "mask": 0, "value": null}}, "cls_probs": [0.4156748354434967, 0.4261636435985565, 0.15816153585910797], "s_value": 0.5208643674850464, "orig_id": 1717, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.07722676545381546, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.10399207472801208, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11775162070989609, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.14723791182041168, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0786159336566925, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.12270506471395493, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.1224868893623352, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.12844064831733704, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.12077834457159042, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13088631629943848, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12414587289094925, "mask": 0.0}, "tags": {"value": null, "prob": 0.1268196403980255, "mask": 0.0}, "comments": {"value": null, "prob": 0.12373721599578857, "mask": 0.0}}], "prob": 0.16465133428573608, "mask": 0.0}, "badges": {"value": null, "prob": 0.3017246127128601, "mask": 0.0}, "terminate": {"prob": 0.008799724280834198, "mask": 0, "value": null}}, "cls_probs": [0.42882460355758667, 0.4195379912853241, 0.15163737535476685], "s_value": 0.5172091126441956, "orig_id": 1717, "true_y": 0, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.08705303072929382, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.11614785343408585, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.16924552619457245, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08807813376188278, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.1226789578795433, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.12247750908136368, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.1282905489206314, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.12092142552137375, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13085955381393433, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12415400892496109, "mask": 0.0}, "tags": {"value": null, "prob": 0.12693437933921814, "mask": 0.0}, "comments": {"value": null, "prob": 0.12368368357419968, "mask": 0.0}}], "prob": 0.1895071566104889, "mask": 0.0}, "badges": {"value": null, "prob": 0.34196043014526367, "mask": 0.0}, "terminate": {"prob": 0.008007945492863655, "mask": 0, "value": null}}, "cls_probs": [0.41294580698013306, 0.43335026502609253, 0.153703972697258], "s_value": 0.5079635381698608, "orig_id": 1717, "true_y": 0, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.09939911961555481, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18771010637283325, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.1021045446395874, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.12246891856193542, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.12243863940238953, "mask": 0.0, "selected": true}, "score": {"value": 0.03, "prob": 0.12844190001487732, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.12063493579626083, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.13128629326820374, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.1243060827255249, "mask": 0.0}, "tags": {"value": null, "prob": 0.12717244029045105, "mask": 0.0}, "comments": {"value": null, "prob": 0.12325068563222885, "mask": 0.0}}], "prob": 0.20193834602832794, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.3978813886642456, "mask": 0.0}, "terminate": {"prob": 0.010966441594064236, "mask": 0, "value": null}}, "cls_probs": [0.42042094469070435, 0.43374988436698914, 0.1458292156457901], "s_value": 0.5042790174484253, "orig_id": 1717, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.10804351419210434, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18560053408145905, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.11134783178567886, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.13913844525814056, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.03, "prob": 0.14622217416763306, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.13752610981464386, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.1497807651758194, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14200299978256226, "mask": 0.0}, "tags": {"value": null, "prob": 0.14694713056087494, "mask": 0.0}, "comments": {"value": null, "prob": 0.13838230073451996, "mask": 0.0}}], "prob": 0.20091870427131653, "mask": 0.125}, "badges": {"value": null, "prob": 0.3784082531929016, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.015681227669119835, "mask": 0, "value": null}}, "cls_probs": [0.4210825264453888, 0.4198906123638153, 0.1590268611907959], "s_value": 0.4999528229236603, "orig_id": 1717, "true_y": 0, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.0975959450006485, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.1681634932756424, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.10368573665618896, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.1383827030658722, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.03, "prob": 0.1470017284154892, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.1366885006427765, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.15097589790821075, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.1418096274137497, "mask": 0.0}, "tags": {"value": null, "prob": 0.14732897281646729, "mask": 0.0}, "comments": {"value": null, "prob": 0.1378125548362732, "mask": 0.0}}], "prob": 0.17354044318199158, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Announcer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}], "prob": 0.4408944547176361, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.016119839623570442, "mask": 0, "value": null}}, "cls_probs": [0.42510586977005005, 0.4201686382293701, 0.15472549200057983], "s_value": 0.5026406645774841, "orig_id": 1717, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\n\n<hr>\n\n<ul>\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\n</ul>\n", "prob": 0.0727318599820137, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0118, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.06788722425699234, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06874015927314758, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "What do you call a chart that looks like a half pie chart with a needle indicating a percentage?", "prob": 0.14255349338054657, "mask": 0.0}, "body": {"value": "<p>Is there a name for this data visualization?</p>\n\n<p><img src=\"http://i.stack.imgur.com/GIM3W.jpg\" alt=\"\"></p>\n\n<hr>\n\n<p>It's almost like half a pie chart. In this case, it was animated like a speedometer you would see on a car dashboard; the divider (the needle) pointed left fully horizontal at first.</p>\n", "prob": 0.0, "mask": 1.0}, "score": {"value": 0.03, "prob": 0.14672870934009552, "mask": 0.0}, "views": {"value": 0.0583, "prob": 0.1385270655155182, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.14836791157722473, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.14425161480903625, "mask": 0.0}, "tags": {"value": null, "prob": 0.13743554055690765, "mask": 0.0}, "comments": {"value": null, "prob": 0.14213569462299347, "mask": 0.0}}], "prob": 0.035184942185878754, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Announcer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.11254970729351044, "mask": 0.25}, "terminate": {"prob": 0.642906129360199, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5733751654624939, 0.3338681161403656, 0.09275670349597931], "s_value": 0.5444157719612122, "orig_id": 1717, "true_y": 0, "last_cost": 0.0, "total_cost": 4.600000001490116}], [{"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0665, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.062, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 3130, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0665, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.062, "prob": 0.08534760773181915, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 1, "prob": 0.3274863362312317, "mask": 0.0}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.06666667014360428, "mask": 0.0}}], "prob": 0.15204817056655884, "mask": 0.0}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 3130, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.056121304631233215, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.10122435539960861, "mask": 0.0}, "reputation": {"value": 0.0665, "prob": 0.0740346685051918, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.07291989773511887, "mask": 0.0}, "up_votes": {"value": 0.062, "prob": 0.08588903397321701, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.058149777352809906, "mask": 0.0}, "website": {"value": 1, "prob": 0.28025129437446594, "mask": 0.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030195291619747877, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0030340575613081455, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003121395595371723, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002938191406428814, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031681195832788944, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003036431036889553, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030034303199499846, "mask": 0.0}, "comments": {"value": null, "prob": 0.003069087862968445, "mask": 0.0}}], "prob": 0.07027135789394379, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.06666667014360428, "mask": 0.0}}], "prob": 0.1772807538509369, "mask": 0.0}, "terminate": {"prob": 0.02385759726166725, "mask": 0, "value": null}}, "cls_probs": [0.4914620816707611, 0.3903696537017822, 0.11816830188035965], "s_value": 0.5374606847763062, "orig_id": 3130, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.06033506244421005, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.11098844558000565, "mask": 0.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.07750275731086731, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.062, "prob": 0.09153185039758682, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.06372997909784317, "mask": 0.0}, "website": {"value": 1, "prob": 0.29185551404953003, "mask": 0.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030124897602945566, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.003030948806554079, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031282752752304077, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002928362926468253, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003183632856234908, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039928851649165, "mask": 0.0}, "tags": {"value": null, "prob": 0.0030098548159003258, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030567492358386517, "mask": 0.0}}], "prob": 0.07245000451803207, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.06666667014360428, "mask": 0.0}}], "prob": 0.20225314795970917, "mask": 0.0}, "terminate": {"prob": 0.02935326099395752, "mask": 0, "value": null}}, "cls_probs": [0.4980376064777374, 0.3939153850078583, 0.10804694145917892], "s_value": 0.5385057926177979, "orig_id": 3130, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.06638019531965256, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.12536846101284027, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.10754742473363876, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.07013631612062454, "mask": 0.0}, "website": {"value": 1, "prob": 0.28932926058769226, "mask": 0.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0030053139198571444, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0030220411717891693, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031313018407672644, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029253107495605946, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031939877662807703, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003039534902200103, "mask": 0.0}, "tags": {"value": null, "prob": 0.003026279853656888, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030464716255664825, "mask": 0.0}}], "prob": 0.08837101608514786, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Curious", "prob": 0.06666667014360428, "mask": 0.0}}], "prob": 0.23271316289901733, "mask": 0.0}, "terminate": {"prob": 0.020154200494289398, "mask": 0, "value": null}}, "cls_probs": [0.4715597331523895, 0.41835689544677734, 0.11008336395025253], "s_value": 0.5269951820373535, "orig_id": 3130, "true_y": 1, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.08667327463626862, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.13107746839523315, "mask": 0.0}, "down_votes": {"value": 0.01, "prob": 0.09134256839752197, "mask": 0.0}, "website": {"value": 1, "prob": 0.2477644830942154, "mask": 0.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002991690067574382, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.00301219685934484, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322427093982697, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927746856585145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0031984043307602406, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030395090579986572, "mask": 0.0}, "tags": {"value": null, "prob": 0.003060644492506981, "mask": 0.0}, "comments": {"value": null, "prob": 0.0030278076883405447, "mask": 0.0}}], "prob": 0.10943819582462311, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.06666667014360428, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.06666667014360428, "mask": 0.0, "selected": true}}, {"badge": {"value": "Curious", "prob": 0.06666667014360428, "mask": 0.0}}], "prob": 0.30039867758750916, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.033305369317531586, "mask": 0, "value": null}}, "cls_probs": [0.4434567093849182, 0.4373832046985626, 0.11916010826826096], "s_value": 0.5240662097930908, "orig_id": 3130, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.08768535405397415, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.13113141059875488, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.01, "prob": 0.09227382391691208, "mask": 0.0}, "website": {"value": 1, "prob": 0.25373002886772156, "mask": 0.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029914132319390774, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.003012255299836397, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031322252470999956, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002927047200500965, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003198175923898816, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003040323266759515, "mask": 0.0}, "tags": {"value": null, "prob": 0.003062016097828746, "mask": 0.0}, "comments": {"value": null, "prob": 0.00302678975276649, "mask": 0.0}}], "prob": 0.10931991040706635, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.2903406620025635, "mask": 0.06666667014360428}, "terminate": {"prob": 0.03551892191171646, "mask": 0, "value": null}}, "cls_probs": [0.4431767761707306, 0.4382290542125702, 0.11859409511089325], "s_value": 0.5223397016525269, "orig_id": 3130, "true_y": 1, "last_cost": 0.5, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.10662338137626648, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.11391492187976837, "mask": 0.0}, "website": {"value": 1, "prob": 0.228597953915596, "mask": 0.0, "selected": true}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002982637844979763, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0030119146686047316, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031314415391534567, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002921797800809145, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032003738451749086, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030388482846319675, "mask": 0.0}, "tags": {"value": null, "prob": 0.003075535176321864, "mask": 0.0}, "comments": {"value": null, "prob": 0.003027699887752533, "mask": 0.0}}], "prob": 0.12330634891986847, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.3696049153804779, "mask": 0.06666667014360428}, "terminate": {"prob": 0.05795251950621605, "mask": 0, "value": null}}, "cls_probs": [0.4525086581707001, 0.4307645857334137, 0.11672677099704742], "s_value": 0.5205856561660767, "orig_id": 3130, "true_y": 1, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.12413925677537918, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.13321243226528168, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029533240012824535, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0029689178336411715, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031407917849719524, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925632754340768, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032247628550976515, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030328792054206133, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031687715090811253, "mask": 0.0}, "comments": {"value": null, "prob": 0.002975163282826543, "mask": 0.0}}], "prob": 0.1885862946510315, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0, "selected": true}}, {"badge": {"value": "Tumbleweed", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0714285746216774, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.0714285746216774, "mask": 0.0}}], "prob": 0.521600067615509, "mask": 0.06666667014360428, "selected": true}, "terminate": {"prob": 0.032461978495121, "mask": 0, "value": null}}, "cls_probs": [0.3940887451171875, 0.45190590620040894, 0.15400534868240356], "s_value": 0.5100483298301697, "orig_id": 3130, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.1274886131286621, "mask": 0.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.13716088235378265, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002950528636574745, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.002966939937323332, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031394432298839092, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002924596192315221, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003225318156182766, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003034190973266959, "mask": 0.0}, "tags": {"value": null, "prob": 0.0031795192044228315, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029697066638618708, "mask": 0.0}}], "prob": 0.1909037083387375, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.07692307978868484, "mask": 0.0, "selected": true}}, {"badge": {"value": "Commentator", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.07692307978868484, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.07692307978868484, "mask": 0.0}}], "prob": 0.5086320638656616, "mask": 0.13333334028720856, "selected": true}, "terminate": {"prob": 0.0358147993683815, "mask": 0, "value": null}}, "cls_probs": [0.39256104826927185, 0.45294633507728577, 0.15449258685112], "s_value": 0.5076814889907837, "orig_id": 3130, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.13169436156749725, "mask": 0.0, "selected": true}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.14409169554710388, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029353208374232054, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0029524508863687515, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003132718848064542, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002925160573795438, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231545677408576, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003038872266188264, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032409874256700277, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029331897385418415, "mask": 0.0}}], "prob": 0.19396646320819855, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.0833333358168602, "mask": 0.0}}], "prob": 0.4918917715549469, "mask": 0.20000000298023224}, "terminate": {"prob": 0.038355752825737, "mask": 0, "value": null}}, "cls_probs": [0.3763512969017029, 0.4562731683254242, 0.16737550497055054], "s_value": 0.5027948021888733, "orig_id": 3130, "true_y": 1, "last_cost": 1.0, "total_cost": 4.800000004470348}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.18567423522472382, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029259209986776114, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.00294776470400393, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003130869707092643, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029174350202083588, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0032396770548075438, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030479251872748137, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032671403605490923, "mask": 0.0}, "comments": {"value": null, "prob": 0.002913511823862791, "mask": 0.0}}], "prob": 0.23122163116931915, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0833333358168602, "mask": 0.0, "selected": true}}, {"badge": {"value": "Editor", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0833333358168602, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.0833333358168602, "mask": 0.0}}], "prob": 0.5161744356155396, "mask": 0.20000000298023224, "selected": true}, "terminate": {"prob": 0.0669296458363533, "mask": 0, "value": null}}, "cls_probs": [0.36754748225212097, 0.4711134135723114, 0.16133899986743927], "s_value": 0.49561598896980286, "orig_id": 3130, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.800000004470348}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.19316598773002625, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}, {"title": {"value": null, "prob": 0.0029409867711365223, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.0029675776604562998, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0031366506591439247, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.002914685057476163, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003231335198506713, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.003050015075132251, "mask": 0.0}, "tags": {"value": null, "prob": 0.003204406937584281, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029445881955325603, "mask": 0.0}}], "prob": 0.2088799625635147, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.09090909361839294, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.09090909361839294, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.09090909361839294, "mask": 0.0}}], "prob": 0.4677344560623169, "mask": 0.2666666805744171, "selected": true}, "terminate": {"prob": 0.13021954894065857, "mask": 0, "value": null}}, "cls_probs": [0.39151254296302795, 0.467430979013443, 0.14105650782585144], "s_value": 0.4949394464492798, "orig_id": 3130, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 5.9000000059604645}, {"sample": {"about_me": {"value": "<p>Risk Manager at Raiffeisen Capital Management</p>\n\n<p>External Lecturer at Vienna University of Technology</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.86, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0665, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.062, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.01, "prob": 0.1946810930967331, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "How does Cornish-Fisher VaR (aka modified VaR) scale with time?", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I have already posted this question in the quant section, maybe the statistics community is more familiar with the topic:</p>\n\n<p>I am thinking about the time-scaling of <strong>Cornish-Fisher VaR</strong> (see e.g.page 130  <a href=\"http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceAnalytics.pdf\" rel=\"nofollow\">here</a> for the formula).</p>\n\n<p>It involves the <strong>skewness</strong> and the <strong>excess-kurtosis</strong> of returns. The formula is clear and well studied (and criticized) in various papers for a single time period (e.g. daily returns and a VaR with one-day holding period).</p>\n\n<p>Does anybody know a reference on how to scale it with time? I would be looking for something like the square-root-of-time rule (e.g. daily returns and a VaR with $d$ days holding period). But scaling skewness, kurtosis and volatility separately and plugging them back does not feel good. Any ideas?</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0681, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I'll try to give an answer for the mixture case. Let's formalize the set-up. We consider a random variable $X$ and an indicator random variable $I$, with $P[I=1] = 1-P[I=2] = p$, independent of $X$. Furthermore, for the mixture we have that the law of $X$ given that $I=1$ is the law of $X_1$, which is Gaussian with mean $U_1$ and variance $\\\\sigma^2_1$; and, if $I=2$, the law is that of $X_2$ with law $N(U_2,\\\\sigma^2_2)$.</p>\n\n<p>Then, for $Y=\\\\exp(X)$ we can calculate the expectation as\n$$\n\\\\begin{align}\nE[Y] &amp;= E[\\\\exp(X)] = p E[\\\\exp(X)|I=1] + (1-p) E[\\\\exp(X)|I=2] \\\\\\\\\n     &amp;= p E[\\\\exp(X_1)] + (1-p) E[\\\\exp(X_2)] \\\\\\\\\n     &amp;= p \\\\exp(U_1+ \\\\sigma^2_1/2) + (1-p) \\\\exp(U_2+ \\\\sigma^2_2/2) \n\\\\end{align}\n$$\nby using the expectation of a log-normal.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.07, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:\n<a href=\"http://otexts.com/fpp/\" rel=\"nofollow\">http://otexts.com/fpp/</a>.\nThere you find R code and a package to do all those things.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Let's write down what your lasyt expression means in dimension $2$:\n$$\nm(t_1,t_2) =  E[e^{t_1X_1 + t_2 X_2}],\n$$\nthen we get \n$$\n\\\\frac{\\\\partial m}{\\\\partial t_i} = E[X_i e^{t_1X_1 + t_2 X_2}]\n$$\nand we can evaluate at $t_1=t_2 = 0$ to get $E[X_i]$ for $i = 1,2$.\nMore interesting:\n$$\n\\\\frac{\\\\partial m^2}{\\\\partial t_1 \\\\partial t_2} = E[X_1 X_2 e^{t_1X_1 + t_2 X_2}]\n$$\nand again evaluating at $t_1=t_2 = 0$ to get $E[X_1X_2]$ which we need for the covariance.\nPlay with the derivatives and you should get all mixed moments.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I have posted the question on quant.stackexchange. Sorry for the dublicate. You find the answer under <a href=\"http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time\">http://quant.stackexchange.com/questions/3646/how-does-cornish-fisher-var-aka-modified-var-scale-with-time</a>.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Time series model of intraday data on weekdays and weekends", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday data of energy load. The data show strong seasonality within the day (which is clear and well known) and a different pattern on weekdays and weekends. I use the time series packages of R (package ts and then a decomposition in seasonality, level and trend). I get good results when I just concatenate the data ignoring the day of the week and estimate an aggregate model. But I would like to improve the quality on weekends.</p>\n\n<p>How can I formulate a time series model that distinguishes between weekdays and weekends? </p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0331, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.3, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>One possibility is a 2-step model.</p>\n\n<ol>\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\n</ol>\n\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Forecast with STL and regressors (using R package forecast)", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>\n\n<p>Question 1: This can only be done using \"\" method=\"arima\"  \"\" - right?</p>\n\n<p>Question 2: For the model calibration the parameter \"xreg\" should work but how can I enter the new data for th forecast? \"newxreg\" did not work for me.</p>\n\n<p>Thank you</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0251, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>in our article \n<a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">Scaling portfolio volatility and calculating risk contributions in the presence of serial\ncross-correlations</a> we analyze a multivariate model of asset returns. Due to different closing times of the stock exchanges a dependence structure (by the covariance) appears. This dependence only holds for one period. Thus we model this as a vector moving average process of order $1$ (see pages 4 and 5). </p>\n\n<p>The resulting portfolio process is a linear transformation of an $VMA(1)$ process which in general is an $MA(q)$ process with $q\\\\le1$ (see details on pages 15 and 16).</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Daylight saving time in time series modelling (e.g. load data)", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to model intraday electricity load data. I have quarter-hourly data for each day of the year. </p>\n\n<p>In many countries daylight saving time is practive. This has the consequence that once a year a day is 1 hour longer (it has 25 hours) and once a year a day is shorter (only 23 hours). I don't think that changing the time (e.g. to UTC) is a solution here because people go to work at 8:00 a.m. in their \"local\" time not in UTC.</p>\n\n<p>What I have found in the literature is e.g. here <a href=\"http://www.irps.ilstu.edu/research/documents/LoadForecastingHinman-HickeyFall2009.pdf\" rel=\"nofollow\">MODELING AND FORECASTING SHORT-TERMELECTRICITY LOAD USING REGRESSION ANALYSIS</a>. There the extra hour is discarded and the missing hours is filled with average values. </p>\n\n<p>What is the most used procedure here? How do you cope with this probem?</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0194, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Conditional model using function tslm in R package forecast", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days.\nIf data.ts is my time series then I would like to use something like</p>\n\n<pre><code>tslm(data.ts~season|businesss.dummy)\n</code></pre>\n\n<p>Thus I want to model season given that the dummy for this hour is True or False.\nI don't want</p>\n\n<pre><code>tslm(data.ts~season + businesss.dummy)\n</code></pre>\n\n<p>as this would just give a parallel shift on business days.\nI know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?\nThanks!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0368, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Load forecast model with temperature data", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to estimate a regression model of the type:</p>\n\n<p>$\nload_t  = seasonality_t + trend_t + \\\\beta * temperature_t,\n$</p>\n\n<p>and I have load data and temperature on high frequency (hourly data). My impression is that\nthe temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in <a href=\"http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf\" rel=\"nofollow\">http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf</a></p>\n\n<p>I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Formula for one-sided Hodrick-Prescott filter", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I am not very familiar with filters. The Hodrick-Prescott filter as one can find it e.g. <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\">in wikipedia</a> is two-sided. I also found an R implementation for this in the R package <a href=\"http://cran.r-project.org/web/packages/mFilter/mFilter.pdf\" rel=\"nofollow\">mFilter</a>.\nThere the filter is given as: find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=2}^{T-1} (\\\\tau_{t+1}-2 \\\\tau_{t} + \\\\tau_{t-1} )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>If I try to formulate a one-sided version of it myself, then I would take backward looking second order differences. I.e. find $(\\\\tau_t)_{t=1}^T$ such that\n$$\n\\\\left(\\\\sum_{t=1}^T (y_t - \\\\tau_t)^2 + \\\\lambda \\\\sum_{t=3}^{T} (\\\\tau_{t-2}-2 \\\\tau_{t-1} + \\\\tau_t )^2\\\\right) \\\\rightarrow Min.\n$$</p>\n\n<p>How is the usual formulation of a one-sided Hodrick-Prescott filter and does there exist a robust R implementation?</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0874, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "State Space formulation of Hodrick-Prescott \ufb01lter", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I would like to apply the Kalman filter in order to get a causal Hodrick-Prescott \ufb01lter.\nThe Hodrick-Prescott \ufb01lter models a time series $(y_t)_{t=0}^T$ as \n$$\ny_t = \\\\tau_t + c_t\n$$\nwhere $\\\\tau_t$ is a trend component and $c_t$ is a cyclical component.</p>\n\n<p>This <a href=\"http://home.ubalt.edu/ntsbarsh/stat-data/cardamone.pdf\" rel=\"nofollow\">reference</a> defines a state space formulation of the form\n$$\ny_t = \\\\tau_t + c_t\n$$\nas the measurement equation and\n$$\n\\\\tau_t = 2 \\\\tau_{t\u22121} \u2212 \\\\tau_{t\u22122} + \\\\epsilon_t\n$$\nfor the unobservable trend.</p>\n\n<p>I have three questions on this: </p>\n\n<p>A) $c_t$ is assumed to be a random error here, right? Normally distributed with constant variance. This seems a difficult assumption to me.</p>\n\n<p>B) What's the logic behind the equation for the trend?</p>\n\n<p>C) Does anybody know a different state space formulation for this problem? Or a nother reference?</p>\n\n<p>Thanks!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0598, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.02, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Hi as posted in my comments. You can first check whether your original data is normally distributed. Then when you do the integral by the trapezoidal rule you will preserve normality as the operations that are performed do. \nE.g. If $X$ and $Y$ are normally distributed then also $X+Y$ or $X-Y$ or $a X + bY$ for arbitrary real number $a,b$. In short: if a vector $X$ is normally distributed then also linear transformations $A X $ are normal for some matrix $A$. Of course mean and variance change according to the transformation.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>It turns out that the Kalman filter representation as one can find here:\n<a href=\"http://stats.stackexchange.com/questions/48326/state-space-formulation-of-hodrick-prescott-lter/48336#48336\">State Space formulation of Hodrick-Prescott \ufb01lter</a></p>\n\n<p>yields a solution.</p>\n\n<p>Having formulated the equations one can use the package <a href=\"http://cran.r-project.org/web/packages/sspir/index.html\" rel=\"nofollow\">SSPiR</a> to estimate the solution.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>If you just look at your recursions then this should yield the solution.\nIf $X_0=0$ then $X_n = 0$ for all $n$ -> let's call this the absorbtion. </p>\n\n<p>So we analyze the case that $X_0=1$.\nThen\n$$\nX_{n+1} = X_n - 1 + Y_n\n$$\nConsider $n=0$ if $Y_0 = 0$ then $X_1 = 1 - 1 + 0 = 0$ and we are caught in $0$ due to the absorption.\nIf $Y_0=1$ then $X_1 = 1 - 1 + 1 = 1$ and we are back in the case $X_1 = 1$. Because $Y$ can only take the values $0$ or $1$ (all other probabilities are zero). Then we can continue this reasoning for the transition from $X_1$ to $X_2$ and finally for $X_n,n\\\\ge 0$ this concludes the proof. </p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Using regression splines for values outside of the calibration range", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>in <a href=\"http://stats.stackexchange.com/questions/47519/load-forecast-model-with-temperature-data\">my question</a> on a load forecast model using temperature data as covariates I was advised to use regression splines. This really seems to be a/the solution. </p>\n\n<p>Now I face the following problem: if I calibrate my model on winter data (for technical reasons calibration can not be done on a daily basis, rather every second month) and slowly spring arises I will have temperature data outside of the calibration set.</p>\n\n<p>Are there good techniques to make the regression spline fit robust for values outside of the calibration range? Any experiences or references? Thanks!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0301, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>If variance is greater than the mean then this is called over-dispersion. A natural model for this is the negative binomial distribution. This can also be seen as a Poisson distribution where the Parameter lambda follows a Gamma distribution. A first and easy step could be to fit a negative binomial distribution.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.06, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>What the other answers already suggest is to compare mean and variance.</p>\n\n<ol>\n<li>If mean equals variance then it could be Poisson</li>\n<li>If mean is less than veriaance then it could be negative binomial</li>\n<li>If mean is greater than variance then it could be binomial</li>\n</ol>\n\n<p>And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). </p>\n\n<p>In fact just knowing mean and variance is not much more than\nguessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>the reasoning must (except for the seasonal term) be the same as for a log normal random variable. Let $X$ be $N(\\\\mu,\\\\sigma^2)$ and consider $Y = \\\\exp(X)$ then the expectation of $Y$ is given by\n$$\nE[Y] =  \\\\exp(\\\\mu + \\\\sigma^2/2).\n$$\nThen the seasoal term just shifts the mean further and you arrive at the formula that you have up there.</p>\n\n<p>You can see more details in the wikipedia article on the <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I think the solution is the concept of a compound Poisson distribution. The idea is a random sum \n$$\nS = \\\\sum_{i=1}^N X_i\n$$ \nwith $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by\n$$\nE[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \\\\exp(\\\\lambda(s^k-1))\n$$\nFor the sum $Z = k_1 N_1 + k_2 N_2$ you get\n$$\nG_Z(s) = \\\\exp(\\\\lambda_1(s^{k_1}-1) + \\\\lambda_2(s^{k_2}-1)).\n$$ \ndefine $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ then\n$$\nG_Z(s) = \\\\exp(\\\\lambda ( \\\\frac{\\\\lambda_1}{\\\\lambda}(s^{k_1}-1)+ \\\\frac{\\\\lambda_2}{\\\\lambda}(s^{k_1}-1)) = \\\\exp(\\\\lambda (\\\\frac{\\\\lambda_1}{\\\\lambda}s^{k_1}+ \\\\frac{\\\\lambda_2}{\\\\lambda}s^{k_1}-1)).\n$$\nThe final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\\\\lambda = \\\\lambda_1 + \\\\lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\\\\lambda_1/\\\\lambda$ and the value $k_2$ with $\\\\lambda_2/\\\\lambda$.</p>\n\n<p>Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. </p>\n\n<p>Edit after a discussion:</p>\n\n<p>I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. </p>\n\n<ol>\n<li>sample N from $Pois(\\\\lambda)$ (very efficient) </li>\n<li>then for each $i=1,\\\\ldots,N$ sample whether it is from $X_1$ or $X_2$ where\nthe probability of the first is $\\\\lambda_1/\\\\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\\\\lambda_1/\\\\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. </li>\n</ol>\n\n<p>You will have a sample of say 100 000 in seconds. </p>\n\n<p>Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.</p>\n\n<p>Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>The Wiener process is defined as a process having normal/Gaussian increments. If you assume a Laplace distribution then it is called a <a href=\"http://en.wikipedia.org/wiki/L%C3%A9vy_process\" rel=\"nofollow\">L\u00e9vy process</a>. Note that the class of L\u00e9vy processes is much wider and allows for various distributions of the increments.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I think what is meant is: produce (pseudo-) random numbers.\nNote that for a distribution function $F$ and its inverse $F^{-1}$ and a uniform random variable $U$ it holds that\n$ F^{-1}(U) $ has distribution $F$. You an google random number generation and find this procedure.\nIn you case:</p>\n\n<ol>\n<li>find the inverse of $F$</li>\n<li>Generate uniform random variables</li>\n<li>Apply $F^{-1}$</li>\n</ol>\n\n<p>Then you are done. Which programming language do you use?</p>\n\n<p>Edit: As remarked by Macro there is an atom at $0$ with probabilty $\\\\frac12$. I think we can still use the inverse in the following way:</p>\n\n<ol>\n<li>find the inverse for $x \\\\in [-1,0)$ and $x \\\\in (0,1]$</li>\n<li>Generate a uniform $U$</li>\n<li><p>Call the generated rv $X$</p>\n\n<p>3.a) If $U \\\\in [0,\\\\frac14]$ then apply $F^{-1}$ as definded on the left of $0$ and set $X = F^{-1}(U)$</p>\n\n<p>3.b) If $U \\\\in (\\\\frac14,\\\\frac34)$ then set $X = 0$</p>\n\n<p>3.c) If $U \\\\in [\\\\frac34,1]$  then apply $F^{-1}$ as definded on the right of $0$ and set $X = F^{-1}(U)$.</p></li>\n</ol>\n\n<p>This should take care for the atom. Note that I did not care about open or closed intervals in the definition above as the probability for the uniform to reach the the point at the interval end is zero.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.05, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Formulation of a nearly linear model", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I try to fit a model of the following form\n$$\n Y = (\\\\beta X - Z)^+ + \\\\epsilon,\n$$\nwhere $Y,Z \\\\ge 0$ and $X,Y,Z \\\\in \\\\mathbb{R}$ and $(x)^+ = \\\\max(x,0)$. \nNote that $Y,X$ and $Z$ come from a sample, especially $Z$ is not a constant.\nI did this in R in the following way:\n<pre><code>\npositivePart&lt;- function(x){\n  x[x&lt;0]&lt;-0\n  return(x)\n}\nmy.optim = nls(y~ positivePart(beta*x-z) , start = list(beta=5))\n</pre></code>\nI am getting good results, this is not the problem.</p>\n\n<p>My question: is there a name, a class for this kind of model? If so, which R function is the natural candidate to solve this problem, ie. to formulate the model in R? Thank you!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>What about first sampling a uniform $U$. If $U&lt;0.3$ (this has probability $0.3$) then you sample $N(0,1)$ else you do something else.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>are you sure that this is true in general - for continuous as well as discrete distributions? Can you provide a link to the other pages?\nFor a general distibution on $[a,b]$ it is trivial to show that\n$$\nVar(X) = E[(X-E[X])^2] \\\\le E[(b-a)^2] = (b-a)^2.\n$$\nI can imagine that sharper inequalities exist ... \nDo you need the factor $1/4$ for your result? </p>\n\n<p>On the other hand one can find it with the factor $1/4$ under the name <a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">Popoviciu's_inequality</a> on wikipedia.</p>\n\n<p><a href=\"http://files.ele-math.com/articles/jmi-04-32.pdf\" rel=\"nofollow\">This article</a> looks better than the wikipedia article ...</p>\n\n<p>For a uniform distribution it holds that\n$$\nVar(X) = \\\\frac{(b-a)^2}{12}.\n$$</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p><a href=\"http://www.econ.sinica.edu.tw/upload/file/20071130.pdf\" rel=\"nofollow\">In this paper</a> coskewness is defined as\n$$\ncoskew_{i,m} = \\\\frac{COV(r_i,(r_m-\\\\mu_m)^2) }{E[(r_m-\\\\mu_m)^3]}.\n$$\nYou can calculate it by using the standard moment estimators - that's what I would do.\nThus, given a sample for market returns $(r_m^j)_{j=1}^N$ and asset returns $(r_i^j)_{j=1}^N$ you calculate the quantities for each sample pair and do the calculation. </p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Statistical Model from distributions", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I am aware of GLM. There the dependent variable $Y$ is assumed to be generated according to a certain distribution and for the mean it holds that\n$$\n E[Y] = g^{-1}(X \\\\beta).\n$$</p>\n\n<p>Is there a common theory for models of the form\n$$\nE[Y] = g^{-1}(X \\\\beta)\n$$\nand we assume distributions for the components of $X$ (e.g. $X_1$ is Guassian, $X_2$ is Gamma)? I am thinking about an MLE approach in this context.</p>\n\n<p>I guess this is related to GLM but maybe I am missing something. Thank you!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0087, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Unevenly sampled data and the Lomb-Scargle method", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I managed to estimate the periodogram of unevenly sampled data using the <a href=\"http://en.wikipedia.org/wiki/Least-squares_spectral_analysis\" rel=\"nofollow\">Lomb-Scargle Method</a>. Analyzing the frequency domain it would be interesting to filter out a frequency band and then apply IFFT and get back a filtered signal (of course this would be evenly sampled and quite different from the original). </p>\n\n<p>The Lomb-Scargle method does not allow for this procedure. Is there a common approach? Am I missing something here? Thank you!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0327, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>If you write \n$$\nPr(X_{(k)}\\\\le x) = Pr(\\\\{ k \\\\text{ of the } X_i \\\\text{ are } \\\\le x\\\\} \\\\cup \\\\{ k+1 \\\\text{ of the } X_i \\\\text{ are } \\\\le x \\\\} \\\\cup  \\\\cdots  \\\\cup \\\\{ \\\\text{all of the } X_i \\\\text{ are } \\\\le x\\\\}) = Pr(\\\\#\\\\{i|X_i \\\\le x\\\\} \\\\ge k)\n$$\nthen (1) is clear. Recall the definition of ordering. </p>\n\n<p>Then we consider $P[N(x)=k]$ (and not $P[N(x)\\\\ge k])$. \nAs this is an iid sample\nwe get $Pr(X_i\\\\le x) = F(x)$ by definition and by independence and counting the number permutations of $i$ such that $X_i \\\\le x$ you get the Binomial distribution with $p = F(x)$. </p>\n\n<p>I tried to derive the density directly but it didn't wokr out. Googling \"order statistics\" leads you to papers like <a href=\"http://stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec15.pdf\" rel=\"nofollow\">this</a> where the density is derived. It is too long for here, I guess.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>What I found in the meanwhile: this kind of problem is called a Tobit model:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Tobit_model</a></p>\n\n<p>package AER in R can handel them.</p>\n\n<p><a href=\"http://cran.r-project.org/web/packages/AER/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/AER/index.html</a></p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Hi your question was recently covered on <a href=\"http://www.r-bloggers.com/an-infelicity-with-value-at-risk/\" rel=\"nofollow\">r-bloggers</a> in the context of value-at-risk. As far as I see your calculations are correct. And yes ... the curcial point is not where the densities cross, but where the cumulative distribution functions cross.</p>\n\n<p>Recall that quantiles are not about densities - they are derived from the cdf.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Inference of the parameters of a linear congruential generator", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Is there an R package or other open source software that solves the problem of inferring the parameters of a <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\" rel=\"nofollow\">linear congruential generator</a>?\nSuch inference can be done as e.g. described <a href=\"http://www.reteam.org/papers/e59.pdf\" rel=\"nofollow\">here</a> in a strict sense.</p>\n\n<p>On ther other hand, I would also be interested in statistical methods that can be applied in this context.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0031, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Shrinkage Estimator for Newey-West Covariance Matrix", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>This is a <a href=\"http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix\">cross post</a>.\nI would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by\n$$\n\\\\Sigma = \\\\Sigma(0) + \\\\frac12 \\\\left (\\\\Sigma(1) + \\\\Sigma(1)^T \\\\right),\n$$\nwhere $\\\\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\\\\Sigma(0)$ is plausible.</p>\n\n<p>What would you use as prior for $\\\\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\\\\Sigma(0)$ and use the standard sample-estimator for $\\\\Sigma(1)$ then it is not assured that $\\\\Sigma$ is positive-definite.</p>\n\n<p>The above definition is taken from:</p>\n\n<p>Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.</p>\n\n<p>It can also be found <a href=\"http://arxiv.org/pdf/1009.3638.pdf\" rel=\"nofollow\">here</a> in formula (1.9) on page 6.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0119, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Application of likelihood ratio test to test the Markov property", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Do you know a reference (freely available on the web) where the likelihood ratio test is applied in order to test for the Markov property?</p>\n\n<p>The setting is a directly observable discrete Markov-chain with given transition matrices. \nThe concrete application is a model for credit rating transitions. Some statistical tests are applied in this paper <a href=\"http://www.bundesbank.de/Redaktion/EN/Downloads/Publications/Discussion_Paper_2/2005/2005_11_23_dkp_14.pdf?__blob=publicationFile\" rel=\"nofollow\">Time series properties of a rating system\nbased on financial ratios</a> but there are too little details.</p>\n\n<p><a href=\"http://www.econbiz.de/Record/evaluating-the-markov-property-in-studies-of-economic-convergence-bickenbach-frank/10001777562\" rel=\"nofollow\">Evaluating the Markov property</a> by Frank Bickenbach and Eckhardt Bode is an example for testing the Markov property but it is behind a pay wall.</p>\n\n<p>EDIT: \nThe concrete test is:\nNull hypothesis: rating transitions do not depend on past rating distributions. \nThe alternative hypothesis: rating transitions depend on past rating distributions (in the sense of a 1st order Markov chain).</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.04, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0092, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.03, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "How to interpret the dendrogram of a hierarchical cluster analysis", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:</p>\n\n<p><code>\nplot( hclust(dist(USArrests), \"ave\") )\n</code></p>\n\n<ol>\n<li><p>What exactly does the y-axis \"Height\" mean? </p></li>\n<li><p>Looking at North Carolina and California (rather on the left). Is California \"closer\" to North Carolina than Arizona? Can I make this interpretation? </p></li>\n<li>Hawaii (right) joins the cluster rather late. I can see this as it is \"higher\" than other states. In general how can I interpret the fact that labels are \"higher\" or \"lower\" in the dendrogram correctly? </li>\n</ol>\n\n<p>Thanks for any comments or references.<img src=\"http://i.stack.imgur.com/tzoq6.png\" alt=\"enter image description here\"></p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.186, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.2, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.01, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Classification/Regression Tree with nonngeative response", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I try to model the duration until a unit is inspected by a large number of possible explanatory variables.\nThe duration is non-negative and the explanatory variables are factors and numerical variables.</p>\n\n<p>If I knew the best explanatory variables then I would use <code>glm</code> in R. I find family \"Gamma\" with link \"log\" appropriate for modelling something like a waiting time.</p>\n\n<p>Can I combine trees and <code>glm</code> in R? E.g. with <code>randomForest</code> I can use a formula but the regression is a simple linear model (like <code>lm</code>). Thanks!</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0045, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>Recall the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">definition</a> of the $\\\\chi^2$-distribution with $n$ degress of freedom. If \n$X_i$ are $iid$ standard normal then\n$$\nZ = \\\\sum_{i=1}^n X_i^2\n$$\nhas a $\\\\chi^2$-distribution with $n$ degrees of freedom.\nThus it is natural to apply this distribution for the sample estimator of variance if we assume a normal distribution.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": "Generalized linear model with lasso regularization for continuous non-negative response", "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>I have a big data problem with a large number of predictors and a non-negative response (time until inspection). \nFor a full model I would use a glm with Gamma distributed response (link=\"log\").</p>\n\n<p>However I would like to find a small model. The \"best subset glm\" approach does not work for me as I run out of memory - it seems that it is not efficient enough for my setting (big data, weak computer).</p>\n\n<p>So I switched to the LASSO approach (using R packages <code>lars</code> or <code>glmnet</code>). \n<code>glmnet</code> even offers some distribution families besides the Gaussian but not the Gamma family. How can I do a lasso regularization for a glm with Gamma distributed response in R? Could it be a Cox-model (Cox net) for modelling some kind of waiting time? </p>\n\n<p>EDIT: As my data consists of all data points with the information about the time since the last inspection it really seems appropriate to apply a COX model. Putting data in the right format (as <code>Surv</code> does) and calling <code>glmnet</code> with <code>family=\"cox\"</code> could do the job in my case of \"waiting times\" or survival analysis. In my data all data points \"died\" and the Cox model allows to analyse which ones \"died\" sooner. It seems as if in this case  <code>family=\"gamma\"</code> is not needed. Comments are very welcome.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.03, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0086, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}, {"title": {"value": null, "prob": 0.002941540675237775, "mask": 0.0}, "body": {"value": "<p>The approach that I took (mathematician (PhD) by trade with interest in statistics and nowadays as everyone: machine learning):</p>\n\n<p>Go to <a href=\"https://www.kaggle.com/\" rel=\"nofollow\">https://www.kaggle.com/</a> and pick one of the self study projects (e.g. the Titanic challenge). There are tutorials where you learn essential techniques applied to the problem at hand. </p>\n\n<p>Additionally you can read the following two books that are free to download:\n<a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The Elements of \nStatistical Learning:</a>\n<a href=\"http://www-bcf.usc.edu/~gareth/ISL/\" rel=\"nofollow\">An Introduction to Statistical Learning</a></p>\n\n<p>Recently I read this survey article <a href=\"http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3\" rel=\"nofollow\">Big Data: New Tricks for Econometrics</a> and it does both: the Titanic data set and it references the free books.</p>\n", "prob": 0.00296802562661469, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.003136217128485441, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0029146119486540556, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.003230778267607093, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0030504497699439526, "mask": 0.0}, "tags": {"value": null, "prob": 0.0032033035531640053, "mask": 0.0}, "comments": {"value": null, "prob": 0.0029453225433826447, "mask": 0.0}}], "prob": 0.21187791228294373, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Commentator", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Critic", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Editor", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Promoter", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Tumbleweed", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Yearling", "prob": 0.10000000149011612, "mask": 0.0}}, {"badge": {"value": "Popular Question", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Curious", "prob": 0.10000000149011612, "mask": 0.0}}], "prob": 0.46146517992019653, "mask": 0.3333333432674408}, "terminate": {"prob": 0.13197588920593262, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3918284475803375, 0.4679940342903137, 0.14017751812934875], "s_value": 0.4942508935928345, "orig_id": 3130, "true_y": 1, "last_cost": 0.0, "total_cost": 6.000000007450581}], [{"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0654018297791481, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12516337633132935, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.0880502462387085, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.104502834379673, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.1336667537689209, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06495398283004761, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1644158512353897, "mask": 0.0}, "badges": {"value": null, "prob": 0.24869905412197113, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005146146286278963, "mask": 0, "value": null}}, "cls_probs": [0.4182262718677521, 0.4197993576526642, 0.16197440028190613], "s_value": 0.5230421423912048, "orig_id": 2171, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.06145130470395088, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12017360329627991, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.08465293794870377, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09470614790916443, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.12501758337020874, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06252031028270721, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.13996605575084686, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.305805504322052, "mask": 0.0}, "terminate": {"prob": 0.005706647876650095, "mask": 0, "value": null}}, "cls_probs": [0.42206740379333496, 0.4181351363658905, 0.1597975343465805], "s_value": 0.5215128660202026, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.06393192708492279, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12605567276477814, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.08850457519292831, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09802098572254181, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.13112056255340576, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.1466534584760666, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.25, "mask": 0.0, "selected": true}}, {"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}], "prob": 0.3399604856967926, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.005752329248934984, "mask": 0, "value": null}}, "cls_probs": [0.415014386177063, 0.4198645353317261, 0.16512107849121094], "s_value": 0.5162007808685303, "orig_id": 2171, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.06543052196502686, "mask": 0.0}, "views": {"value": 0.03, "prob": 0.12836545705795288, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.09009489417076111, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.10121774673461914, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.13415677845478058, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15390007197856903, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.3209039866924286, "mask": 0.25}, "terminate": {"prob": 0.005930609535425901, "mask": 0, "value": null}}, "cls_probs": [0.4169793128967285, 0.4206150770187378, 0.16240563988685608], "s_value": 0.5151426792144775, "orig_id": 2171, "true_y": 0, "last_cost": 1.0, "total_cost": 2.100000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.06653895229101181, "mask": 0.0, "selected": true}, "views": {"value": 0.03, "prob": 0.1304684579372406, "mask": 0.0}, "reputation": {"value": 0.0116, "prob": 0.0922485738992691, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.10030152648687363, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.13013139367103577, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06087322533130646, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.0608772374689579, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06450364738702774, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.060030248016119, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06601572781801224, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.061987292021512985, "mask": 0.0}, "tags": {"value": null, "prob": 0.06421258300542831, "mask": 0.0}, "comments": {"value": null, "prob": 0.06150002032518387, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.06087322533130646, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.0608772374689579, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06450364738702774, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.060030248016119, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06601572781801224, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.061987292021512985, "mask": 0.0}, "tags": {"value": null, "prob": 0.06421258300542831, "mask": 0.0}, "comments": {"value": null, "prob": 0.06150002032518387, "mask": 0.0}}], "prob": 0.14196528494358063, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.33053404092788696, "mask": 0.25}, "terminate": {"prob": 0.007811844348907471, "mask": 0, "value": null}}, "cls_probs": [0.4256134629249573, 0.4232574999332428, 0.15112900733947754], "s_value": 0.5186076164245605, "orig_id": 2171, "true_y": 0, "last_cost": 1.0, "total_cost": 3.100000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.14675597846508026, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0116, "prob": 0.10449734330177307, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11234106123447418, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.14243578910827637, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.060663823038339615, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.06074320897459984, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06445787847042084, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.059906210750341415, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06619914621114731, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06217125058174133, "mask": 0.0}, "tags": {"value": null, "prob": 0.06484196335077286, "mask": 0.0}, "comments": {"value": null, "prob": 0.06101655587553978, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.060663823038339615, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06074320897459984, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06445787847042084, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.059906210750341415, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06619914621114731, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06217125058174133, "mask": 0.0}, "tags": {"value": null, "prob": 0.06484196335077286, "mask": 0.0}, "comments": {"value": null, "prob": 0.06101655587553978, "mask": 0.0}}], "prob": 0.15723183751106262, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.32532817125320435, "mask": 0.25}, "terminate": {"prob": 0.011409793980419636, "mask": 0, "value": null}}, "cls_probs": [0.4123295545578003, 0.4386632442474365, 0.14900726079940796], "s_value": 0.5133087635040283, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.12172357738018036, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1273743063211441, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.16070501506328583, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06067606434226036, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.060814786702394485, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06453008949756622, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.059858787804841995, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.066348135471344, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06224435567855835, "mask": 0.0}, "tags": {"value": null, "prob": 0.06462362408638, "mask": 0.0}, "comments": {"value": null, "prob": 0.06090417131781578, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.06067606434226036, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.060814786702394485, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06453008949756622, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.059858787804841995, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.066348135471344, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06224435567855835, "mask": 0.0}, "tags": {"value": null, "prob": 0.06462362408638, "mask": 0.0, "selected": true}, "comments": {"value": null, "prob": 0.06090417131781578, "mask": 0.0}}], "prob": 0.17292934656143188, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.4016668498516083, "mask": 0.25}, "terminate": {"prob": 0.015601005405187607, "mask": 0, "value": null}}, "cls_probs": [0.4332939386367798, 0.4271389842033386, 0.13956701755523682], "s_value": 0.5143608450889587, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 4.600000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.12301340699195862, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.12829090654850006, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.16138336062431335, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.060637783259153366, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.060808613896369934, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06451331079006195, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05981051176786423, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06642305105924606, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06231087073683739, "mask": 0.0}, "tags": {"value": null, "prob": 0.06475580483675003, "mask": 0.0}, "comments": {"value": null, "prob": 0.060779087245464325, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.060634318739175797, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06080011650919914, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06452235579490662, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.05979826673865318, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06642411649227142, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0622994601726532, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.06468308717012405, "mask": 0.0}, "comments": {"value": null, "prob": 0.0607992447912693, "mask": 0.0}}], "prob": 0.17334458231925964, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.39743778109550476, "mask": 0.25}, "terminate": {"prob": 0.016529927030205727, "mask": 0, "value": null}}, "cls_probs": [0.4338468313217163, 0.429765909910202, 0.13638721406459808], "s_value": 0.5122377276420593, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 5.100000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.1457452028989792, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.14523810148239136, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06045066565275192, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.060777708888053894, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06447825580835342, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.059717074036598206, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06648606061935425, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06230388954281807, "mask": 0.0}, "tags": {"value": null, "prob": 0.06511779129505157, "mask": 0.0}, "comments": {"value": null, "prob": 0.06070777028799057, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.06044721603393555, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.0607692189514637, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06448729336261749, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.059704847633838654, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06648712605237961, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06229247897863388, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.06504467129707336, "mask": 0.0}, "comments": {"value": null, "prob": 0.06072790548205376, "mask": 0.0}}], "prob": 0.191401869058609, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}], "prob": 0.49268797039985657, "mask": 0.25, "selected": true}, "terminate": {"prob": 0.024926848709583282, "mask": 0, "value": null}}, "cls_probs": [0.4418317675590515, 0.42309823632240295, 0.13506996631622314], "s_value": 0.5119351744651794, "orig_id": 2171, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.1544504463672638, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.15974122285842896, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.0602240227162838, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.060596056282520294, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06437067687511444, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.05961870029568672, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06666956096887589, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06247099116444588, "mask": 0.0}, "tags": {"value": null, "prob": 0.06585283577442169, "mask": 0.0}, "comments": {"value": null, "prob": 0.060236856341362, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.06022058054804802, "mask": 0.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06058759242296219, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06437969952821732, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.05960649624466896, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06667062640190125, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0624595545232296, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.06577888876199722, "mask": 0.0}, "comments": {"value": null, "prob": 0.06025683879852295, "mask": 0.0}}], "prob": 0.21310602128505707, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.44734281301498413, "mask": 0.5}, "terminate": {"prob": 0.02535950019955635, "mask": 0, "value": null}}, "cls_probs": [0.42435136437416077, 0.44795286655426025, 0.12769576907157898], "s_value": 0.5121414661407471, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 5.700000002980232}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1848296821117401, "mask": 0.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.060141172260046005, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.06059178337454796, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0644768550992012, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0594579353928566, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06689684092998505, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06253290176391602, "mask": 0.0}, "tags": {"value": null, "prob": 0.06584884971380234, "mask": 0.0}, "comments": {"value": null, "prob": 0.060093410313129425, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.060137733817100525, "mask": 0.0, "selected": true}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06058332324028015, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06448588520288467, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.05944576486945152, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.066897913813591, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06252145767211914, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.06577491015195847, "mask": 0.0}, "comments": {"value": null, "prob": 0.060113340616226196, "mask": 0.0}}], "prob": 0.23571941256523132, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.5438869595527649, "mask": 0.5}, "terminate": {"prob": 0.03556390106678009, "mask": 0, "value": null}}, "cls_probs": [0.43319809436798096, 0.4487372636795044, 0.11806463450193405], "s_value": 0.5109268426895142, "orig_id": 2171, "true_y": 0, "last_cost": 0.20000000298023224, "total_cost": 6.200000002980232}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.18540304899215698, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06404811888933182, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.0645219087600708, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06857816874980927, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06330960243940353, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07112235575914383, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06656848639249802, "mask": 0.0}, "tags": {"value": null, "prob": 0.07000770419836044, "mask": 0.0}, "comments": {"value": null, "prob": 0.06390047073364258, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06451450288295746, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06859658658504486, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.06329350173473358, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07114849239587784, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06657683104276657, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.06993880867958069, "mask": 0.0}, "comments": {"value": null, "prob": 0.06387442350387573, "mask": 0.0}}], "prob": 0.2312224954366684, "mask": 0.0625}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.5443364381790161, "mask": 0.5}, "terminate": {"prob": 0.03903799131512642, "mask": 0, "value": null}}, "cls_probs": [0.43712112307548523, 0.4433395564556122, 0.11953931301832199], "s_value": 0.5076993703842163, "orig_id": 2171, "true_y": 0, "last_cost": 0.5, "total_cost": 6.4000000059604645}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06390745937824249, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.06440749019384384, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06849441677331924, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06329112499952316, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07119520008563995, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06659436225891113, "mask": 0.0}, "tags": {"value": null, "prob": 0.07037796080112457, "mask": 0.0}, "comments": {"value": null, "prob": 0.06371861696243286, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.0644000917673111, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06851281970739365, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.06327502429485321, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07122135907411575, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06660271435976028, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.07030869275331497, "mask": 0.0}, "comments": {"value": null, "prob": 0.06369265168905258, "mask": 0.0}}], "prob": 0.30059441924095154, "mask": 0.0625}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.6578084230422974, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.041597139090299606, "mask": 0, "value": null}}, "cls_probs": [0.4138270616531372, 0.4640117585659027, 0.12216118723154068], "s_value": 0.49416789412498474, "orig_id": 2171, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 6.9000000059604645}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06391232460737228, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.06440044194459915, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06836634129285812, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06337980180978775, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07102835178375244, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06659309566020966, "mask": 0.0}, "tags": {"value": null, "prob": 0.07060200721025467, "mask": 0.0}, "comments": {"value": null, "prob": 0.0637069046497345, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.0643930435180664, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06838469952344894, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.0633636862039566, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07105445116758347, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06660143285989761, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.07053251564502716, "mask": 0.0}, "comments": {"value": null, "prob": 0.06368094682693481, "mask": 0.0}}], "prob": 0.3208637237548828, "mask": 0.0625}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0, "selected": true}}, {"badge": {"value": "Student", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}], "prob": 0.6340357661247253, "mask": 0.75, "selected": true}, "terminate": {"prob": 0.045100484043359756, "mask": 0, "value": null}}, "cls_probs": [0.40809395909309387, 0.46432748436927795, 0.12757854163646698], "s_value": 0.4874952435493469, "orig_id": 2171, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 7.000000007450581}, {"sample": {"about_me": {"value": "<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.03, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0116, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.06274069100618362, "mask": 0.0}, "body": {"value": "<p>In principle, you can use a permutation test on any function on data of two groups. To compare regression slopes, you simply pick to species, shuffle the data points between the groups randomly, but instead of comparing the mean value of each permuted group, compare the regression slope. You may have to center your data before computing the regressor (which may be linear or custom built, as long as it can be described by a single parameter after centering the data). I'm not sure how to deal with multiple comparisons in permutation test -- for a few categories, Bonferroni correction will do the trick, but I guess with several dozen species that will kill any effect... Ideas on this?</p>\n", "prob": 0.06313157081604004, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06752900779247284, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06375101953744888, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07109282165765762, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06692890077829361, "mask": 0.0}, "tags": {"value": null, "prob": 0.0758001059293747, "mask": 0.0}, "comments": {"value": null, "prob": 0.060431234538555145, "mask": 0.0}}, {"title": {"value": "Guess level of measurement of unknown data", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>i'm writing a few convenience functions for exploring datasets. For that, I have to guess the type of data from the values. What I'm looking for are some good rules of thumb.</p>\n\n<p>Some easy cases:</p>\n\n<ul>\n<li><code>['spam', 'eggs']</code> is nominal, because the dataset contains strings.</li>\n<li><code>[true, false, false]</code> is nominal, too.</li>\n<li><code>[0, 1, 0, 0, 0, 1, 1]</code> is either nominal or ordinal.</li>\n<li><code>[0, 1, 4, 3, 1, 2, 2]</code> too, but it's probably ordinal. Many values, and they have numbers, but not too many.</li>\n<li><code>[0.2, 2, 1.6, 3]</code> is cardinal, as the level of precision suggests there could possibly be many other values.</li>\n</ul>\n\n<p>As you see the judgment is absolutely vague, but that's not the point: I just need heuristics that work in 80% of the cases. Or better, 95%. More ideas?</p>\n", "prob": 0.06312432140111923, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06754714250564575, "mask": 0.0}, "views": {"value": 0.0052, "prob": 0.06373480707406998, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07111895084381104, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06693729013204575, "mask": 0.0}, "tags": {"value": [{"tag": {"value": "machine-learning", "prob": 0.5, "mask": 0.0}}, {"tag": {"value": "scales", "prob": 0.5, "mask": 0.0}}], "prob": 0.0757255032658577, "mask": 0.0}, "comments": {"value": null, "prob": 0.060406606644392014, "mask": 0.0}}], "prob": 0.8937955498695374, "mask": 0.0625}, "badges": {"value": [{"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.10620447993278503, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3198751211166382, 0.4870489537715912, 0.19307589530944824], "s_value": 0.47051161527633667, "orig_id": 2171, "true_y": 0, "last_cost": 0.0, "total_cost": 7.100000008940697}], [{"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 5770, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.05567750707268715, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.09003882110118866, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.06750227510929108, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07280804216861725, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.052288029342889786, "mask": 0.0}, "website": {"value": 0, "prob": 0.449741929769516, "mask": 0.0}, "posts": {"value": null, "prob": 0.08290823549032211, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.1103576049208641, "mask": 0.0}, "terminate": {"prob": 0.018677571788430214, "mask": 0, "value": null}}, "cls_probs": [0.45891156792640686, 0.401665598154068, 0.13942290842533112], "s_value": 0.5302353501319885, "orig_id": 5770, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.05548827722668648, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.09916920959949493, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.07224227488040924, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07436434924602509, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05598073825240135, "mask": 0.0, "selected": true}, "website": {"value": 0, "prob": 0.4092787206172943, "mask": 0.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03101261518895626, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.031141232699155807, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03189302980899811, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.030187739059329033, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03231852501630783, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031156232580542564, "mask": 0.0}, "tags": {"value": null, "prob": 0.030792908743023872, "mask": 0.0}, "comments": {"value": null, "prob": 0.03149770200252533, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03101261518895626, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.031141232699155807, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03189302980899811, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.030187739059329033, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03231852501630783, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031156232580542564, "mask": 0.0}, "tags": {"value": null, "prob": 0.030792908743023872, "mask": 0.0}, "comments": {"value": null, "prob": 0.03149770200252533, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03101261518895626, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.031141232699155807, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03189302980899811, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.030187739059329033, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03231852501630783, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031156232580542564, "mask": 0.0}, "tags": {"value": null, "prob": 0.030792908743023872, "mask": 0.0}, "comments": {"value": null, "prob": 0.03149770200252533, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.03101261518895626, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.031141232699155807, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03189302980899811, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.030187739059329033, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03231852501630783, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031156232580542564, "mask": 0.0}, "tags": {"value": null, "prob": 0.030792908743023872, "mask": 0.0}, "comments": {"value": null, "prob": 0.03149770200252533, "mask": 0.0}}], "prob": 0.07575203478336334, "mask": 0.0}, "badges": {"value": null, "prob": 0.1360747516155243, "mask": 0.0}, "terminate": {"prob": 0.021649593487381935, "mask": 0, "value": null}}, "cls_probs": [0.4795724153518677, 0.40699541568756104, 0.11343216150999069], "s_value": 0.5388158559799194, "orig_id": 5770, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.060893554240465164, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.11292654275894165, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.08132099360227585, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.08234541118144989, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.38175851106643677, "mask": 0.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03089880757033825, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.03106117807328701, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.031948503106832504, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.03010088950395584, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03250550106167793, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117564134299755, "mask": 0.0}, "tags": {"value": null, "prob": 0.031000226736068726, "mask": 0.0}, "comments": {"value": null, "prob": 0.031309254467487335, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03089880757033825, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.03106117807328701, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.031948503106832504, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.03010088950395584, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03250550106167793, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117564134299755, "mask": 0.0}, "tags": {"value": null, "prob": 0.031000226736068726, "mask": 0.0}, "comments": {"value": null, "prob": 0.031309254467487335, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03089880757033825, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.03106117807328701, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.031948503106832504, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.03010088950395584, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03250550106167793, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117564134299755, "mask": 0.0}, "tags": {"value": null, "prob": 0.031000226736068726, "mask": 0.0}, "comments": {"value": null, "prob": 0.031309254467487335, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.03089880757033825, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.03106117807328701, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.031948503106832504, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.03010088950395584, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03250550106167793, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117564134299755, "mask": 0.0}, "tags": {"value": null, "prob": 0.031000226736068726, "mask": 0.0}, "comments": {"value": null, "prob": 0.031309254467487335, "mask": 0.0}}], "prob": 0.08674401789903641, "mask": 0.0}, "badges": {"value": null, "prob": 0.1741013079881668, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.019909696653485298, "mask": 0, "value": null}}, "cls_probs": [0.48865264654159546, 0.4032197594642639, 0.10812762379646301], "s_value": 0.5439004898071289, "orig_id": 5770, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.06913798302412033, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.13028717041015625, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.09494874626398087, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.08766304701566696, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.22763989865779877, "mask": 0.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.030709680169820786, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.030978316441178322, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03206401318311691, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02995864301919937, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03273913636803627, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031134359538555145, "mask": 0.0}, "tags": {"value": null, "prob": 0.031206225976347923, "mask": 0.0}, "comments": {"value": null, "prob": 0.03120962530374527, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.030709680169820786, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.030978316441178322, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03206401318311691, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.02995864301919937, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03273913636803627, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031134359538555145, "mask": 0.0}, "tags": {"value": null, "prob": 0.031206225976347923, "mask": 0.0}, "comments": {"value": null, "prob": 0.03120962530374527, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.030709680169820786, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.030978316441178322, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03206401318311691, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.02995864301919937, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03273913636803627, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031134359538555145, "mask": 0.0}, "tags": {"value": null, "prob": 0.031206225976347923, "mask": 0.0}, "comments": {"value": null, "prob": 0.03120962530374527, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.030709680169820786, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.030978316441178322, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03206401318311691, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02995864301919937, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03273913636803627, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031134359538555145, "mask": 0.0}, "tags": {"value": null, "prob": 0.031206225976347923, "mask": 0.0}, "comments": {"value": null, "prob": 0.03120962530374527, "mask": 0.0}}], "prob": 0.08610878884792328, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1428571492433548, "mask": 0.0}}], "prob": 0.272323340177536, "mask": 0.0}, "terminate": {"prob": 0.03189107030630112, "mask": 0, "value": null}}, "cls_probs": [0.5030030608177185, 0.38816219568252563, 0.10883472114801407], "s_value": 0.5446807146072388, "orig_id": 5770, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.0755293145775795, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.14412103593349457, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.09429088234901428, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.24460607767105103, "mask": 0.0, "selected": true}, "posts": {"value": [{"title": {"value": null, "prob": 0.030650554224848747, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.030960218980908394, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.032123979181051254, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.02987162210047245, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03287861496210098, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117094747722149, "mask": 0.0}, "tags": {"value": null, "prob": 0.031241845339536667, "mask": 0.0}, "comments": {"value": null, "prob": 0.031102215871214867, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.030650554224848747, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.030960218980908394, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.032123979181051254, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.02987162210047245, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03287861496210098, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117094747722149, "mask": 0.0}, "tags": {"value": null, "prob": 0.031241845339536667, "mask": 0.0}, "comments": {"value": null, "prob": 0.031102215871214867, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.030650554224848747, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.030960218980908394, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.032123979181051254, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.02987162210047245, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03287861496210098, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117094747722149, "mask": 0.0}, "tags": {"value": null, "prob": 0.031241845339536667, "mask": 0.0}, "comments": {"value": null, "prob": 0.031102215871214867, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.030650554224848747, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.030960218980908394, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.032123979181051254, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.02987162210047245, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03287861496210098, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03117094747722149, "mask": 0.0}, "tags": {"value": null, "prob": 0.031241845339536667, "mask": 0.0}, "comments": {"value": null, "prob": 0.031102215871214867, "mask": 0.0}}], "prob": 0.0892915353178978, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1428571492433548, "mask": 0.0}}], "prob": 0.31174734234809875, "mask": 0.0}, "terminate": {"prob": 0.04041380062699318, "mask": 0, "value": null}}, "cls_probs": [0.5104996562004089, 0.3901554048061371, 0.09934498369693756], "s_value": 0.5442579388618469, "orig_id": 5770, "true_y": 0, "last_cost": 0.5, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.09201023727655411, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.169413760304451, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.10829951614141464, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03077499195933342, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.031195474788546562, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0322406031191349, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.029795290902256966, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03281592205166817, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031188571825623512, "mask": 0.0}, "tags": {"value": null, "prob": 0.030437948182225227, "mask": 0.0}, "comments": {"value": null, "prob": 0.03155117481946945, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03077499195933342, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.031195474788546562, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0322406031191349, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.029795290902256966, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03281592205166817, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031188571825623512, "mask": 0.0}, "tags": {"value": null, "prob": 0.030437948182225227, "mask": 0.0}, "comments": {"value": null, "prob": 0.03155117481946945, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03077499195933342, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.031195474788546562, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0322406031191349, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.029795290902256966, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03281592205166817, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.031188571825623512, "mask": 0.0}, "tags": {"value": null, "prob": 0.030437948182225227, "mask": 0.0}, "comments": {"value": null, "prob": 0.03155117481946945, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.03077499195933342, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.031195474788546562, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0322406031191349, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.029795290902256966, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03281592205166817, "mask": 0.0, "selected": true}, "favorites": {"value": 0.0, "prob": 0.031188571825623512, "mask": 0.0}, "tags": {"value": null, "prob": 0.030437948182225227, "mask": 0.0}, "comments": {"value": null, "prob": 0.03155117481946945, "mask": 0.0}}], "prob": 0.08712006360292435, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1428571492433548, "mask": 0.0}}], "prob": 0.3404301106929779, "mask": 0.0}, "terminate": {"prob": 0.20272637903690338, "mask": 0, "value": null}}, "cls_probs": [0.5764186382293701, 0.3509758412837982, 0.07260551303625107], "s_value": 0.5583970546722412, "orig_id": 5770, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.09237323701381683, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.16884949803352356, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.10859110951423645, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.031796541064977646, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.032223451882600784, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0333486907184124, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.030805518850684166, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03397113084793091, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03226807340979576, "mask": 0.0}, "tags": {"value": null, "prob": 0.03156839311122894, "mask": 0.0}, "comments": {"value": null, "prob": 0.032512497156858444, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.031796541064977646, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.032223451882600784, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0333486907184124, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.030805518850684166, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03397113084793091, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03226807340979576, "mask": 0.0}, "tags": {"value": null, "prob": 0.03156839311122894, "mask": 0.0}, "comments": {"value": null, "prob": 0.032512497156858444, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.031796541064977646, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.032223451882600784, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0333486907184124, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.030805518850684166, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03397113084793091, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03226807340979576, "mask": 0.0}, "tags": {"value": null, "prob": 0.03156839311122894, "mask": 0.0}, "comments": {"value": null, "prob": 0.032512497156858444, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.031794577836990356, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.0322219580411911, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03335735946893692, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.030801944434642792, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.03227671980857849, "mask": 0.0}, "tags": {"value": null, "prob": 0.0315752737224102, "mask": 0.0}, "comments": {"value": null, "prob": 0.03248921036720276, "mask": 0.0}}], "prob": 0.08746397495269775, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.1428571492433548, "mask": 0.0, "selected": true}}, {"badge": {"value": "Scholar", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1428571492433548, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1428571492433548, "mask": 0.0}}], "prob": 0.34299176931381226, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.19973038136959076, "mask": 0, "value": null}}, "cls_probs": [0.576389491558075, 0.3502044081687927, 0.07340608537197113], "s_value": 0.5584497451782227, "orig_id": 5770, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.09615971893072128, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.17908287048339844, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.11629197001457214, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03176514804363251, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.032194603234529495, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0333537757396698, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.03076053410768509, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03403601050376892, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03229084983468056, "mask": 0.0}, "tags": {"value": null, "prob": 0.031639132648706436, "mask": 0.0}, "comments": {"value": null, "prob": 0.03247045353055, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03176514804363251, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.032194603234529495, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.0333537757396698, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.03076053410768509, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03403601050376892, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03229084983468056, "mask": 0.0}, "tags": {"value": null, "prob": 0.031639132648706436, "mask": 0.0}, "comments": {"value": null, "prob": 0.03247045353055, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03176514804363251, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.032194603234529495, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0333537757396698, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.03076053410768509, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03403601050376892, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03229084983468056, "mask": 0.0}, "tags": {"value": null, "prob": 0.031639132648706436, "mask": 0.0}, "comments": {"value": null, "prob": 0.03247045353055, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.031763188540935516, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.03219310939311981, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03336244449019432, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.030756963416934013, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.032299503684043884, "mask": 0.0}, "tags": {"value": null, "prob": 0.03164602816104889, "mask": 0.0}, "comments": {"value": null, "prob": 0.0324472039937973, "mask": 0.0}}], "prob": 0.09671590477228165, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.1666666716337204, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.1666666716337204, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.1666666716337204, "mask": 0.0}}], "prob": 0.34413942694664, "mask": 0.1428571492433548, "selected": true}, "terminate": {"prob": 0.16761009395122528, "mask": 0, "value": null}}, "cls_probs": [0.5689154863357544, 0.3601762056350708, 0.07090835273265839], "s_value": 0.5560034513473511, "orig_id": 5770, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.200000002980232}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.10444331169128418, "mask": 0.0}, "views": {"value": 0.13, "prob": 0.18257500231266022, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.12355789542198181, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03151609003543854, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.03191402181982994, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.033214133232831955, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.030871037393808365, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03408404812216759, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03233944624662399, "mask": 0.0}, "tags": {"value": null, "prob": 0.032767053693532944, "mask": 0.0}, "comments": {"value": null, "prob": 0.03181653097271919, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03151609003543854, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.03191402181982994, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.033214133232831955, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.030871037393808365, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03408404812216759, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03233944624662399, "mask": 0.0}, "tags": {"value": null, "prob": 0.032767053693532944, "mask": 0.0}, "comments": {"value": null, "prob": 0.03181653097271919, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03151609003543854, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.03191402181982994, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.033214133232831955, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.030871037393808365, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03408404812216759, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03233944624662399, "mask": 0.0}, "tags": {"value": null, "prob": 0.032767053693532944, "mask": 0.0}, "comments": {"value": null, "prob": 0.03181653097271919, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.03151414170861244, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.03191254287958145, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03322276845574379, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.030867455527186394, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.032348111271858215, "mask": 0.0}, "tags": {"value": null, "prob": 0.0327741913497448, "mask": 0.0}, "comments": {"value": null, "prob": 0.0317937470972538, "mask": 0.0}}], "prob": 0.10805779695510864, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.20000000298023224, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.20000000298023224, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.20000000298023224, "mask": 0.0}}], "prob": 0.34935998916625977, "mask": 0.2857142984867096, "selected": true}, "terminate": {"prob": 0.13200588524341583, "mask": 0, "value": null}}, "cls_probs": [0.5250243544578552, 0.383270800113678, 0.09170480072498322], "s_value": 0.5378144979476929, "orig_id": 5770, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 4.300000004470348}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.13472628593444824, "mask": 0.0, "selected": true}, "views": {"value": 0.13, "prob": 0.18250495195388794, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.14136338233947754, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.030746236443519592, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.03101729229092598, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03280632942914963, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.031218362972140312, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03409240022301674, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03245146945118904, "mask": 0.0}, "tags": {"value": null, "prob": 0.03634711727499962, "mask": 0.0}, "comments": {"value": null, "prob": 0.029844678938388824, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.030746236443519592, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.03101729229092598, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03280632942914963, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.031218362972140312, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03409240022301674, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03245146945118904, "mask": 0.0}, "tags": {"value": null, "prob": 0.03634711727499962, "mask": 0.0}, "comments": {"value": null, "prob": 0.029844678938388824, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.030746236443519592, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.03101729229092598, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03280632942914963, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.031218362972140312, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03409240022301674, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03245146945118904, "mask": 0.0}, "tags": {"value": null, "prob": 0.03634711727499962, "mask": 0.0}, "comments": {"value": null, "prob": 0.029844678938388824, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.030744336545467377, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.031015856191515923, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03281485661864281, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.031214741989970207, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.03246016800403595, "mask": 0.0}, "tags": {"value": null, "prob": 0.03635503724217415, "mask": 0.0}, "comments": {"value": null, "prob": 0.02982330694794655, "mask": 0.0}}], "prob": 0.1307869553565979, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.25, "mask": 0.0}}], "prob": 0.31058749556541443, "mask": 0.4285714328289032}, "terminate": {"prob": 0.10003101825714111, "mask": 0, "value": null}}, "cls_probs": [0.359767347574234, 0.42397403717041016, 0.21625864505767822], "s_value": 0.4903644919395447, "orig_id": 5770, "true_y": 0, "last_cost": 1.0, "total_cost": 4.4000000059604645}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.13, "prob": 0.2134784460067749, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.16778218746185303, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.03074619360268116, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.03103448636829853, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03273635357618332, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.03126310184597969, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034071680158376694, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03256997466087341, "mask": 0.0}, "tags": {"value": null, "prob": 0.036453019827604294, "mask": 0.0}, "comments": {"value": null, "prob": 0.02964385598897934, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.03074619360268116, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.03103448636829853, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03273635357618332, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.03126310184597969, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.034071680158376694, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03256997466087341, "mask": 0.0}, "tags": {"value": null, "prob": 0.036453019827604294, "mask": 0.0}, "comments": {"value": null, "prob": 0.02964385598897934, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.03074619360268116, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.03103448636829853, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03273635357618332, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.03126310184597969, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.034071680158376694, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.03256997466087341, "mask": 0.0}, "tags": {"value": null, "prob": 0.036453019827604294, "mask": 0.0}, "comments": {"value": null, "prob": 0.02964385598897934, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.030744295567274094, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.031033046543598175, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.032744862139225006, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.03125947341322899, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.03257870301604271, "mask": 0.0}, "tags": {"value": null, "prob": 0.036460962146520615, "mask": 0.0}, "comments": {"value": null, "prob": 0.029622627422213554, "mask": 0.0}}], "prob": 0.15090009570121765, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.25, "mask": 0.0, "selected": true}}], "prob": 0.3147190511226654, "mask": 0.4285714328289032, "selected": true}, "terminate": {"prob": 0.153120219707489, "mask": 0, "value": null}}, "cls_probs": [0.35616278648376465, 0.4312439262866974, 0.21259327232837677], "s_value": 0.49016445875167847, "orig_id": 5770, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.4000000059604645}, {"sample": {"about_me": {"value": "<p>Graduate student at Western Michigan University perusing master's degree in Statistics.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.13, "prob": 0.21517272293567657, "mask": 0.0}, "reputation": {"value": 0.0028, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.17132261395454407, "mask": 0.0}, "up_votes": {"value": 0.013, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.030767224729061127, "mask": 0.0}, "body": {"value": "<p>You nay want to try conducting a Levene's test, this checks the assumption of homogeneous variance. If the p value is $&lt; .25$, you assume unequal variances, and your safest bet is to run welch related tests. If your p value is $&gt;.25$, run ANOVA, which for two groups is just a t-test. The assumption of normality is not a big deal when your sample size is so large.</p>\n\n<p>Note that the p value of .25 for the Levene's test is a bit arbitrary, other books/professors may suggest a less conservative value such as .15 or .10, etc...</p>\n", "prob": 0.03104984387755394, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03269736096262932, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0312996581196785, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03400849923491478, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032576095312833786, "mask": 0.0}, "tags": {"value": null, "prob": 0.03645051643252373, "mask": 0.0}, "comments": {"value": null, "prob": 0.029653707519173622, "mask": 0.0}}, {"title": {"value": "Confidence Interval for a Random Sample Selected from Gamma Distribution", "prob": 0.030767224729061127, "mask": 0.0}, "body": {"value": "<p>Working on a homework question and having some trouble... Any help would be greatly appreciated. </p>\n\n<p>Based on a sample 1.23, 0.36, 2.13, 0.91, 0.16, 0.12 from the GAM$(2,\\\\theta)$ distribution, find an exact 95% CI for parameter $\\\\theta$.</p>\n\n<p>So we know GAM$(\\\\alpha, \\\\lambda)$ has the pdf $f(x)= \\\\dfrac{\\\\lambda^{\\\\alpha}}{\\\\Gamma{(\\\\alpha)}} x^{\\\\alpha - 1} \\\\ e^{-\\\\lambda x} $.</p>\n\n<p>Therefore our random sample is distributed with pdf $f(x)=\\\\theta^{2} x e^{-\\\\theta x}$.</p>\n\n<p>I understand that because the question asks for an <em>\"exact\"</em> confidence interval, that I need to find the pivotal variable.</p>\n\n<p>The problem I am having is that most examples I find are along the lines of a random sample...\n$X_1,...,X_n \\\\sim N(\\\\theta, \n\\\\sigma^{2})$ if $\\\\sigma$ is known then $Z= \\\\dfrac{\\\\bar{X}-\\\\theta}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\\\sim N(0,1)$, is pivotal. And from there finding the CI is relatively simple.</p>\n\n<p>I guess I am at a loss as to how one would go about finding the pivotal variable when things are not normally distributed. </p>\n\n<p>Thank you for your help, any suggestions would be appreciated. </p>\n", "prob": 0.03104984387755394, "mask": 0.0}, "score": {"value": 0.02, "prob": 0.03269736096262932, "mask": 0.0}, "views": {"value": 0.0286, "prob": 0.0312996581196785, "mask": 0.0}, "answers": {"value": 0.1, "prob": 0.03400849923491478, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032576095312833786, "mask": 0.0}, "tags": {"value": null, "prob": 0.03645051643252373, "mask": 0.0}, "comments": {"value": null, "prob": 0.029653707519173622, "mask": 0.0}}, {"title": {"value": "Homogeneity of Variance, 2 Way Completely Crossed Design", "prob": 0.030767224729061127, "mask": 0.0}, "body": {"value": "<p>I am looking for some advice as to how I might handle having an unfavorable Levene's test outcome, that is to say a highly significant value. </p>\n\n<p><strong>DOE</strong></p>\n\n<p>Various containers with water were microwaved for 30 seconds under various conditions, the response variable the max temperature. The experiment run order was randomized. There were 4 levels for container types; ceramic, plastic, paper, and foam. 3 Levels of microwave turntable conditions, dirty, wet, and clean. 12 treatment combinations, 4 replicates per treatment, i.e. a completely crossed design. The goal is to examine how the water temp is impacted by these 2 factors.</p>\n\n<p><img src=\"http://i.stack.imgur.com/vc4jI.png\" alt=\"enter image description here\"></p>\n\n<p>As one can see from this interaction plot, there is a pattern when it comes to microwave cleanliness, clean microwaves preform the best while wet ones preform the worst.</p>\n\n<p>Unfortunately, I ran a Levene's Test on the treatment combinations, and it comes out to be highly significant, which means the assumption of homogeneity is broken, and ANOVA is inappropriate. I believe this is caused by the greater deal of variation in the ceramic temps.</p>\n\n<p>I know Welch t-tests on all pairwise comparisons is an option, but then I loose the ability to say the different levels of microwave cleanliness are significantly different (which seems like a crime given the interaction plot).</p>\n\n<p>I also thought about doing a transformation of the response variable, but a box-cox shows that none of the possible transformations are particularly intuitive.</p>\n\n<p><img src=\"http://i.stack.imgur.com/uGOod.png\" alt=\"enter image description here\"></p>\n\n<p>Any ideas about how to handle this homogeneity issue I would be much appreciated.  </p>\n", "prob": 0.03104984387755394, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.03269736096262932, "mask": 0.0}, "views": {"value": 0.0015, "prob": 0.0312996581196785, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.03400849923491478, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.032576095312833786, "mask": 0.0}, "tags": {"value": null, "prob": 0.03645051643252373, "mask": 0.0}, "comments": {"value": null, "prob": 0.029653707519173622, "mask": 0.0}}, {"title": {"value": "UMP of Folded Normal, Finding the Critical Value", "prob": 0.030765322968363762, "mask": 0.0}, "body": {"value": "<p>Let $X_1,...,X_n$  be a random sample from the folded normal distribution with the density...</p>\n\n<p>$f(x)= \\\\sqrt{\\\\dfrac{2}{\\\\pi}} \\\\theta^{-1} e^{(-x^{2} \\\\theta^{-2})/2}$ for $x&gt;0$ and $\\\\theta&gt;0$, 0 otherwise. </p>\n\n<p>The question asks...</p>\n\n<p>Find the UMP for test size $\\\\alpha$ for $H_0:\\\\theta = \\\\theta_0$ vs. $H_0:\\\\theta &gt; \\\\theta_0$. Determine the critical region for the value. </p>\n\n<p>So the likelihood ratio is...\n$$\\\\dfrac{L(f(x,\\\\theta_1))}{L(f(x,\\\\theta_0))}= \\\\bigg(\\\\dfrac{\\\\theta_1}{\\\\theta_0}\\\\bigg)^{-n} exp\\\\bigg[{\\\\sum{x^{2}_{i}}\\\\frac{\\\\theta_{0}^{-2}-\\\\theta_{1}^{-2}}{2}} \\\\bigg]$$</p>\n\n<p>We know the UMP exists because the function is decreasing in $\\\\sum x_{i}^{2}= T(x)$. So we reject the null if $T(x)&lt;k$. </p>\n\n<p>I am unsure about how to find $k$ because I do not know how to find the distribution of $T(x)$. Any ideas on this would be greatly appreciated. </p>\n", "prob": 0.031048405915498734, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.03270586207509041, "mask": 0.0}, "views": {"value": 0.0034, "prob": 0.0312960259616375, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.03258482366800308, "mask": 0.0}, "tags": {"value": null, "prob": 0.03645845875144005, "mask": 0.0}, "comments": {"value": null, "prob": 0.02963247150182724, "mask": 0.0}}], "prob": 0.15363915264606476, "mask": 0.03125}, "badges": {"value": [{"badge": {"value": "Student", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Scholar", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Editor", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Commentator", "prob": 0.0, "mask": 1.0}}], "prob": 0.2984432876110077, "mask": 0.5714285969734192}, "terminate": {"prob": 0.1614222377538681, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.3537807762622833, 0.43168023228645325, 0.21453897655010223], "s_value": 0.48761284351348877, "orig_id": 5770, "true_y": 0, "last_cost": 0.0, "total_cost": 5.500000007450581}], [{"sample": {"about_me": {"value": "<p>I'm a PhD student in Machine Learning at <a href=\"http://www.xrce.xerox.com/Research-Development/Services-Innovation-Laboratory/Machine-Learning-for-Services\" rel=\"nofollow\">Xerox Research Centre Europe</a> and <a href=\"http://ama.liglab.fr/\" rel=\"nofollow\">Laboratoire d'Informatique de Grenoble</a>.</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 8041, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>I'm a PhD student in Machine Learning at <a href=\"http://www.xrce.xerox.com/Research-Development/Services-Innovation-Laboratory/Machine-Learning-for-Services\" rel=\"nofollow\">Xerox Research Centre Europe</a> and <a href=\"http://ama.liglab.fr/\" rel=\"nofollow\">Laboratoire d'Informatique de Grenoble</a>.</p>\n", "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.07812844961881638, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.06484165042638779, "mask": 0.0, "selected": true}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 8041, "true_y": 0, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>I'm a PhD student in Machine Learning at <a href=\"http://www.xrce.xerox.com/Research-Development/Services-Innovation-Laboratory/Machine-Learning-for-Services\" rel=\"nofollow\">Xerox Research Centre Europe</a> and <a href=\"http://ama.liglab.fr/\" rel=\"nofollow\">Laboratoire d'Informatique de Grenoble</a>.</p>\n", "prob": 0.07047983258962631, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.10195153951644897, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.07685013115406036, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08378825336694717, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06735152751207352, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.06815963238477707, "mask": 0.0}, "badges": {"value": null, "prob": 0.08463393151760101, "mask": 0.0}, "terminate": {"prob": 0.44678518176078796, "mask": 0, "value": null}}, "cls_probs": [0.5433760285377502, 0.3726280927658081, 0.08399589359760284], "s_value": 0.5501903891563416, "orig_id": 8041, "true_y": 0, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>I'm a PhD student in Machine Learning at <a href=\"http://www.xrce.xerox.com/Research-Development/Services-Innovation-Laboratory/Machine-Learning-for-Services\" rel=\"nofollow\">Xerox Research Centre Europe</a> and <a href=\"http://ama.liglab.fr/\" rel=\"nofollow\">Laboratoire d'Informatique de Grenoble</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.08718953281641006, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.06487246602773666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07016751170158386, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.059261616319417953, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.05621685832738876, "mask": 0.0}, "badges": {"value": null, "prob": 0.06575477868318558, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.596537172794342, "mask": 0, "value": null}}, "cls_probs": [0.5414372086524963, 0.37480002641677856, 0.08376272022724152], "s_value": 0.5501857399940491, "orig_id": 8041, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>I'm a PhD student in Machine Learning at <a href=\"http://www.xrce.xerox.com/Research-Development/Services-Innovation-Laboratory/Machine-Learning-for-Services\" rel=\"nofollow\">Xerox Research Centre Europe</a> and <a href=\"http://ama.liglab.fr/\" rel=\"nofollow\">Laboratoire d'Informatique de Grenoble</a>.</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.02783093973994255, "mask": 0.0}, "reputation": {"value": 0.0001, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 1, "prob": 0.022818293422460556, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.023470131680369377, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.02281210944056511, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.017357997596263885, "mask": 0.0}, "badges": {"value": null, "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.8857104778289795, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5542475581169128, 0.35326436161994934, 0.09248807281255722], "s_value": 0.5443478226661682, "orig_id": 8041, "true_y": 0, "last_cost": 0.0, "total_cost": 3.0}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0006, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 6801, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.055667996406555176, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.09037550538778305, "mask": 0.0}, "reputation": {"value": 0.0006, "prob": 0.06762965768575668, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07299014180898666, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.05232794210314751, "mask": 0.0}, "website": {"value": 1, "prob": 0.44816169142723083, "mask": 0.0}, "posts": {"value": null, "prob": 0.08329818397760391, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.11120704561471939, "mask": 0.0}, "terminate": {"prob": 0.018341800197958946, "mask": 0, "value": null}}, "cls_probs": [0.45844823122024536, 0.4023456275463104, 0.13920611143112183], "s_value": 0.5301821231842041, "orig_id": 6801, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.05541279539465904, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.09911029040813446, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0006, "prob": 0.07215610891580582, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.07433279603719711, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0559016577899456, "mask": 0.0}, "website": {"value": 1, "prob": 0.40978625416755676, "mask": 0.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.06202569976449013, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.06228078529238701, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06378994882106781, "mask": 0.0}, "views": {"value": 0.0114, "prob": 0.060372792184352875, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06464067846536636, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06230967864394188, "mask": 0.0}, "tags": {"value": null, "prob": 0.0615801066160202, "mask": 0.0}, "comments": {"value": null, "prob": 0.06300033628940582, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.06202569976449013, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.06228078529238701, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06378994882106781, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.060372792184352875, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06464067846536636, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06230967864394188, "mask": 0.0}, "tags": {"value": null, "prob": 0.0615801066160202, "mask": 0.0}, "comments": {"value": null, "prob": 0.06300033628940582, "mask": 0.0}}], "prob": 0.07577292621135712, "mask": 0.0}, "badges": {"value": null, "prob": 0.13616420328617096, "mask": 0.0}, "terminate": {"prob": 0.021362990140914917, "mask": 0, "value": null}}, "cls_probs": [0.479217529296875, 0.40739554166793823, 0.11338690668344498], "s_value": 0.5388408899307251, "orig_id": 6801, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": null, "prob": 0.06253111362457275, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.08243659138679504, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.08224182575941086, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.06342621892690659, "mask": 0.0}, "website": {"value": 1, "prob": 0.43250298500061035, "mask": 0.0, "selected": true}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.06202451512217522, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.062334924936294556, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06389821320772171, "mask": 0.0}, "views": {"value": 0.0114, "prob": 0.0603182427585125, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06481312960386276, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062364611774683, "mask": 0.0}, "tags": {"value": null, "prob": 0.061352480202913284, "mask": 0.0}, "comments": {"value": null, "prob": 0.06289388239383698, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.06202451512217522, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.062334924936294556, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06389821320772171, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.0603182427585125, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06481312960386276, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.062364611774683, "mask": 0.0}, "tags": {"value": null, "prob": 0.061352480202913284, "mask": 0.0}, "comments": {"value": null, "prob": 0.06289388239383698, "mask": 0.0}}], "prob": 0.08187197148799896, "mask": 0.0}, "badges": {"value": null, "prob": 0.1667802333831787, "mask": 0.0}, "terminate": {"prob": 0.028209030628204346, "mask": 0, "value": null}}, "cls_probs": [0.499311625957489, 0.3956686854362488, 0.10501962900161743], "s_value": 0.5402030944824219, "orig_id": 6801, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": null, "prob": 0.09190310537815094, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.12330880016088486, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1374673843383789, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.09506268054246902, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.06120429188013077, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.061183203011751175, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06409598886966705, "mask": 0.0}, "views": {"value": 0.0114, "prob": 0.06041170284152031, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06549536436796188, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06225122883915901, "mask": 0.0}, "tags": {"value": null, "prob": 0.06399574130773544, "mask": 0.0}, "comments": {"value": null, "prob": 0.061362482607364655, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.06120429188013077, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.061183203011751175, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06409598886966705, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.06041170284152031, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06549536436796188, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06225122883915901, "mask": 0.0}, "tags": {"value": null, "prob": 0.06399574130773544, "mask": 0.0}, "comments": {"value": null, "prob": 0.061362482607364655, "mask": 0.0}}], "prob": 0.195290207862854, "mask": 0.0}, "badges": {"value": null, "prob": 0.34399300813674927, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.012974828481674194, "mask": 0, "value": null}}, "cls_probs": [0.43442100286483765, 0.4143083095550537, 0.15127065777778625], "s_value": 0.5149675607681274, "orig_id": 6801, "true_y": 1, "last_cost": 1.0, "total_cost": 2.5}, {"sample": {"about_me": {"value": null, "prob": 0.08042275905609131, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.11224166303873062, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.1137249767780304, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.08627139031887054, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.06082186475396156, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.06099778413772583, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0645049512386322, "mask": 0.0}, "views": {"value": 0.0114, "prob": 0.0599820502102375, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0660235732793808, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06198131665587425, "mask": 0.0}, "tags": {"value": null, "prob": 0.06410035490989685, "mask": 0.0}, "comments": {"value": null, "prob": 0.061588093638420105, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.06082186475396156, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.06099778413772583, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.0645049512386322, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.0599820502102375, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0660235732793808, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06198131665587425, "mask": 0.0}, "tags": {"value": null, "prob": 0.06410035490989685, "mask": 0.0}, "comments": {"value": null, "prob": 0.061588093638420105, "mask": 0.0}}], "prob": 0.14896216988563538, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Student", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.4439159035682678, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.014461157843470573, "mask": 0, "value": null}}, "cls_probs": [0.44255658984184265, 0.40716463327407837, 0.1502787470817566], "s_value": 0.5160548686981201, "orig_id": 6801, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.5}, {"sample": {"about_me": {"value": null, "prob": 0.1392931342124939, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.19117633998394012, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.20753639936447144, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.14740468561649323, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.061033930629491806, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.0611487552523613, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.06428610533475876, "mask": 0.0}, "views": {"value": 0.0114, "prob": 0.060072991997003555, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06585559248924255, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0622645728290081, "mask": 0.0}, "tags": {"value": null, "prob": 0.06391731649637222, "mask": 0.0}, "comments": {"value": null, "prob": 0.06142072379589081, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.061033930629491806, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.0611487552523613, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06428610533475876, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.060072991997003555, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.06585559248924255, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0622645728290081, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.06391731649637222, "mask": 0.0}, "comments": {"value": null, "prob": 0.06142072379589081, "mask": 0.0}}], "prob": 0.2874293327331543, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.027160175144672394, "mask": 0, "value": null}}, "cls_probs": [0.4582054913043976, 0.4055652320384979, 0.13622929155826569], "s_value": 0.5091692805290222, "orig_id": 6801, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": null, "prob": 0.13861346244812012, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.18775375187397003, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.20451252162456512, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.14794407784938812, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.06499412655830383, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.06507726013660431, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0684313252568245, "mask": 0.0, "selected": true}, "views": {"value": 0.0114, "prob": 0.06418401002883911, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07030681520700455, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.06661594659090042, "mask": 0.0}, "tags": {"value": null, "prob": 0.06894021481275558, "mask": 0.0}, "comments": {"value": null, "prob": 0.06474541872739792, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.0649886503815651, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.06507407128810883, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.06844258308410645, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.0641801506280899, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07033195346593857, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0689813494682312, "mask": 0.0}, "comments": {"value": null, "prob": 0.06470619142055511, "mask": 0.0}}], "prob": 0.29223352670669556, "mask": 0.0625, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.028942648321390152, "mask": 0, "value": null}}, "cls_probs": [0.4586869776248932, 0.3983049690723419, 0.14300794899463654], "s_value": 0.5065270662307739, "orig_id": 6801, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.7000000029802322}, {"sample": {"about_me": {"value": null, "prob": 0.13981419801712036, "mask": 0.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.18653035163879395, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.2030535787343979, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.15015067160129547, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.0696864128112793, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.0697435662150383, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0114, "prob": 0.06902049481868744, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07558568567037582, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0717458426952362, "mask": 0.0}, "tags": {"value": null, "prob": 0.07474459707736969, "mask": 0.0}, "comments": {"value": null, "prob": 0.06864892691373825, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.0696837529540062, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.0697428435087204, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.07336702942848206, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.0690196231007576, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07557900249958038, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.07476790249347687, "mask": 0.0}, "comments": {"value": null, "prob": 0.06866424530744553, "mask": 0.0}}], "prob": 0.28983619809150696, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.03061499260365963, "mask": 0, "value": null}}, "cls_probs": [0.4598740339279175, 0.3911425471305847, 0.14898347854614258], "s_value": 0.5077944993972778, "orig_id": 6801, "true_y": 1, "last_cost": 0.5, "total_cost": 3.8000000044703484}, {"sample": {"about_me": {"value": null, "prob": 0.1600751429796219, "mask": 0.0, "selected": true}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.19897855818271637, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.16485561430454254, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.07017074525356293, "mask": 0.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.07065851241350174, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0114, "prob": 0.06943470239639282, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07470209896564484, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.07213503122329712, "mask": 0.0}, "tags": {"value": null, "prob": 0.07349586486816406, "mask": 0.0}, "comments": {"value": null, "prob": 0.06897136569023132, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.07016806304454803, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.07065778225660324, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.07297106087207794, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.06943383067846298, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07469549030065536, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0735187828540802, "mask": 0.0}, "comments": {"value": null, "prob": 0.06898675113916397, "mask": 0.0}}], "prob": 0.18486300110816956, "mask": 0.125}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.2912277579307556, "mask": 0, "value": null}}, "cls_probs": [0.5312056541442871, 0.3425125479698181, 0.12628182768821716], "s_value": 0.524207592010498, "orig_id": 6801, "true_y": 1, "last_cost": 1.0, "total_cost": 4.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.1654987335205078, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.14177125692367554, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.07044212520122528, "mask": 0.0, "selected": true}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.07098393887281418, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0114, "prob": 0.06966638565063477, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07432476431131363, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.07232337445020676, "mask": 0.0}, "tags": {"value": null, "prob": 0.07293053716421127, "mask": 0.0}, "comments": {"value": null, "prob": 0.0690961480140686, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.07043943554162979, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.07098320126533508, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.07276159524917603, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.06966550648212433, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07431819289922714, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.07295328378677368, "mask": 0.0}, "comments": {"value": null, "prob": 0.06911156326532364, "mask": 0.0}}], "prob": 0.14156033098697662, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.5511696934700012, "mask": 0, "value": null}}, "cls_probs": [0.5440662503242493, 0.34045347571372986, 0.1154801994562149], "s_value": 0.5261839032173157, "orig_id": 6801, "true_y": 1, "last_cost": 0.20000000298023224, "total_cost": 5.300000004470348}, {"sample": {"about_me": {"value": null, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0006, "prob": 0.15470105409622192, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13305282592773438, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": "regression with kNN on dataset with categorical variables", "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>I am trying to train a regression model for dataset with 500k observations and 3 features. The features are categorical and have 50, 50 and 100 levels.</p>\n\n<p>Is (generally) kNN appropriate for this kind of task?</p>\n\n<p>I am using R. I tried to turn my categorical variables into dummy variables but I end up with very large and sparse data set. I am using data.matrix for conversion and it sets the matrix to double by default. </p>\n\n<p>Is there a way to set it to logical instead?</p>\n", "prob": 0.07643724977970123, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0114, "prob": 0.0749610885977745, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07998853176832199, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0779130607843399, "mask": 0.0}, "tags": {"value": null, "prob": 0.07842520624399185, "mask": 0.0}, "comments": {"value": null, "prob": 0.07416467368602753, "mask": 0.0}}, {"title": {"value": "Optimal feature selection for MAPE criteria with RandomForest cross-validation", "prob": 0.0758284330368042, "mask": 0.0}, "body": {"value": "<p>I am trying to optimize my set of features against random forest cross-validation using MAPE criteria. </p>\n\n<p>I tried forward selection with Univariate linear regression test (f_regression in sklearn), I calculate MAPE for each set of variables selected by SelectKBest:</p>\n\n<pre><code>for i in range(1,len(X.columns)):\n    selektor = SelectKBest(f_regression, k = i)\n    clf = RandomForestRegressor(n_estimators=10, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    MAPE = mean(abs(pred-y_real)/y_real)\n</code></pre>\n\n<p>I also tries backward selection with the RandomForest feature_importance attribute. I start with full set and in each iteration I remove the least important feature, and I calculate MAPE. I remove then, the feature with the least important MAPE:</p>\n\n<pre><code>while X: \n    clf = RandomForestRegressor(n_estimators=n, max_depth=None)\n    pred = clf.fit(X, y).predict(T)\n    imp = dict(zip(list(X.columns.values), clf.feature_importances_))\n    todrop = min(imp.iteritems(), key=itemgetter(1))[0]\n</code></pre>\n\n<p>First technique returns decent results but it is not optimized because I select K best features with liner regression score criteria. Second technique returns noisy results (MAPE doesn't converge).</p>\n\n<p>I am looking for a technique for feature selection to use in sklearn that will measure MAPE directly. The size of my full set is 150 features and 80,000 observations.</p>\n\n<p>Thank you in advance for any suggestion...</p>\n", "prob": 0.07643269747495651, "mask": 0.0}, "score": {"value": 0.0, "prob": 0.07826852053403854, "mask": 0.0}, "views": {"value": 0.0036, "prob": 0.07496362179517746, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.07994884997606277, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.07841844111680984, "mask": 0.0}, "comments": {"value": null, "prob": 0.07424961775541306, "mask": 0.0}}], "prob": 0.12790939211845398, "mask": 0.1875}, "badges": {"value": [{"badge": {"value": "Student", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.5843366980552673, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5505102276802063, 0.33506545424461365, 0.11442440003156662], "s_value": 0.5227327942848206, "orig_id": 6801, "true_y": 1, "last_cost": 0.0, "total_cost": 5.500000007450581}], [{"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 3.07, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.001, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2694, "true_y": 0, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 3.07, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0, "selected": true}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 2694, "true_y": 0, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.053191423416137695, "mask": 0.0}, "views": {"value": 3.07, "prob": 0.08493419736623764, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06222096458077431, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.08807074278593063, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046432364732027054, "mask": 0.0}, "website": {"value": 1, "prob": 0.5474385023117065, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.1096181869506836, "mask": 0.0}, "terminate": {"prob": 0.008093626238405704, "mask": 0, "value": null}}, "cls_probs": [0.433281272649765, 0.41703012585639954, 0.14968863129615784], "s_value": 0.5145041942596436, "orig_id": 2694, "true_y": 0, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.08963814377784729, "mask": 0.0, "selected": true}, "views": {"value": 3.07, "prob": 0.16942733526229858, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.11810021847486496, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.18814072012901306, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08511225879192352, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.34444013237953186, "mask": 0.0}, "terminate": {"prob": 0.005141121800988913, "mask": 0, "value": null}}, "cls_probs": [0.4052918553352356, 0.4295039772987366, 0.16520413756370544], "s_value": 0.5113533735275269, "orig_id": 2694, "true_y": 0, "last_cost": 1.0, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.19242450594902039, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.13599759340286255, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.2059360295534134, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.10210952162742615, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.3555474281311035, "mask": 0.0}, "terminate": {"prob": 0.00798492506146431, "mask": 0, "value": null}}, "cls_probs": [0.40186601877212524, 0.43274301290512085, 0.1653909981250763], "s_value": 0.5026875734329224, "orig_id": 2694, "true_y": 0, "last_cost": 0.5, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.24592313170433044, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.17486904561519623, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.13420701026916504, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": null, "prob": 0.43262651562690735, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.012374287471175194, "mask": 0, "value": null}}, "cls_probs": [0.40757784247398376, 0.42608362436294556, 0.16633853316307068], "s_value": 0.5020482540130615, "orig_id": 2694, "true_y": 0, "last_cost": 1.0, "total_cost": 3.5}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.22089391946792603, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.15695162117481232, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.1198185384273529, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0}}], "prob": 0.49015694856643677, "mask": 0.0}, "terminate": {"prob": 0.012178894132375717, "mask": 0, "value": null}}, "cls_probs": [0.4113434851169586, 0.42744219303131104, 0.16121427714824677], "s_value": 0.5021008253097534, "orig_id": 2694, "true_y": 0, "last_cost": 0.5, "total_cost": 4.5}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.24119018018245697, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.16976681351661682, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0}}], "prob": 0.5765847563743591, "mask": 0.0}, "terminate": {"prob": 0.012458257377147675, "mask": 0, "value": null}}, "cls_probs": [0.39948806166648865, 0.4351755678653717, 0.16533632576465607], "s_value": 0.49903103709220886, "orig_id": 2694, "true_y": 0, "last_cost": 0.5, "total_cost": 5.0}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.27401185035705566, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.25, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.25, "mask": 0.0, "selected": true}}], "prob": 0.6430062055587769, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.08298196643590927, "mask": 0, "value": null}}, "cls_probs": [0.3103383779525757, 0.5027633905410767, 0.18689824640750885], "s_value": 0.5139869451522827, "orig_id": 2694, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.5}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.2796899676322937, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.0, "mask": 1.0}}], "prob": 0.6175453662872314, "mask": 0.25, "selected": true}, "terminate": {"prob": 0.10276464372873306, "mask": 0, "value": null}}, "cls_probs": [0.2559642493724823, 0.5010592937469482, 0.24297647178173065], "s_value": 0.4914376735687256, "orig_id": 2694, "true_y": 0, "last_cost": 0.10000000149011612, "total_cost": 5.600000001490116}, {"sample": {"about_me": {"value": "<p>i'm a software engineer and web developer, specialized in environmental information systems (eis) with focus on modelling/simulation and geo-information sciences (gis) with focus on visualization. currently enrolled at the potsdam university (germany) in my master degree studies.</p>\n\n<p>interested in open source projects, environmental protection, human rights and progressive minds.</p>\n\n<p><a href=\"http://stackexchange.com/users/1313419\"><img src=\"http://stackexchange.com/users/flair/1313419.png\" alt=\"profile for vertoe on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 3.07, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.2902189791202545, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.001, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Enthusiast", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Fanatic", "prob": 0.0, "mask": 1.0}}], "prob": 0.6027610301971436, "mask": 0.5}, "terminate": {"prob": 0.1070200726389885, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.25751322507858276, 0.49911239743232727, 0.24337442219257355], "s_value": 0.4891681969165802, "orig_id": 2694, "true_y": 0, "last_cost": 0.0, "total_cost": 5.700000002980232}], [{"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.003, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.05086076632142067, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.08377869427204132, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.06073615327477455, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.08429394662380219, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.046267472207546234, "mask": 0.0}, "website": {"value": 1, "prob": 0.47903963923454285, "mask": 0.0}, "posts": {"value": null, "prob": 0.07981307804584503, "mask": 0.0}, "badges": {"value": null, "prob": 0.10642211139202118, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.008788165636360645, "mask": 0, "value": null}}, "cls_probs": [0.4324195981025696, 0.4235406517982483, 0.14403972029685974], "s_value": 0.5171700119972229, "orig_id": 0, "true_y": 1, "last_cost": 1.0, "total_cost": 0.5}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.06109046936035156, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.10398659855127335, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.07540115714073181, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.10018464177846909, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0578303299844265, "mask": 0.0}, "website": {"value": 1, "prob": 0.32791465520858765, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.08979415893554688, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.17072291672229767, "mask": 0.0}, "terminate": {"prob": 0.013075140304863453, "mask": 0, "value": null}}, "cls_probs": [0.44618523120880127, 0.40970873832702637, 0.14410604536533356], "s_value": 0.5159000158309937, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 1.5}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.06813771277666092, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.13405771553516388, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09282467514276505, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.1425197273492813, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06867030262947083, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.15965689718723297, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.32917311787605286, "mask": 0.0}, "terminate": {"prob": 0.004959794692695141, "mask": 0, "value": null}}, "cls_probs": [0.4051554799079895, 0.4371482729911804, 0.1576962172985077], "s_value": 0.5128061175346375, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.0710844174027443, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.14111016690731049, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.09737872332334518, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.1496697962284088, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.16918617486953735, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.36655429005622864, "mask": 0.0}, "terminate": {"prob": 0.0050164684653282166, "mask": 0, "value": null}}, "cls_probs": [0.39762547612190247, 0.4375667870044708, 0.16480770707130432], "s_value": 0.5092275738716125, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 2.5}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.07895054668188095, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.15683481097221375, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.16254104673862457, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.17949551343917847, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.41577696800231934, "mask": 0.0}, "terminate": {"prob": 0.006401119753718376, "mask": 0, "value": null}}, "cls_probs": [0.39982229471206665, 0.4405551552772522, 0.15962255001068115], "s_value": 0.5045518279075623, "orig_id": 0, "true_y": 1, "last_cost": 1.0, "total_cost": 3.0}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.09504464268684387, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.1884964108467102, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.2010713517665863, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.3333333432674408, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.3333333432674408, "mask": 0.0, "selected": true}}, {"badge": {"value": "Quorum", "prob": 0.3333333432674408, "mask": 0.0}}], "prob": 0.5089921355247498, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.006395446136593819, "mask": 0, "value": null}}, "cls_probs": [0.4071320593357086, 0.433837354183197, 0.15903057157993317], "s_value": 0.5004737973213196, "orig_id": 0, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.0}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.10034232586622238, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.203675776720047, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.21696646511554718, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": 0.5, "mask": 0.0, "selected": true}}], "prob": 0.47205469012260437, "mask": 0.3333333432674408, "selected": true}, "terminate": {"prob": 0.006960791535675526, "mask": 0, "value": null}}, "cls_probs": [0.3975846469402313, 0.4549863040447235, 0.14742903411388397], "s_value": 0.4971071481704712, "orig_id": 0, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.100000001490116}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.15471166372299194, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.19447988271713257, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.2243138551712036, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}], "prob": 0.42268192768096924, "mask": 0.6666666865348816}, "terminate": {"prob": 0.003812683280557394, "mask": 0, "value": null}}, "cls_probs": [0.18681618571281433, 0.4400695860385895, 0.3731142580509186], "s_value": 0.48160555958747864, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 4.200000002980232}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.19568881392478943, "mask": 0.0}, "views": {"value": 0.25, "prob": 0.24463610351085663, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 1.0, "mask": 0.0, "selected": true}}, {"badge": {"value": "Supporter", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": 0.0, "mask": 1.0}}], "prob": 0.5542207360267639, "mask": 0.6666666865348816, "selected": true}, "terminate": {"prob": 0.005454300902783871, "mask": 0, "value": null}}, "cls_probs": [0.18882644176483154, 0.4315391182899475, 0.37963446974754333], "s_value": 0.4803776741027832, "orig_id": 0, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.700000002980232}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.4394896924495697, "mask": 0.0, "selected": true}, "views": {"value": 0.25, "prob": 0.5472767949104309, "mask": 0.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.013233531266450882, "mask": 0, "value": null}}, "cls_probs": [0.19080029428005219, 0.43570321798324585, 0.37349647283554077], "s_value": 0.4744681417942047, "orig_id": 0, "true_y": 1, "last_cost": 1.0, "total_cost": 4.800000004470348}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.25, "prob": 0.9697646498680115, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.030235370621085167, "mask": 0, "value": null}}, "cls_probs": [0.18723580241203308, 0.4440179765224457, 0.36874625086784363], "s_value": 0.4683459997177124, "orig_id": 0, "true_y": 1, "last_cost": 0.5, "total_cost": 5.800000004470348}, {"sample": {"about_me": {"value": "<p>Developer on the StackOverflow team.  Find me on</p>\n\n<p><a href=\"http://www.twitter.com/SuperDalgas\" rel=\"nofollow\">Twitter</a>\n<br><br>\n<a href=\"http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/\">Stack Overflow Valued Associate #00003</a></p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.25, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0101, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.003, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Supporter", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Quorum", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=1", "selected": true}}, "cls_probs": [0.18037980794906616, 0.4534739553928375, 0.36614617705345154], "s_value": 0.466518759727478, "orig_id": 0, "true_y": 1, "last_cost": 0.0, "total_cost": 6.300000004470348}], [{"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 1, "prob": 0.5591423511505127, "mask": 0.0}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 4245, "true_y": 1, "last_cost": 1.0, "total_cost": 0}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0551145076751709, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.09331733733415604, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.06905834376811981, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.0707111731171608, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08534760773181915, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0527891106903553, "mask": 0.0}, "website": {"value": 1, "prob": 0.3274863362312317, "mask": 0.0}, "posts": {"value": null, "prob": 0.07465331256389618, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0}}, {"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}], "prob": 0.15204817056655884, "mask": 0.0}, "terminate": {"prob": 0.01947406306862831, "mask": 0, "value": null}}, "cls_probs": [0.471068412065506, 0.3902972638607025, 0.13863438367843628], "s_value": 0.5301045775413513, "orig_id": 4245, "true_y": 1, "last_cost": 1.0, "total_cost": 1.0}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.05612130090594292, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.10122436285018921, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.0740346685051918, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.07291989773511887, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.08588903397321701, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.058149777352809906, "mask": 0.0}, "website": {"value": 1, "prob": 0.28025126457214355, "mask": 0.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.12380070984363556, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.12439636886119843, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.12797723710536957, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12046585977077484, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.12989291548728943, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12449368089437485, "mask": 0.0}, "tags": {"value": null, "prob": 0.12314065545797348, "mask": 0.0}, "comments": {"value": null, "prob": 0.12583261728286743, "mask": 0.0}}], "prob": 0.07027135789394379, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.5, "mask": 0.0, "selected": true}}, {"badge": {"value": "Teacher", "prob": 0.5, "mask": 0.0}}], "prob": 0.1772807538509369, "mask": 0.0, "selected": true}, "terminate": {"prob": 0.023857595399022102, "mask": 0, "value": null}}, "cls_probs": [0.4914620816707611, 0.3903696537017822, 0.11816830188035965], "s_value": 0.5374606847763062, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 2.0}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0522737056016922, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.08778290450572968, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.06535492092370987, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.06727083027362823, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.07756220549345016, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.051649175584316254, "mask": 0.0}, "website": {"value": 1, "prob": 0.38645026087760925, "mask": 0.0, "selected": true}, "posts": {"value": [{"title": {"value": null, "prob": 0.12419494241476059, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.12466473132371902, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.12744252383708954, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12105678021907806, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.12889742851257324, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.12452273815870285, "mask": 0.0}, "tags": {"value": null, "prob": 0.12285196781158447, "mask": 0.0}, "comments": {"value": null, "prob": 0.12636886537075043, "mask": 0.0}}], "prob": 0.06507880985736847, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.12160438299179077, "mask": 0.5}, "terminate": {"prob": 0.024972742423415184, "mask": 0, "value": null}}, "cls_probs": [0.4806089401245117, 0.39375436305999756, 0.1256367266178131], "s_value": 0.5282795429229736, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 2.100000001490116}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.06689979135990143, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.126107856631279, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.09018106758594513, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.09991754591464996, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.12654973566532135, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.06983054429292679, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.12190882116556168, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.12178961932659149, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.12864285707473755, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12032223492860794, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.1313677728176117, "mask": 0.0}, "favorites": {"value": 0.0, "prob": 0.1239820197224617, "mask": 0.0}, "tags": {"value": null, "prob": 0.12877978384494781, "mask": 0.0}, "comments": {"value": null, "prob": 0.1232069581747055, "mask": 0.0}}], "prob": 0.1439325511455536, "mask": 0.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.2677701413631439, "mask": 0.5}, "terminate": {"prob": 0.00881083682179451, "mask": 0, "value": null}}, "cls_probs": [0.41723471879959106, 0.42443224787712097, 0.1583329737186432], "s_value": 0.5127017498016357, "orig_id": 4245, "true_y": 1, "last_cost": 1.0, "total_cost": 2.600000001490116}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.13925081491470337, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.1015557274222374, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.11009865999221802, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13516825437545776, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.0818786472082138, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.1215517520904541, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.1216425746679306, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.1283482015132904, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.12027257680892944, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.13150812685489655, "mask": 0.0, "selected": true}, "favorites": {"value": 0.0, "prob": 0.12436524778604507, "mask": 0.0}, "tags": {"value": null, "prob": 0.13015587627887726, "mask": 0.0}, "comments": {"value": null, "prob": 0.12215562164783478, "mask": 0.0}}], "prob": 0.15204255282878876, "mask": 0.0, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.2658640444278717, "mask": 0.5}, "terminate": {"prob": 0.014141318388283253, "mask": 0, "value": null}}, "cls_probs": [0.41018950939178467, 0.42969733476638794, 0.160113126039505], "s_value": 0.5069003105163574, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 3.600000001490116}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1357390135526657, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.10190028697252274, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1097441166639328, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13121028244495392, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.08478469401597977, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.1393575817346573, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.13938471674919128, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.14778780937194824, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.13871215283870697, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.14378412067890167, "mask": 0.0}, "tags": {"value": null, "prob": 0.1530131697654724, "mask": 0.0, "selected": true}, "comments": {"value": null, "prob": 0.13796046376228333, "mask": 0.0}}], "prob": 0.15234392881393433, "mask": 0.125, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0}}], "prob": 0.2696290612220764, "mask": 0.5}, "terminate": {"prob": 0.014648639596998692, "mask": 0, "value": null}}, "cls_probs": [0.40307945013046265, 0.4226391613483429, 0.17428137362003326], "s_value": 0.5060673356056213, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 3.7000000029802322}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.11674921959638596, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.09345639497041702, "mask": 0.0}, "profile_img": {"value": 0, "prob": 0.1121167466044426, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1232302188873291, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.09362132102251053, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.16370652616024017, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.16345646977424622, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.1757126897573471, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17106570303440094, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.1795465052127838, "mask": 0.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.14651204645633698, "mask": 0.0}}], "prob": 0.2281160205602646, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": 0.0, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": 1.0, "mask": 0.0, "selected": true}}], "prob": 0.2254689782857895, "mask": 0.5, "selected": true}, "terminate": {"prob": 0.007241041865199804, "mask": 0, "value": null}}, "cls_probs": [0.3309599459171295, 0.40922605991363525, 0.25981393456459045], "s_value": 0.5182982087135315, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.200000002980232}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.12990833818912506, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.11824935674667358, "mask": 0.0, "selected": true}, "profile_img": {"value": 0, "prob": 0.1483803689479828, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13656504452228546, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.15495027601718903, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.16222067177295685, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.16195252537727356, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.17643964290618896, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17743292450904846, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.18769203126430511, "mask": 0.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.13426220417022705, "mask": 0.0}}], "prob": 0.29985782504081726, "mask": 0.25}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01208878867328167, "mask": 0, "value": null}}, "cls_probs": [0.20200181007385254, 0.3590022921562195, 0.43899592757225037], "s_value": 0.47872984409332275, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 4.300000004470348}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.15013712644577026, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.1694696843624115, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.1556922048330307, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.18043795228004456, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.16217555105686188, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.1619757115840912, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.17667588591575623, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.17714281380176544, "mask": 0.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.1874314248561859, "mask": 0.0, "selected": true}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.13459861278533936, "mask": 0.0}}], "prob": 0.32835420966148376, "mask": 0.25, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01590891368687153, "mask": 0, "value": null}}, "cls_probs": [0.20475763082504272, 0.3638056218624115, 0.431436687707901], "s_value": 0.4723508954048157, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.800000004470348}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1479499191045761, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.16935396194458008, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.15160994231700897, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.18346528708934784, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.19985054433345795, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.19957460463047028, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.21752740442752838, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.21897189319133759, "mask": 0.0, "selected": true}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.164075568318367, "mask": 0.0}}], "prob": 0.33107665181159973, "mask": 0.375, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.016544172540307045, "mask": 0, "value": null}}, "cls_probs": [0.20152142643928528, 0.34682968258857727, 0.45164892077445984], "s_value": 0.464513897895813, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 4.9000000059604645}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.14433123171329498, "mask": 0.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.16751018166542053, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.14774401485919952, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.18641971051692963, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.25764399766921997, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.25735965371131897, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.27868330478668213, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.20631305873394012, "mask": 0.0, "selected": true}}], "prob": 0.3377546966075897, "mask": 0.5, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.01624010130763054, "mask": 0, "value": null}}, "cls_probs": [0.20455120503902435, 0.33490896224975586, 0.4605397880077362], "s_value": 0.46587908267974854, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 5.000000007450581}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.1299339085817337, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.15907566249370575, "mask": 0.0}, "up_votes": {"value": 0.0, "prob": 0.13685406744480133, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.18747514486312866, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.323397696018219, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.32313263416290283, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.3534696400165558, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.37412241101264954, "mask": 0.625}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.012538798153400421, "mask": 0, "value": null}}, "cls_probs": [0.17828626930713654, 0.3108223080635071, 0.5108914375305176], "s_value": 0.47129032015800476, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 5.500000007450581}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.18319785594940186, "mask": 0.0, "selected": true}, "up_votes": {"value": 0.0, "prob": 0.15776242315769196, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.21755772829055786, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.3231724798679352, "mask": 0.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.3229908347129822, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.35383668541908264, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.42563238739967346, "mask": 0.625}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.015849551185965538, "mask": 0, "value": null}}, "cls_probs": [0.18888118863105774, 0.3178972899913788, 0.49322158098220825], "s_value": 0.4674002528190613, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 6.000000007450581}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.19620345532894135, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.273637056350708, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.3227662146091461, "mask": 0.0, "selected": true}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.32275307178497314, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.3544807434082031, "mask": 0.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.5097565054893494, "mask": 0.625, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.020402999594807625, "mask": 0, "value": null}}, "cls_probs": [0.17703592777252197, 0.3274531364440918, 0.4955109655857086], "s_value": 0.4605990946292877, "orig_id": 4245, "true_y": 1, "last_cost": 0.20000000298023224, "total_cost": 6.500000007450581}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.18967944383621216, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.2869188189506531, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 0.4761902689933777, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.5238096714019775, "mask": 0.0, "selected": true}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.5011711716651917, "mask": 0.75, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02223062887787819, "mask": 0, "value": null}}, "cls_probs": [0.16930876672267914, 0.32051604986190796, 0.5101752281188965], "s_value": 0.45815134048461914, "orig_id": 4245, "true_y": 1, "last_cost": 0.10000000149011612, "total_cost": 6.700000010430813}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.17552229762077332, "mask": 0.0, "selected": true}, "down_votes": {"value": 0.0, "prob": 0.29408350586891174, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 1.0, "mask": 0.0}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.5096945762634277, "mask": 0.875}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.02069959230720997, "mask": 0, "value": null}}, "cls_probs": [0.15221883356571198, 0.30370956659317017, 0.544071614742279], "s_value": 0.46071457862854004, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 6.800000011920929}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.35214030742645264, "mask": 0.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": 0.0, "mask": 1.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": 1.0, "mask": 0.0, "selected": true}, "score": {"value": 0.01, "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "answers": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "favorites": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "tags": {"value": null, "prob": 0.0, "mask": 1.0}, "comments": {"value": null, "prob": 0.0, "mask": 1.0}}], "prob": 0.6223915815353394, "mask": 0.875, "selected": true}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.025468138977885246, "mask": 0, "value": null}}, "cls_probs": [0.1534847915172577, 0.2974405288696289, 0.5490747690200806], "s_value": 0.4580737352371216, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 7.300000011920929}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.9309059977531433, "mask": 0.0, "selected": true}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": NaN, "mask": 1.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": NaN, "mask": 1.0}, "score": {"value": 0.01, "prob": NaN, "mask": 1.0}, "views": {"value": 0.0, "prob": NaN, "mask": 1.0}, "answers": {"value": 0.0, "prob": NaN, "mask": 1.0}, "favorites": {"value": 0.0, "prob": NaN, "mask": 1.0}, "tags": {"value": null, "prob": NaN, "mask": 1.0}, "comments": {"value": null, "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 0.06909400224685669, "mask": 0, "value": null}}, "cls_probs": [0.14306724071502686, 0.2810104191303253, 0.5759223103523254], "s_value": 0.45124584436416626, "orig_id": 4245, "true_y": 1, "last_cost": 0.5, "total_cost": 7.800000011920929}, {"sample": {"about_me": {"value": "<p>Love programming</p>\n", "prob": 0.0, "mask": 1.0}, "views": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0126, "prob": 0.0, "mask": 1.0}, "profile_img": {"value": 0, "prob": 0.0, "mask": 1.0}, "up_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "down_votes": {"value": 0.0, "prob": 0.0, "mask": 1.0}, "website": {"value": 1, "prob": 0.0, "mask": 1.0}, "posts": {"value": [{"title": {"value": null, "prob": NaN, "mask": 1.0}, "body": {"value": "<p>There's a method called predict_probability which returns a distribution for the prediction</p>\n", "prob": NaN, "mask": 1.0}, "score": {"value": 0.01, "prob": NaN, "mask": 1.0}, "views": {"value": 0.0, "prob": NaN, "mask": 1.0}, "answers": {"value": 0.0, "prob": NaN, "mask": 1.0}, "favorites": {"value": 0.0, "prob": NaN, "mask": 1.0}, "tags": {"value": null, "prob": NaN, "mask": 1.0}, "comments": {"value": null, "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "badges": {"value": [{"badge": {"value": "Autobiographer", "prob": NaN, "mask": 1.0}}, {"badge": {"value": "Teacher", "prob": NaN, "mask": 1.0}}], "prob": 0.0, "mask": 1.0}, "terminate": {"prob": 1.0, "mask": 1.0, "value": "class=2", "selected": true}}, "cls_probs": [0.14618757367134094, 0.29546838998794556, 0.5583440065383911], "s_value": 0.45631492137908936, "orig_id": 4245, "true_y": 1, "last_cost": 0.0, "total_cost": 8.300000011920929}], [{"sample": {"about_me": {"value": null, "prob": 0.03980446234345436, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.06316898763179779, "mask": 0.0}, "reputation": {"value": 0.0023, "prob": 0.04720594361424446, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.051973842084407806, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.060845181345939636, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.035958293825387955, "mask": 0.0}, "website": {"value": 0, "prob": 0.5591423511505127, "mask": 0.0, "selected": true}, "posts": {"value": null, "prob": 0.05742502212524414, "mask": 0.0}, "badges": {"value": null, "prob": 0.07320040464401245, "mask": 0.0}, "terminate": {"prob": 0.01127553265541792, "mask": 0, "value": null}}, "cls_probs": [0.4540405571460724, 0.41202276945114136, 0.13393673300743103], "s_value": 0.5324000716209412, "orig_id": 2421, "true_y": 1, "last_cost": 0.5, "total_cost": 0}, {"sample": {"about_me": {"value": null, "prob": 0.057598087936639786, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.07812844961881638, "mask": 0.0, "selected": true}, "reputation": {"value": 0.0023, "prob": 0.06484165042638779, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06083064526319504, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.06626950204372406, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05408487096428871, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.054625995457172394, "mask": 0.0}, "badges": {"value": null, "prob": 0.05665849149227142, "mask": 0.0}, "terminate": {"prob": 0.5069622993469238, "mask": 0, "value": null}}, "cls_probs": [0.535437285900116, 0.3701036274433136, 0.09445909410715103], "s_value": 0.5519338846206665, "orig_id": 2421, "true_y": 1, "last_cost": 0.5, "total_cost": 0.5}, {"sample": {"about_me": {"value": null, "prob": 0.06134842708706856, "mask": 0.0}, "views": {"value": 0.02, "prob": 0.0, "mask": 1.0}, "reputation": {"value": 0.0023, "prob": 0.07157180458307266, "mask": 0.0}, "profile_img": {"value": 1, "prob": 0.06503504514694214, "mask": 0.0}, "up_votes": {"value": 0.003, "prob": 0.07145826518535614, "mask": 0.0}, "down_votes": {"value": 0.0, "prob": 0.05830765888094902, "mask": 0.0}, "website": {"value": 0, "prob": 0.0, "mask": 1.0}, "posts": {"value": null, "prob": 0.05748957023024559, "mask": 0.0}, "badges": {"value": null, "prob": 0.0698375478386879, "mask": 0.0}, "terminate": {"prob": 0.5449517965316772, "mask": 1.0, "value": "class=0", "selected": true}}, "cls_probs": [0.5511713027954102, 0.36606156826019287, 0.08276713639497757], "s_value": 0.5538324117660522, "orig_id": 2421, "true_y": 1, "last_cost": 0.0, "total_cost": 1.0}]]
